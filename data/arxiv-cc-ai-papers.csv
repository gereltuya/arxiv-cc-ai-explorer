,entry_id,updated,published,title,authors,summary,comment,journal_ref,doi,primary_category,categories,links,pdf_url
0,http://arxiv.org/abs/2105.05621v1,2021-05-12 12:30:02+00:00,2021-05-12 12:30:02+00:00,NLP for Climate Policy: Creating a Knowledge Platform for Holistic and Effective Climate Action,"['Pradip Swarnakar', 'Ashutosh Modi']","Climate change is a burning issue of our time, with the Sustainable Development Goal (SDG) 13 of the United Nations demanding global climate action. Realizing the urgency, in 2015 in Paris, world leaders signed an agreement committing to taking voluntary action to reduce carbon emissions. However, the scale, magnitude, and climate action processes vary globally, especially between developed and developing countries. Therefore, from parliament to social media, the debates and discussions on climate change gather data from wide-ranging sources essential to the policy design and implementation. The downside is that we do not currently have the mechanisms to pool the worldwide dispersed knowledge emerging from the structured and unstructured data sources.   The paper thematically discusses how NLP techniques could be employed in climate policy research and contribute to society's good at large. In particular, we exemplify symbiosis of NLP and Climate Policy Research via four methodologies. The first one deals with the major topics related to climate policy using automated content analysis. We investigate the opinions (sentiments) of major actors' narratives towards climate policy in the second methodology. The third technique explores the climate actors' beliefs towards pro or anti-climate orientation. Finally, we discuss developing a Climate Knowledge Graph.   The present theme paper further argues that creating a knowledge platform would help in the formulation of a holistic climate policy and effective climate action. Such a knowledge platform would integrate the policy actors' varied opinions from different social sectors like government, business, civil society, and the scientific community. The research outcome will add value to effective climate action because policymakers can make informed decisions by looking at the diverse public opinion on a comprehensive platform.",12 Pages (8 + 4 pages for references),,,cs.CL,"['cs.CL', 'cs.AI']","['http://arxiv.org/abs/2105.05621v1', 'http://arxiv.org/pdf/2105.05621v1']",http://arxiv.org/pdf/2105.05621v1
1,http://arxiv.org/abs/2012.00614v2,2021-01-02 16:07:48+00:00,2020-12-01 16:32:54+00:00,CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims,"['Thomas Diggelmann', 'Jordan Boyd-Graber', 'Jannis Bulian', 'Massimiliano Ciaramita', 'Markus Leippold']","We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.","Accepted for the Tackling Climate Change with Machine Learning
  Workshop at NeurIPS 2020",,,cs.CL,"['cs.CL', 'cs.AI']","['http://arxiv.org/abs/2012.00614v2', 'http://arxiv.org/pdf/2012.00614v2']",http://arxiv.org/pdf/2012.00614v2
2,http://arxiv.org/abs/2011.12443v1,2020-11-24 23:34:54+00:00,2020-11-24 23:34:54+00:00,The Human Effect Requires Affect: Addressing Social-Psychological Factors of Climate Change with Machine Learning,"['Kyle Tilbury', 'Jesse Hoey']","Machine learning has the potential to aid in mitigating the human effects of climate change. Previous applications of machine learning to tackle the human effects in climate change include approaches like informing individuals of their carbon footprint and strategies to reduce it. For these methods to be the most effective they must consider relevant social-psychological factors for each individual. Of social-psychological factors at play in climate change, affect has been previously identified as a key element in perceptions and willingness to engage in mitigative behaviours. In this work, we propose an investigation into how affect could be incorporated to enhance machine learning based interventions for climate change. We propose using affective agent-based modelling for climate change as well as the use of a simulated climate change social dilemma to explore the potential benefits of affective machine learning interventions. Behavioural and informational interventions can be a powerful tool in helping humans adopt mitigative behaviours. We expect that utilizing affective ML can make interventions an even more powerful tool and help mitigative behaviours become widely adopted.","Accepted paper at the Tackling Climate Change with Machine Learning
  workshop at NeurIPS 2020",,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2011.12443v1', 'http://arxiv.org/pdf/2011.12443v1']",http://arxiv.org/pdf/2011.12443v1
3,http://arxiv.org/abs/2108.10855v2,2021-12-10 07:35:34+00:00,2021-07-28 19:00:33+00:00,Quantum Artificial Intelligence for the Science of Climate Change,"['Manmeet Singh', 'Chirag Dhara', 'Adarsh Kumar', 'Sukhpal Singh Gill', 'Steve Uhlig']","Climate change has become one of the biggest global problems increasingly compromising the Earth's habitability. Recent developments such as the extraordinary heat waves in California & Canada, and the devastating floods in Germany point to the role of climate change in the ever-increasing frequency of extreme weather. Numerical modelling of the weather and climate have seen tremendous improvements in the last five decades, yet stringent limitations remain to be overcome. Spatially and temporally localized forecasting is the need of the hour for effective adaptation measures towards minimizing the loss of life and property. Artificial Intelligence-based methods are demonstrating promising results in improving predictions, but are still limited by the availability of requisite hardware and software required to process the vast deluge of data at a scale of the planet Earth. Quantum computing is an emerging paradigm that has found potential applicability in several fields. In this opinion piece, we argue that new developments in Artificial Intelligence algorithms designed for quantum computers - also known as Quantum Artificial Intelligence (QAI) - may provide the key breakthroughs necessary to furthering the science of climate change. The resultant improvements in weather and climate forecasts are expected to cascade to numerous societal benefits.",,,,cs.AI,"['cs.AI', 'physics.ao-ph', 'quant-ph']","['http://arxiv.org/abs/2108.10855v2', 'http://arxiv.org/pdf/2108.10855v2']",http://arxiv.org/pdf/2108.10855v2
4,http://arxiv.org/abs/2104.04008v1,2021-04-08 18:54:21+00:00,2021-04-08 18:54:21+00:00,Handling Climate Change Using Counterfactuals: Using Counterfactuals in Data Augmentation to Predict Crop Growth in an Uncertain Climate Future,"['Mohammed Temraz', 'Eoin Kenny', 'Elodie Ruelle', 'Laurence Shalloo', 'Barry Smyth', 'Mark T Keane']","Climate change poses a major challenge to humanity, especially in its impact on agriculture, a challenge that a responsible AI should meet. In this paper, we examine a CBR system (PBI-CBR) designed to aid sustainable dairy farming by supporting grassland management, through accurate crop growth prediction. As climate changes, PBI-CBRs historical cases become less useful in predicting future grass growth. Hence, we extend PBI-CBR using data augmentation, to specifically handle disruptive climate events, using a counterfactual method (from XAI). Study 1 shows that historical, extreme climate-events (climate outlier cases) tend to be used by PBI-CBR to predict grass growth during climate disrupted periods. Study 2 shows that synthetic outliers, generated as counterfactuals on a outlier-boundary, improve the predictive accuracy of PBICBR, during the drought of 2018. This study also shows that an instance-based counterfactual method does better than a benchmark, constraint-guided method.","15 pages, 6 figures, 3 tables",,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2104.04008v1', 'http://arxiv.org/pdf/2104.04008v1']",http://arxiv.org/pdf/2104.04008v1
5,http://arxiv.org/abs/1905.03709v1,2019-05-02 15:34:53+00:00,2019-05-02 15:34:53+00:00,Visualizing the Consequences of Climate Change Using Cycle-Consistent Adversarial Networks,"['Victor Schmidt', 'Alexandra Luccioni', 'S. Karthik Mukkavilli', 'Narmada Balasooriya', 'Kris Sankaran', 'Jennifer Chayes', 'Yoshua Bengio']","We present a project that aims to generate images that depict accurate, vivid, and personalized outcomes of climate change using Cycle-Consistent Adversarial Networks (CycleGANs). By training our CycleGAN model on street-view images of houses before and after extreme weather events (e.g. floods, forest fires, etc.), we learn a mapping that can then be applied to images of locations that have not yet experienced these events. This visual transformation is paired with climate model predictions to assess likelihood and type of climate-related events in the long term (50 years) in order to bring the future closer in the viewers mind. The eventual goal of our project is to enable individuals to make more informed choices about their climate future by creating a more visceral understanding of the effects of climate change, while maintaining scientific credibility by drawing on climate model projections.",,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/1905.03709v1', 'http://arxiv.org/pdf/1905.03709v1']",http://arxiv.org/pdf/1905.03709v1
6,http://arxiv.org/abs/2210.03324v1,2022-10-07 04:52:26+00:00,2022-10-07 04:52:26+00:00,AutoML for Climate Change: A Call to Action,"['Renbo Tu', 'Nicholas Roberts', 'Vishak Prasad', 'Sibasis Nayak', 'Paarth Jain', 'Frederic Sala', 'Ganesh Ramakrishnan', 'Ameet Talwalkar', 'Willie Neiswanger', 'Colin White']","The challenge that climate change poses to humanity has spurred a rapidly developing field of artificial intelligence research focused on climate change applications. The climate change AI (CCAI) community works on a diverse, challenging set of problems which often involve physics-constrained ML or heterogeneous spatiotemporal data. It would be desirable to use automated machine learning (AutoML) techniques to automatically find high-performing architectures and hyperparameters for a given dataset. In this work, we benchmark popular AutoML libraries on three high-leverage CCAI applications: climate modeling, wind power forecasting, and catalyst discovery. We find that out-of-the-box AutoML libraries currently fail to meaningfully surpass the performance of human-designed CCAI models. However, we also identify a few key weaknesses, which stem from the fact that most AutoML techniques are tailored to computer vision and NLP applications. For example, while dozens of search spaces have been designed for image and language data, none have been designed for spatiotemporal data. Addressing these key weaknesses can lead to the discovery of novel architectures that yield substantial performance gains across numerous CCAI applications. Therefore, we present a call to action to the AutoML community, since there are a number of concrete, promising directions for future work in the space of AutoML for CCAI. We release our code and a list of resources at https://github.com/climate-change-automl/climate-change-automl.",,,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","['http://arxiv.org/abs/2210.03324v1', 'http://arxiv.org/pdf/2210.03324v1']",http://arxiv.org/pdf/2210.03324v1
7,http://arxiv.org/abs/2110.02871v1,2021-10-06 15:54:57+00:00,2021-10-06 15:54:57+00:00,ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods,"['Victor Schmidt', 'Alexandra Sasha Luccioni', 'Mélisande Teng', 'Tianyu Zhang', 'Alexia Reynaud', 'Sunand Raghupathi', 'Gautier Cosne', 'Adrien Juraver', 'Vahe Vardanyan', 'Alex Hernandez-Garcia', 'Yoshua Bengio']","Climate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places can help make the abstract impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable training data, we propose ClimateGAN, a model that leverages both simulated and real data for unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding.",,ICLR 2022,,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY']","['http://arxiv.org/abs/2110.02871v1', 'http://arxiv.org/pdf/2110.02871v1']",http://arxiv.org/pdf/2110.02871v1
8,http://arxiv.org/abs/2012.00483v2,2021-01-02 16:13:06+00:00,2020-12-01 13:42:37+00:00,ClimaText: A Dataset for Climate Change Topic Detection,"['Francesco S. Varini', 'Jordan Boyd-Graber', 'Massimiliano Ciaramita', 'Markus Leippold']","Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce \textsc{ClimaText}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT \cite{devlin2018bert} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.","Accepted for the Tackling Climate Change with Machine Learning
  Workshop at NeurIPS 2020",,,cs.CL,"['cs.CL', 'cs.AI']","['http://arxiv.org/abs/2012.00483v2', 'http://arxiv.org/pdf/2012.00483v2']",http://arxiv.org/pdf/2012.00483v2
9,http://arxiv.org/abs/2211.10884v1,2022-11-20 06:46:35+00:00,2022-11-20 06:46:35+00:00,Multi-scale Digital Twin: Developing a fast and physics-informed surrogate model for groundwater contamination with uncertain climate models,"['Lijing Wang', 'Takuya Kurihana', 'Aurelien Meray', 'Ilijana Mastilovic', 'Satyarth Praveen', 'Zexuan Xu', 'Milad Memarzadeh', 'Alexander Lavin', 'Haruko Wainwright']","Soil and groundwater contamination is a pervasive problem at thousands of locations across the world. Contaminated sites often require decades to remediate or to monitor natural attenuation. Climate change exacerbates the long-term site management problem because extreme precipitation and/or shifts in precipitation/evapotranspiration regimes could re-mobilize contaminants and proliferate affected groundwater. To quickly assess the spatiotemporal variations of groundwater contamination under uncertain climate disturbances, we developed a physics-informed machine learning surrogate model using U-Net enhanced Fourier Neural Operator (U-FNO) to solve Partial Differential Equations (PDEs) of groundwater flow and transport simulations at the site scale.We develop a combined loss function that includes both data-driven factors and physical boundary constraints at multiple spatiotemporal scales. Our U-FNOs can reliably predict the spatiotemporal variations of groundwater flow and contaminant transport properties from 1954 to 2100 with realistic climate projections. In parallel, we develop a convolutional autoencoder combined with online clustering to reduce the dimensionality of the vast historical and projected climate data by quantifying climatic region similarities across the United States. The ML-based unique climate clusters provide climate projections for the surrogate modeling and help return reliable future recharge rate projections immediately without querying large climate datasets. In all, this Multi-scale Digital Twin work can advance the field of environmental remediation under climate change.","5 pages, 2 figures, 1 table, Machine Learning and the Physical
  Sciences workshop, NeurIPS 2022",,,physics.geo-ph,"['physics.geo-ph', 'cs.AI']","['http://arxiv.org/abs/2211.10884v1', 'http://arxiv.org/pdf/2211.10884v1']",http://arxiv.org/pdf/2211.10884v1
10,http://arxiv.org/abs/2209.12080v2,2022-09-27 10:39:34+00:00,2022-09-24 20:25:39+00:00,Climate Impact Modelling Framework,"['Blair Edwards', 'Paolo Fraccaro', 'Nikola Stoyanov', 'Nelson Bore', 'Julian Kuehnert', 'Kommy Weldemariam', 'Anne Jones']","The application of models to assess the risk of the physical impacts of weather and climate and their subsequent consequences for society and business is of the utmost importance in our changing climate. The operation of such models is historically bespoke and constrained to specific compute infrastructure, driving datasets and predefined configurations. These constraints introduce challenges with scaling model runs and putting the models in the hands of interested users. Here we present a cloud-based modular framework for the deployment and operation of geospatial models, initially applied to climate impacts. The Climate Impact Modelling Frameworks (CIMF) enables the deployment of modular workflows in a dynamic and flexible manner. Users can specify workflow components in a streamlined manner, these components can then be easily organised into different configurations to assess risk in different ways and at different scales. This also enables different models (physical simulation or machine learning models) and workflows to be connected to produce combined risk assessment. Flood modelling is used as an end-to-end example to demonstrate the operation of CIMF.",KDD Fragile Earth workshop 2022,,,cs.DC,"['cs.DC', 'cs.AI']","['http://arxiv.org/abs/2209.12080v2', 'http://arxiv.org/pdf/2209.12080v2']",http://arxiv.org/pdf/2209.12080v2
11,http://arxiv.org/abs/2211.03219v1,2022-11-06 20:56:25+00:00,2022-11-06 20:56:25+00:00,B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,"['Mikhail Genkin', 'J. J. McArthur']","The pervasive application of artificial intelligence and machine learning algorithms is transforming many industries and aspects of the human experience. One very important industry trend is the move to convert existing human dwellings to smart buildings, and to create new smart buildings. Smart buildings aim to mitigate climate change by reducing energy consumption and associated carbon emissions. To accomplish this, they leverage artificial intelligence, big data, and machine learning algorithms to learn and optimize system performance. These fields of research are currently very rapidly evolving and advancing, but there has been very little guidance to help engineers and architects working on smart buildings apply artificial intelligence algorithms and technologies in a systematic and effective manner. In this paper we present B-SMART: the first reference architecture for autonomic smart buildings. B-SMART facilitates the application of artificial intelligence techniques and technologies to smart buildings by decoupling conceptually distinct layers of functionality and organizing them into an autonomic control loop. We also present a case study illustrating how B-SMART can be applied to accelerate the introduction of artificial intelligence into an existing smart building.",,,,cs.AI,"['cs.AI', 'I.2.1']","['http://arxiv.org/abs/2211.03219v1', 'http://arxiv.org/pdf/2211.03219v1']",http://arxiv.org/pdf/2211.03219v1
12,http://arxiv.org/abs/2012.09433v1,2020-12-17 08:04:22+00:00,2020-12-17 08:04:22+00:00,Helping Reduce Environmental Impact of Aviation with Machine Learning,['Ashish Kapoor'],"Commercial aviation is one of the biggest contributors towards climate change. We propose to reduce environmental impact of aviation by considering solutions that would reduce the flight time. Specifically, we first consider improving winds aloft forecast so that flight planners could use better information to find routes that are efficient. Secondly, we propose an aircraft routing method that seeks to find the fastest route to the destination by considering uncertainty in the wind forecasts and then optimally trading-off between exploration and exploitation.","Appeared in NeurIPS 2019 Workshop Tackling Climate Change with
  Machine Learning",,,cs.AI,"['cs.AI', 'cs.CY', 'cs.LG']","['http://arxiv.org/abs/2012.09433v1', 'http://arxiv.org/pdf/2012.09433v1']",http://arxiv.org/pdf/2012.09433v1
13,http://arxiv.org/abs/2211.03179v1,2022-11-06 17:14:14+00:00,2022-11-06 17:14:14+00:00,Personalizing Sustainable Agriculture with Causal Machine Learning,"['Georgios Giannarakis', 'Vasileios Sitokonstantinou', 'Roxanne Suzette Lorilla', 'Charalampos Kontoes']","To fight climate change and accommodate the increasing population, global crop production has to be strengthened. To achieve the ""sustainable intensification"" of agriculture, transforming it from carbon emitter to carbon sink is a priority, and understanding the environmental impact of agricultural management practices is a fundamental prerequisite to that. At the same time, the global agricultural landscape is deeply heterogeneous, with differences in climate, soil, and land use inducing variations in how agricultural systems respond to farmer actions. The ""personalization"" of sustainable agriculture with the provision of locally adapted management advice is thus a necessary condition for the efficient uplift of green metrics, and an integral development in imminent policies. Here, we formulate personalized sustainable agriculture as a Conditional Average Treatment Effect estimation task and use Causal Machine Learning for tackling it. Leveraging climate data, land use information and employing Double Machine Learning, we estimate the heterogeneous effect of sustainable practices on the field-level Soil Organic Carbon content in Lithuania. We thus provide a data-driven perspective for targeting sustainable practices and effectively expanding the global carbon sink.","Accepted for publication and spotlight presentation at Tackling
  Climate Change with Machine Learning: workshop at NeurIPS 2022",,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2211.03179v1', 'http://arxiv.org/pdf/2211.03179v1']",http://arxiv.org/pdf/2211.03179v1
14,http://arxiv.org/abs/2107.02693v2,2022-07-12 15:22:14+00:00,2021-07-06 15:55:26+00:00,Remote sensing and AI for building climate adaptation applications,"['Beril Sirmacek', 'Ricardo Vinuesa']","Urban areas are not only one of the biggest contributors to climate change, but also they are one of the most vulnerable areas with high populations who would together experience the negative impacts. In this paper, we address some of the opportunities brought by satellite remote sensing imaging and artificial intelligence (AI) in order to measure climate adaptation of cities automatically. We propose a framework combining AI and simulation which may be useful for extracting indicators from remote-sensing images and may help with predictive estimation of future states of these climate-adaptation-related indicators. When such models become more robust and used in real life applications, they may help decision makers and early responders to choose the best actions to sustain the well-being of society, natural resources and biodiversity. We underline that this is an open field and an on-going area of research for many scientists, therefore we offer an in-depth discussion on the challenges and limitations of data-driven methods and the predictive estimation models in general.",32 pages,,,cs.LG,"['cs.LG', '68T01', 'I.2.1']","['http://arxiv.org/abs/2107.02693v2', 'http://arxiv.org/pdf/2107.02693v2']",http://arxiv.org/pdf/2107.02693v2
15,http://arxiv.org/abs/2211.16947v1,2022-11-30 12:45:04+00:00,2022-11-30 12:45:04+00:00,Using Text Classification with a Bayesian Correction for Estimating Overreporting in the Creditor Reporting System on Climate Adaptation Finance,"['Janos Borst', 'Thomas Wencker', 'Andreas Niekler']","Development funds are essential to finance climate change adaptation and are thus an important part of international climate policy. % However, the absence of a common reporting practice makes it difficult to assess the amount and distribution of such funds. Research has questioned the credibility of reported figures, indicating that adaptation financing is in fact lower than published figures suggest. Projects claiming a greater relevance to climate change adaptation than they target are referred to as ""overreported"". To estimate realistic rates of overreporting in large data sets over times, we propose an approach based on state-of-the-art text classification. To date, assessments of credibility have relied on small, manually evaluated samples. We use such a sample data set to train a classifier with an accuracy of $89.81\% \pm 0.83\%$ (tenfold cross-validation) and extrapolate to larger data sets to identify overreporting. Additionally, we propose a method that incorporates evidence of smaller, higher-quality data to correct predicted rates using Bayes' theorem. This enables a comparison of different annotation schemes to estimate the degree of overreporting in climate change adaptation. Our results support findings that indicate extensive overreporting of $32.03\%$ with a credible interval of $[19.81\%;48.34\%]$.","9+4 Pages, 3 figures, 4 tables",,,cs.CY,"['cs.CY', 'cs.AI', 'J.1; I.2.7']","['http://arxiv.org/abs/2211.16947v1', 'http://arxiv.org/pdf/2211.16947v1']",http://arxiv.org/pdf/2211.16947v1
16,http://arxiv.org/abs/2209.02396v2,2022-09-22 16:50:31+00:00,2022-09-02 05:38:26+00:00,Avoiding the Great Filter: A Simulation of Important Factors for Human Survival,"['Jonathan H. Jiang', 'Ruoxin Huang', 'Prithwis Das', 'Fuyang Feng', 'Philip E. Rosen', 'Chenyu Zuo', 'Rocky Gao', 'Kristen A. Fahy', 'Leopold Van Ijzendoorn']","Humanity's path to avoiding extinction is a daunting and inevitable challenge which proves difficult to solve, partially due to the lack of data and evidence surrounding the concept. We aim to address this confusion by addressing the most dangerous threats to humanity, in hopes of providing a direction to approach this problem. Using a probabilistic model, we observed the effects of nuclear war, climate change, asteroid impacts, artificial intelligence and pandemics, which are the most harmful disasters in terms of their extent of destruction on the length of human survival. We consider the starting point of the predicted average number of survival years as the present calendar year. Nuclear war, when sampling from an artificial normal distribution, results in an average human survival time of 60 years into the future starting from the present, before a civilization-ending disaster. While climate change results in an average human survival time of 193 years, the simulation based on impact from asteroids results in an average of 1754 years. Since the risks from asteroid impacts could be considered to reside mostly in the far future, it can be concluded that nuclear war, climate change, and pandemics are presently the most prominent threats to humanity. Additionally, the danger from superiority of artificial intelligence over humans, although still somewhat abstract, is worthy of further study as its potential for impeding humankind's progress towards becoming a more advanced civilization cannot be confidently dismissed.",,,,physics.soc-ph,"['physics.soc-ph', 'physics.pop-ph']","['http://arxiv.org/abs/2209.02396v2', 'http://arxiv.org/pdf/2209.02396v2']",http://arxiv.org/pdf/2209.02396v2
17,http://arxiv.org/abs/1906.05433v2,2019-11-05 17:37:20+00:00,2019-06-10 17:51:47+00:00,Tackling Climate Change with Machine Learning,"['David Rolnick', 'Priya L. Donti', 'Lynn H. Kaack', 'Kelly Kochanski', 'Alexandre Lacoste', 'Kris Sankaran', 'Andrew Slavin Ross', 'Nikola Milojevic-Dupont', 'Natasha Jaques', 'Anna Waldman-Brown', 'Alexandra Luccioni', 'Tegan Maharaj', 'Evan D. Sherwin', 'S. Karthik Mukkavilli', 'Konrad P. Kording', 'Carla Gomes', 'Andrew Y. Ng', 'Demis Hassabis', 'John C. Platt', 'Felix Creutzig', 'Jennifer Chayes', 'Yoshua Bengio']","Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.","For additional resources, please visit the website that accompanies
  this paper: https://www.climatechange.ai/",,,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'stat.ML']","['http://arxiv.org/abs/1906.05433v2', 'http://arxiv.org/pdf/1906.05433v2']",http://arxiv.org/pdf/1906.05433v2
18,http://arxiv.org/abs/2104.12469v1,2021-04-26 10:58:44+00:00,2021-04-26 10:58:44+00:00,Generative modeling of spatio-temporal weather patterns with extreme event conditioning,"['Konstantin Klemmer', 'Sudipan Saha', 'Matthias Kahl', 'Tianlin Xu', 'Xiao Xiang Zhu']","Deep generative models are increasingly used to gain insights in the geospatial data domain, e.g., for climate data. However, most existing approaches work with temporal snapshots or assume 1D time-series; few are able to capture spatio-temporal processes simultaneously. Beyond this, Earth-systems data often exhibit highly irregular and complex patterns, for example caused by extreme weather events. Because of climate change, these phenomena are only increasing in frequency. Here, we proposed a novel GAN-based approach for generating spatio-temporal weather patterns conditioned on detected extreme events. Our approach augments GAN generator and discriminator with an encoded extreme weather event segmentation mask. These segmentation masks can be created from raw input using existing event detection frameworks. As such, our approach is highly modular and can be combined with custom GAN architectures. We highlight the applicability of our proposed approach in experiments with real-world surface radiation and zonal wind data.",ICLR'21 Workshop AI: Modeling Oceans and Climate Change (AIMOCC),,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'physics.ao-ph']","['http://arxiv.org/abs/2104.12469v1', 'http://arxiv.org/pdf/2104.12469v1']",http://arxiv.org/pdf/2104.12469v1
19,http://arxiv.org/abs/2202.07424v1,2022-02-11 14:32:55+00:00,2022-02-11 14:32:55+00:00,The potential of artificial intelligence for achieving healthy and sustainable societies,"['B. Sirmacek', 'S. Gupta', 'F. Mallor', 'H. Azizpour', 'Y. Ban', 'H. Eivazi', 'H. Fang', 'F. Golzar', 'I. Leite', 'G. I. Melsion', 'K. Smith', 'F. Fuso Nerini', 'R. Vinuesa']","In this chapter we extend earlier work (Vinuesa et al., Nature Communications 11, 2020) on the potential of artificial intelligence (AI) to achieve the 17 Sustainable Development Goals (SDGs) proposed by the United Nations (UN) for the 2030 Agenda. The present contribution focuses on three SDGs related to healthy and sustainable societies, i.e. SDG 3 (on good health), SDG 11 (on sustainable cities) and SDG 13 (on climate action). This chapter extends the previous study within those three goals, and goes beyond the 2030 targets. These SDGs are selected because they are closely related to the coronavirus disease 19 (COVID-19) pandemic, and also to crises like climate change, which constitute important challenges to our society.",,,,cs.CY,['cs.CY'],"['http://arxiv.org/abs/2202.07424v1', 'http://arxiv.org/pdf/2202.07424v1']",http://arxiv.org/pdf/2202.07424v1
20,http://arxiv.org/abs/2110.04749v1,2021-10-10 10:06:16+00:00,2021-10-10 10:06:16+00:00,Modeling of Pan Evaporation Based on the Development of Machine Learning Methods,['Mustafa Al-Mukhtar'],"For effective planning and management of water resources and implementation of the related strategies, it is important to ensure proper estimation of evaporation losses, especially in regions that are prone to drought. Changes in climatic factors, such as changes in temperature, wind speed, sunshine hours, humidity, and solar radiation can have a significant impact on the evaporation process. As such, evaporation is a highly non-linear, non-stationary process, and can be difficult to be modeled based on climatic factors, especially in different agro-climatic conditions. The aim of this study, therefore, is to investigate the feasibility of several machines learning (ML) models (conditional random forest regression, Multivariate Adaptive Regression Splines, Bagged Multivariate Adaptive Regression Splines, Model Tree M5, K- nearest neighbor, and the weighted K- nearest neighbor) for modeling the monthly pan evaporation estimation. This study proposes the development of newly explored ML models for modeling evaporation losses in three different locations over the Iraq region based on the available climatic data in such areas. The evaluation of the performance of the proposed model based on various evaluation criteria showed the capability of the proposed weighted K- nearest neighbor model in modeling the monthly evaporation losses in the studies areas with better accuracy when compared with the other existing models used as a benchmark in this study.",,,10.1007/s00704-021-03760-4,physics.pop-ph,"['physics.pop-ph', 'cs.AI', 'cs.LG']","['http://dx.doi.org/10.1007/s00704-021-03760-4', 'http://arxiv.org/abs/2110.04749v1', 'http://arxiv.org/pdf/2110.04749v1']",http://arxiv.org/pdf/2110.04749v1
21,http://arxiv.org/abs/2202.13758v2,2022-05-24 10:04:48+00:00,2022-02-28 13:18:26+00:00,Logical Fallacy Detection,"['Zhijing Jin', 'Abhinav Lalwani', 'Tejas Vaidhya', 'Xiaoyu Shen', 'Yiwen Ding', 'Zhiheng Lyu', 'Mrinmaya Sachan', 'Rada Mihalcea', 'Bernhard Schölkopf']","Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46% on Logic and 4.51% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG', 'cs.LO']","['http://arxiv.org/abs/2202.13758v2', 'http://arxiv.org/pdf/2202.13758v2']",http://arxiv.org/pdf/2202.13758v2
22,http://arxiv.org/abs/2203.10749v3,2022-09-29 14:21:19+00:00,2022-03-21 06:38:34+00:00,STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic flow prediction in Intelligent Transportation Systems,"['Wei Zhao', 'Shiqi Zhang', 'Bing Zhou', 'Bei Wang']","Air pollution and carbon emissions caused by modern transportation are closely related to global climate change. With the help of next-generation information technology such as Internet of Things (IoT) and Artificial Intelligence (AI), accurate traffic flow prediction can effectively solve problems such as traffic congestion and mitigate environmental pollution and climate change. It further promotes the development of Intelligent Transportation Systems (ITS) and smart cities. However, the strong spatial and temporal correlation of traffic data makes the task of accurate traffic forecasting a significant challenge. Existing methods are usually based on graph neural networks using predefined spatial adjacency graphs of traffic networks to model spatial dependencies, ignoring the dynamic correlation of relationships between road nodes. In addition, they usually use independent Spatio-temporal components to capture Spatio-temporal dependencies and do not effectively model global Spatio-temporal dependencies. This paper proposes a new Spatio-temporal Causal Graph Attention Network (STCGAT) for traffic prediction to address the above challenges. In STCGAT, we use a node embedding approach that can adaptively generate spatial adjacency subgraphs at each time step without a priori geographic knowledge and fine-grained modeling of the topology of dynamically generated graphs for different time steps. Meanwhile, we propose an efficient causal temporal correlation component that contains node adaptive learning, graph convolution, and local and global causal temporal convolution modules to learn local and global Spatio-temporal dependencies jointly. Extensive experiments on four real, large traffic datasets show that our model consistently outperforms all baseline models.",,IoT-25949-2022,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2203.10749v3', 'http://arxiv.org/pdf/2203.10749v3']",http://arxiv.org/pdf/2203.10749v3
23,http://arxiv.org/abs/1701.08840v1,2017-01-30 21:56:18+00:00,2017-01-30 21:56:18+00:00,Spatial Projection of Multiple Climate Variables using Hierarchical Multitask Learning,"['André R. Gonçalves', 'Arindam Banerjee', 'Fernando J. Von Zuben']","Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the super-tasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection.","Accepted for the 31st AAAI Conference on Artificial Intelligence
  (AAAI-17)",,,cs.LG,"['cs.LG', 'stat.ML']","['http://arxiv.org/abs/1701.08840v1', 'http://arxiv.org/pdf/1701.08840v1']",http://arxiv.org/pdf/1701.08840v1
24,http://arxiv.org/abs/2201.10523v1,2022-01-24 16:55:56+00:00,2022-01-24 16:55:56+00:00,Interpretability in Convolutional Neural Networks for Building Damage Classification in Satellite Imagery,['Thomas Y. Chen'],"Natural disasters ravage the world's cities, valleys, and shores on a regular basis. Deploying precise and efficient computational mechanisms for assessing infrastructure damage is essential to channel resources and minimize the loss of life. Using a dataset that includes labeled pre- and post- disaster satellite imagery, we take a machine learning-based remote sensing approach and train multiple convolutional neural networks (CNNs) to assess building damage on a per-building basis. We present a novel methodology of interpretable deep learning that seeks to explicitly investigate the most useful modalities of information in the training data to create an accurate classification model. We also investigate which loss functions best optimize these models. Our findings include that ordinal-cross entropy loss is the most optimal criterion for optimization to use and that including the type of disaster that caused the damage in combination with pre- and post-disaster training data most accurately predicts the level of damage caused. Further, we make progress in the qualitative representation of which parts of the images that the model is using to predict damage levels, through gradient-weighted class activation mapping (Grad-CAM). Our research seeks to computationally contribute to aiding in this ongoing and growing humanitarian crisis, heightened by anthropogenic climate change.","8 pages; presented as Spotlight Talk at NeurIPS - Tackling Climate
  Change with Machine Learning workshop 2020","NeurIPS 2020 Workshop on Tackling Climate Change with Machine
  Learning",,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'physics.geo-ph', 'I.4.9']","['http://arxiv.org/abs/2201.10523v1', 'http://arxiv.org/pdf/2201.10523v1']",http://arxiv.org/pdf/2201.10523v1
25,http://arxiv.org/abs/2105.00375v1,2021-05-02 01:52:59+00:00,2021-05-02 01:52:59+00:00,Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary Results,"['Harish Panneer Selvam', 'Yan Li', 'Pengyue Wang', 'William F. Northrop', 'Shashi Shekhar']","Given an on-board diagnostics (OBD) dataset and a physics-based emissions prediction model, this paper aims to develop an accurate and computational-efficient AI (Artificial Intelligence) method that predicts vehicle emissions. The problem is of societal importance because vehicular emissions lead to climate change and impact human health. This problem is challenging because the OBD data does not contain enough parameters needed by high-order physics models. Conversely, related work has shown that low-order physics models have poor predictive accuracy when using available OBD data. This paper uses a divergent window co-occurrence pattern detection method to develop a spatiotemporal variability-aware AI model for predicting emission values from the OBD datasets. We conducted a case study using real-world OBD data from a local public transportation agency. Results show that the proposed AI method has approximately 65% improved predictive accuracy than a non-AI low-order physics model and is approximately 35% more accurate than a baseline model.","Accepted by Association for Advancement of Artificial Intelligence
  (AAAI) Fall Symposium Series 2020: Physics-Guided AI to Accelerate Scientific
  Discovery (https://sites.google.com/vt.edu/pgai-aaai-20)",PGAI-AAAI-20(2020),,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2105.00375v1', 'http://arxiv.org/pdf/2105.00375v1']",http://arxiv.org/pdf/2105.00375v1
26,http://arxiv.org/abs/2012.11154v1,2020-12-21 07:08:41+00:00,2020-12-21 07:08:41+00:00,"FlowDB a large scale precipitation, river, and flash flood dataset","['Isaac Godfried', 'Kriti Mahajan', 'Maggie Wang', 'Kevin Li', 'Pranjalya Tiwari']","Flooding results in 8 billion dollars of damage annually in the US and causes the most deaths of any weather related event. Due to climate change scientists expect more heavy precipitation events in the future. However, no current datasets exist that contain both hourly precipitation and river flow data. We introduce a novel hourly river flow and precipitation dataset and a second subset of flash flood events with damage estimates and injury counts. Using these datasets we create two challenges (1) general stream flow forecasting and (2) flash flood damage estimation. We have created several publicly available benchmarks and an easy to use package. Additionally, in the future we aim to augment our dataset with snow pack data and soil index moisture data to improve predictions.",NeurIPS 2020 Workshop Tackling Climate Change with Machine Learning,,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2012.11154v1', 'http://arxiv.org/pdf/2012.11154v1']",http://arxiv.org/pdf/2012.11154v1
27,http://arxiv.org/abs/2103.08829v1,2021-03-16 03:30:53+00:00,2021-03-16 03:30:53+00:00,Towards Indirect Top-Down Road Transport Emissions Estimation,"['Ryan Mukherjee', 'Derek Rollend', 'Gordon Christie', 'Armin Hadzic', 'Sally Matson', 'Anshu Saksena', 'Marisa Hughes']","Road transportation is one of the largest sectors of greenhouse gas (GHG) emissions affecting climate change. Tackling climate change as a global community will require new capabilities to measure and inventory road transport emissions. However, the large scale and distributed nature of vehicle emissions make this sector especially challenging for existing inventory methods. In this work, we develop machine learning models that use satellite imagery to perform indirect top-down estimation of road transport emissions. Our initial experiments focus on the United States, where a bottom-up inventory was available for training our models. We achieved a mean absolute error (MAE) of 39.5 kg CO$_{2}$ of annual road transport emissions, calculated on a pixel-by-pixel (100 m$^{2}$) basis in Sentinel-2 imagery. We also discuss key model assumptions and challenges that need to be addressed to develop models capable of generalizing to global geography. We believe this work is the first published approach for automated indirect top-down estimation of road transport sector emissions using visual imagery and represents a critical step towards scalable, global, near-real-time road transportation emissions inventories that are measured both independently and objectively.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2103.08829v1', 'http://arxiv.org/pdf/2103.08829v1']",http://arxiv.org/pdf/2103.08829v1
28,http://arxiv.org/abs/2107.03182v1,2021-07-07 12:30:22+00:00,2021-07-07 12:30:22+00:00,Urban Tree Species Classification Using Aerial Imagery,"['Emily Waters', 'Mahdi Maktabdar Oghaz', 'Lakshmi Babu Saheer']","Urban trees help regulate temperature, reduce energy consumption, improve urban air quality, reduce wind speeds, and mitigating the urban heat island effect. Urban trees also play a key role in climate change mitigation and global warming by capturing and storing atmospheric carbon-dioxide which is the largest contributor to greenhouse gases. Automated tree detection and species classification using aerial imagery can be a powerful tool for sustainable forest and urban tree management. Hence, This study first offers a pipeline for generating labelled dataset of urban trees using Google Map's aerial images and then investigates how state of the art deep Convolutional Neural Network models such as VGG and ResNet handle the classification problem of urban tree aerial images under different parameters. Experimental results show our best model achieves an average accuracy of 60% over 6 tree species.","International Conference on Machine Learning (ICML 2021), Workshop on
  Tackling Climate Change with Machine Learning",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2107.03182v1', 'http://arxiv.org/pdf/2107.03182v1']",http://arxiv.org/pdf/2107.03182v1
29,http://arxiv.org/abs/2107.08074v1,2021-07-16 18:35:24+00:00,2021-07-16 18:35:24+00:00,A comparative study of stochastic and deep generative models for multisite precipitation synthesis,"['Jorge Guevara', 'Dario Borges', 'Campbell Watson', 'Bianca Zadrozny']","Future climate change scenarios are usually hypothesized using simulations from weather generators. However, there only a few works comparing and evaluating promising deep learning models for weather generation against classical approaches. This study shows preliminary results making such evaluations for the multisite precipitation synthesis task. We compared two open-source weather generators: IBMWeathergen (an extension of the Weathergen library) and RGeneratePrec, and two deep generative models: GAN and VAE, on a variety of metrics. Our preliminary results can serve as a guide for improving the design of deep learning architectures and algorithms for the multisite precipitation synthesis task.",ICML 2021 Workshop Tackling Climate Change with Machine Learning,,,cs.LG,"['cs.LG', 'cs.AI', 'physics.ao-ph']","['http://arxiv.org/abs/2107.08074v1', 'http://arxiv.org/pdf/2107.08074v1']",http://arxiv.org/pdf/2107.08074v1
30,http://arxiv.org/abs/2211.10774v1,2022-11-19 19:11:00+00:00,2022-11-19 19:11:00+00:00,Emulating Fast Processes in Climate Models,"['Noah D. Brenowitz', 'W. Andre Perkins', 'Jacqueline M. Nugent', 'Oliver Watt-Meyer', 'Spencer K. Clark', 'Anna Kwa', 'Brian Henn', 'Jeremy McGibbon', 'Christopher S. Bretherton']","Cloud microphysical parameterizations in atmospheric models describe the formation and evolution of clouds and precipitation, a central weather and climate process. Cloud-associated latent heating is a primary driver of large and small-scale circulations throughout the global atmosphere, and clouds have important interactions with atmospheric radiation. Clouds are ubiquitous, diverse, and can change rapidly. In this work, we build the first emulator of an entire cloud microphysical parameterization, including fast phase changes. The emulator performs well in offline and online (i.e. when coupled to the rest of the atmospheric model) tests, but shows some developing biases in Antarctica. Sensitivity tests demonstrate that these successes require careful modeling of the mixed discrete-continuous output as well as the input-output structure of the underlying code and physical process.","Accepted at the Machine Learning and the Physical Sciences Workshop
  at the 36th conference on Neural Information Processing Systems (NeurIPS)
  December 3, 2022",,,physics.ao-ph,['physics.ao-ph'],"['http://arxiv.org/abs/2211.10774v1', 'http://arxiv.org/pdf/2211.10774v1']",http://arxiv.org/pdf/2211.10774v1
31,http://arxiv.org/abs/2003.04310v1,2020-03-09 20:57:58+00:00,2020-03-09 20:57:58+00:00,Advancing Renewable Electricity Consumption With Reinforcement Learning,['Filip Tolovski'],"As the share of renewable energy sources in the present electric energy mix rises, their intermittence proves to be the biggest challenge to carbon free electricity generation. To address this challenge, we propose an electricity pricing agent, which sends price signals to the customers and contributes to shifting the customer demand to periods of high renewable energy generation. We propose an implementation of a pricing agent with a reinforcement learning approach where the environment is represented by the customers, the electricity generation utilities and the weather conditions.","To be presented at the Workshop on Tackling Climate Change with
  Machine Learning at ICLR 2020",,,eess.SP,"['eess.SP', 'cs.AI', 'cs.LG', 'cs.MA', 'cs.SY', 'eess.SY', 'stat.ML', 'I.2.11; I.2.8; I.2.6']","['http://arxiv.org/abs/2003.04310v1', 'http://arxiv.org/pdf/2003.04310v1']",http://arxiv.org/pdf/2003.04310v1
32,http://arxiv.org/abs/2106.15502v1,2021-06-29 15:30:55+00:00,2021-06-29 15:30:55+00:00,Attentive Neural Processes and Batch Bayesian Optimization for Scalable Calibration of Physics-Informed Digital Twins,"['Ankush Chakrabarty', 'Gordon Wichern', 'Christopher Laughman']","Physics-informed dynamical system models form critical components of digital twins of the built environment. These digital twins enable the design of energy-efficient infrastructure, but must be properly calibrated to accurately reflect system behavior for downstream prediction and analysis. Dynamical system models of modern buildings are typically described by a large number of parameters and incur significant computational expenditure during simulations. To handle large-scale calibration of digital twins without exorbitant simulations, we propose ANP-BBO: a scalable and parallelizable batch-wise Bayesian optimization (BBO) methodology that leverages attentive neural processes (ANPs).","12 pages, accepted to ICML 2021 Workshop on Tackling Climate Change
  with Machine Learning",,,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']","['http://arxiv.org/abs/2106.15502v1', 'http://arxiv.org/pdf/2106.15502v1']",http://arxiv.org/pdf/2106.15502v1
33,http://arxiv.org/abs/2112.15115v1,2021-12-30 16:17:46+00:00,2021-12-30 16:17:46+00:00,Aim in Climate Change and City Pollution,"['Pablo Torres', 'Beril Sirmacek', 'Sergio Hoyas', 'Ricardo Vinuesa']","The sustainability of urban environments is an increasingly relevant problem. Air pollution plays a key role in the degradation of the environment as well as the health of the citizens exposed to it. In this chapter we provide a review of the methods available to model air pollution, focusing on the application of machine-learning methods. In fact, machine-learning methods have proved to importantly increase the accuracy of traditional air-pollution approaches while limiting the development cost of the models. Machine-learning tools have opened new approaches to study air pollution, such as flow-dynamics modelling or remote-sensing methodologies.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY', '68-11', 'G.1.0']","['http://arxiv.org/abs/2112.15115v1', 'http://arxiv.org/pdf/2112.15115v1']",http://arxiv.org/pdf/2112.15115v1
34,http://arxiv.org/abs/2211.17132v1,2022-11-30 16:08:04+00:00,2022-11-30 16:08:04+00:00,Targets in Reinforcement Learning to solve Stackelberg Security Games,"['Saptarashmi Bandyopadhyay', 'Chenqi Zhu', 'Philip Daniel', 'Joshua Morrison', 'Ethan Shay', 'John Dickerson']","Reinforcement Learning (RL) algorithms have been successfully applied to real world situations like illegal smuggling, poaching, deforestation, climate change, airport security, etc. These scenarios can be framed as Stackelberg security games (SSGs) where defenders and attackers compete to control target resources. The algorithm's competency is assessed by which agent is controlling the targets. This review investigates modeling of SSGs in RL with a focus on possible improvements of target representations in RL algorithms.","Appears in Proceedings of AAAI FSS-22 Symposium ""Lessons Learned for
  Autonomous Assessment of Machine Abilities (LLAAMA)""",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT', 'cs.MA', 'stat.ML']","['http://arxiv.org/abs/2211.17132v1', 'http://arxiv.org/pdf/2211.17132v1']",http://arxiv.org/pdf/2211.17132v1
35,http://arxiv.org/abs/2002.05147v1,2020-02-12 18:46:48+00:00,2020-02-12 18:46:48+00:00,Multi-Agent Reinforcement Learning and Human Social Factors in Climate Change Mitigation,"['Kyle Tilbury', 'Jesse Hoey']","Many complex real-world problems, such as climate change mitigation, are intertwined with human social factors. Climate change mitigation, a social dilemma made difficult by the inherent complexities of human behavior, has an impact at a global scale. We propose applying multi-agent reinforcement learning (MARL) in this setting to develop intelligent agents that can influence the social factors at play in climate change mitigation. There are ethical, practical, and technical challenges that must be addressed when deploying MARL in this way. In this paper, we present these challenges and outline an approach to address them. Understanding how intelligent agents can be used to impact human social factors is important to prevent their abuse and can be beneficial in furthering our knowledge of these complex problems as a whole. The challenges we present are not limited to our specific application but are applicable to broader MARL. Thus, developing MARL for social factors in climate change mitigation helps address general problems hindering MARL's applicability to other real-world problems while also motivating discussion on the social implications of MARL deployment.",Accepted paper at COMARL AAAI 2020,,,cs.MA,['cs.MA'],"['http://arxiv.org/abs/2002.05147v1', 'http://arxiv.org/pdf/2002.05147v1']",http://arxiv.org/pdf/2002.05147v1
36,http://arxiv.org/abs/2204.12956v1,2022-04-27 14:13:47+00:00,2022-04-27 14:13:47+00:00,Towards assessing agricultural land suitability with causal machine learning,"['Georgios Giannarakis', 'Vasileios Sitokonstantinou', 'Roxanne Suzette Lorilla', 'Charalampos Kontoes']","Understanding the suitability of agricultural land for applying specific management practices is of great importance for sustainable and resilient agriculture against climate change. Recent developments in the field of causal machine learning enable the estimation of intervention impacts on an outcome of interest, for samples described by a set of observed characteristics. We introduce an extensible data-driven framework that leverages earth observations and frames agricultural land suitability as a geospatial impact assessment problem, where the estimated effects of agricultural practices on agroecosystems serve as a land suitability score and guide decision making. We formulate this as a causal machine learning task and discuss how this approach can be used for agricultural planning in a changing climate. Specifically, we extract the agricultural management practices of ""crop rotation"" and ""landscape crop diversity"" from crop type maps, account for climate and land use data, and use double machine learning to estimate their heterogeneous effect on Net Primary Productivity (NPP), within the Flanders region of Belgium from 2010 to 2020. We find that the effect of crop rotation was insignificant, while landscape crop diversity had a small negative effect on NPP. Finally, we observe considerable effect heterogeneity in space for both practices and analyze it.","This work has been accepted for publication in EARTHVISION 2022, in
  conjunction with the Computer Vision and Pattern Recognition (CVPR) 2022
  Conference",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","['http://arxiv.org/abs/2204.12956v1', 'http://arxiv.org/pdf/2204.12956v1']",http://arxiv.org/pdf/2204.12956v1
37,http://arxiv.org/abs/2211.00534v2,2022-11-06 13:52:22+00:00,2022-11-01 15:39:01+00:00,Deep Learning for Global Wildfire Forecasting,"['Ioannis Prapas', 'Akanksha Ahuja', 'Spyros Kondylatos', 'Ilektra Karasante', 'Eleanna Panagiotou', 'Lazaro Alonso', 'Charalampos Davalas', 'Dimitrios Michail', 'Nuno Carvalhais', 'Ioannis Papoutsis']","Climate change is expected to aggravate wildfire activity through the exacerbation of fire weather. Improving our capabilities to anticipate wildfires on a global scale is of uttermost importance for mitigating their negative effects. In this work, we create a global fire dataset and demonstrate a prototype for predicting the presence of global burned areas on a sub-seasonal scale with the use of segmentation deep learning models. Particularly, we present an open-access global analysis-ready datacube, which contains a variety of variables related to the seasonal and sub-seasonal fire drivers (climate, vegetation, oceanic indices, human-related variables), as well as the historical burned areas and wildfire emissions for 2001-2021. We train a deep learning model, which treats global wildfire forecasting as an image segmentation task and skillfully predicts the presence of burned areas 8, 16, 32 and 64 days ahead of time. Our work motivates the use of deep learning for global burned area forecasting and paves the way towards improved anticipation of global wildfire patterns.","Accepted at the NeurIPS 2022 workshop on Tackling Climate Change with
  Machine Learning",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","['http://arxiv.org/abs/2211.00534v2', 'http://arxiv.org/pdf/2211.00534v2']",http://arxiv.org/pdf/2211.00534v2
38,http://arxiv.org/abs/2211.03195v1,2022-11-06 18:22:17+00:00,2022-11-06 18:22:17+00:00,Evaluating Digital Tools for Sustainable Agriculture using Causal Inference,"['Ilias Tsoumas', 'Georgios Giannarakis', 'Vasileios Sitokonstantinou', 'Alkiviadis Koukos', 'Dimitra Loka', 'Nikolaos Bartsotas', 'Charalampos Kontoes', 'Ioannis Athanasiadis']","In contrast to the rapid digitalization of several industries, agriculture suffers from low adoption of climate-smart farming tools. Even though AI-driven digital agriculture can offer high-performing predictive functionalities, it lacks tangible quantitative evidence on its benefits to the farmers. Field experiments can derive such evidence, but are often costly and time consuming. To this end, we propose an observational causal inference framework for the empirical evaluation of the impact of digital tools on target farm performance indicators. This way, we can increase farmers' trust by enhancing the transparency of the digital agriculture market, and in turn accelerate the adoption of technologies that aim to increase productivity and secure a sustainable and resilient agriculture against a changing climate. As a case study, we perform an empirical evaluation of a recommendation system for optimal cotton sowing, which was used by a farmers' cooperative during the growing season of 2021. We leverage agricultural knowledge to develop a causal graph of the farm system, we use the back-door criterion to identify the impact of recommendations on the yield and subsequently estimate it using several methods on observational data. The results show that a field sown according to our recommendations enjoyed a significant increase in yield (12% to 17%).","Accepted for publication and spotlight presentation at Tackling
  Climate Change with Machine Learning: workshop at NeurIPS 2022",,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2211.03195v1', 'http://arxiv.org/pdf/2211.03195v1']",http://arxiv.org/pdf/2211.03195v1
39,http://arxiv.org/abs/2101.01975v1,2021-01-06 11:22:55+00:00,2021-01-06 11:22:55+00:00,Predicting Forest Fire Using Remote Sensing Data And Machine Learning,"['Suwei Yang', 'Massimo Lupascu', 'Kuldeep S. Meel']","Over the last few decades, deforestation and climate change have caused increasing number of forest fires. In Southeast Asia, Indonesia has been the most affected country by tropical peatland forest fires. These fires have a significant impact on the climate resulting in extensive health, social and economic issues. Existing forest fire prediction systems, such as the Canadian Forest Fire Danger Rating System, are based on handcrafted features and require installation and maintenance of expensive instruments on the ground, which can be a challenge for developing countries such as Indonesia. We propose a novel, cost-effective, machine-learning based approach that uses remote sensing data to predict forest fires in Indonesia. Our prediction model achieves more than 0.81 area under the receiver operator characteristic (ROC) curve, performing significantly better than the baseline approach which never exceeds 0.70 area under ROC curve on the same tasks. Our model's performance remained above 0.81 area under ROC curve even when evaluated with reduced data. The results support our claim that machine-learning based approaches can lead to reliable and cost-effective forest fire prediction systems.","8 pages, 3 figures, to be published in the Thirty-Fifth AAAI
  Conference on Artificial Intelligence (AAAI-21)",,,cs.CV,"['cs.CV', 'cs.LG']","['http://arxiv.org/abs/2101.01975v1', 'http://arxiv.org/pdf/2101.01975v1']",http://arxiv.org/pdf/2101.01975v1
40,http://arxiv.org/abs/2106.11111v1,2021-06-21 13:49:43+00:00,2021-06-21 13:49:43+00:00,Decadal Forecasts with ResDMD: a Residual DMD Neural Network,"['Eduardo Rodrigues', 'Bianca Zadrozny', 'Campbell Watson', 'David Gold']","Operational forecasting centers are investing in decadal (1-10 year) forecast systems to support long-term decision making for a more climate-resilient society. One method that has previously been employed is the Dynamic Mode Decomposition (DMD) algorithm - also known as the Linear Inverse Model - which fits linear dynamical models to data. While the DMD usually approximates non-linear terms in the true dynamics as a linear system with random noise, we investigate an extension to the DMD that explicitly represents the non-linear terms as a neural network. Our weight initialization allows the network to produce sensible results before training and then improve the prediction after training as data becomes available. In this short paper, we evaluate the proposed architecture for simulating global sea surface temperatures and compare the results with the standard DMD and seasonal forecasts produced by the state-of-the-art dynamical model, CFSv2.","Accepted to ICML 2021 Workshop Tackling Climate Change with Machine
  Learning",,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2106.11111v1', 'http://arxiv.org/pdf/2106.11111v1']",http://arxiv.org/pdf/2106.11111v1
41,http://arxiv.org/abs/2112.13465v1,2021-12-26 23:48:23+00:00,2021-12-26 23:48:23+00:00,PreDisM: Pre-Disaster Modelling With CNN Ensembles for At-Risk Communities,"['Vishal Anand', 'Yuki Miura']","The machine learning community has recently had increased interest in the climate and disaster damage domain due to a marked increased occurrences of natural hazards (e.g., hurricanes, forest fires, floods, earthquakes). However, not enough attention has been devoted to mitigating probable destruction from impending natural hazards. We explore this crucial space by predicting building-level damages on a before-the-fact basis that would allow state actors and non-governmental organizations to be best equipped with resource distribution to minimize or preempt losses. We introduce PreDisM that employs an ensemble of ResNets and fully connected layers over decision trees to capture image-level and meta-level information to accurately estimate weakness of man-made structures to disaster-occurrences. Our model performs well and is responsive to tuning across types of disasters and highlights the space of preemptive hazard damage modelling.",,"NeurIPS 2021 Workshop on Tackling Climate Change with Machine
  Learning",,cs.CV,"['cs.CV', 'cs.AI', 'cs.CY']","['http://arxiv.org/abs/2112.13465v1', 'http://arxiv.org/pdf/2112.13465v1']",http://arxiv.org/pdf/2112.13465v1
42,http://arxiv.org/abs/2212.03084v1,2022-12-04 22:41:25+00:00,2022-12-04 22:41:25+00:00,Land Use Prediction using Electro-Optical to SAR Few-Shot Transfer Learning,"['Marcel Hussing', 'Karen Li', 'Eric Eaton']","Satellite image analysis has important implications for land use, urbanization, and ecosystem monitoring. Deep learning methods can facilitate the analysis of different satellite modalities, such as electro-optical (EO) and synthetic aperture radar (SAR) imagery, by supporting knowledge transfer between the modalities to compensate for individual shortcomings. Recent progress has shown how distributional alignment of neural network embeddings can produce powerful transfer learning models by employing a sliced Wasserstein distance (SWD) loss. We analyze how this method can be applied to Sentinel-1 and -2 satellite imagery and develop several extensions toward making it effective in practice. In an application to few-shot Local Climate Zone (LCZ) prediction, we show that these networks outperform multiple common baselines on datasets with a large number of classes. Further, we provide evidence that instance normalization can significantly stabilize the training process and that explicitly shaping the embedding space using supervised contrastive learning can lead to improved performance.","Published at Tackling Climate Change with Machine Learning workshop
  at NeurIPS 2022",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2212.03084v1', 'http://arxiv.org/pdf/2212.03084v1']",http://arxiv.org/pdf/2212.03084v1
43,http://arxiv.org/abs/1911.07690v1,2019-11-18 15:13:48+00:00,2019-11-18 15:13:48+00:00,Leveraging Decentralized Artificial Intelligence to Enhance Resilience of Energy Networks,"['Ahmed Imteaj', 'M. Hadi Amini', 'Javad Mohammadi']","This paper reintroduces the notion of resilience in the context of recent issues originated from climate change triggered events including severe hurricanes and wildfires. A recent example is PG&E's forced power outage to contain wildfire risk which led to widespread power disruption. This paper focuses on answering two questions: who is responsible for resilience? and how to quantify the monetary value of resilience? To this end, we first provide preliminary definitions of resilience for power systems. We then investigate the role of natural hazards, especially wildfire, on power system resilience. Finally, we will propose a decentralized strategy for a resilient management system using distributed storage and demand response resources. Our proposed high fidelity model provides utilities, operators, and policymakers with a clearer picture for strategic decision making and preventive decisions.",,,,cs.AI,"['cs.AI', 'cs.DC', 'cs.MA', 'math.OC']","['http://arxiv.org/abs/1911.07690v1', 'http://arxiv.org/pdf/1911.07690v1']",http://arxiv.org/pdf/1911.07690v1
44,http://arxiv.org/abs/2011.08004v1,2020-11-11 23:05:12+00:00,2020-11-11 23:05:12+00:00,pymgrid: An Open-Source Python Microgrid Simulator for Applied Artificial Intelligence Research,"['Gonzague Henri', 'Tanguy Levent', 'Avishai Halev', 'Reda Alami', 'Philippe Cordier']","Microgrids, self contained electrical grids that are capable of disconnecting from the main grid, hold potential in both tackling climate change mitigation via reducing CO2 emissions and adaptation by increasing infrastructure resiliency. Due to their distributed nature, microgrids are often idiosyncratic; as a result, control of these systems is nontrivial. While microgrid simulators exist, many are limited in scope and in the variety of microgrids they can simulate. We propose pymgrid, an open-source Python package to generate and simulate a large number of microgrids, and the first open-source tool that can generate more than 600 different microgrids. pymgrid abstracts most of the domain expertise, allowing users to focus on control algorithms. In particular, pymgrid is built to be a reinforcement learning (RL) platform, and includes the ability to model microgrids as Markov decision processes. pymgrid also introduces two pre-computed list of microgrids, intended to allow for research reproducibility in the microgrid setting.",,,,cs.AI,"['cs.AI', 'cs.LG']","['http://arxiv.org/abs/2011.08004v1', 'http://arxiv.org/pdf/2011.08004v1']",http://arxiv.org/pdf/2011.08004v1
45,http://arxiv.org/abs/2012.02026v2,2022-05-29 12:52:18+00:00,2020-12-03 16:12:58+00:00,Towards an AI assistant for power grid operators,"['Antoine Marot', 'Alexandre Rozier', 'Matthieu Dussartre', 'Laure Crochepierre', 'Benjamin Donnot']","Power grids are becoming more complex to operate in the digital age given the current energy transition to cope with climate change. As a result, real-time decision-making is getting more challenging as the human operator has to deal with more information, more uncertainty, more applications, and more coordination. While supervision has been primarily used to help them make decisions over the last decades, it cannot reasonably scale up anymore. There is a great need for rethinking the human-machine interface under more unified and interactive frameworks. Taking advantage of the latest developments in Human-Machine Interface and Artificial Intelligence, we expose our vision of a new assistant framework relying on an hypervision interface and greater bidirectional interaction. We review the known principles of decision-making driving our assistant design alongside with its supporting assistance functions. We finally share some guidelines to make progress towards the development of such an assistant.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2012.02026v2', 'http://arxiv.org/pdf/2012.02026v2']",http://arxiv.org/pdf/2012.02026v2
46,http://arxiv.org/abs/2211.04584v1,2022-11-02 16:22:20+00:00,2022-11-02 16:22:20+00:00,Energy System Digitization in the Era of AI: A Three-Layered Approach towards Carbon Neutrality,"['Le Xie', 'Tong Huang', 'Xiangtian Zheng', 'Yan Liu', 'Mengdi Wang', 'Vijay Vittal', 'P. R. Kumar', 'Srinivas Shakkottai', 'Yi Cui']","The transition towards carbon-neutral electricity is one of the biggest game changers in addressing climate change since it addresses the dual challenges of removing carbon emissions from the two largest sectors of emitters: electricity and transportation. The transition to a carbon-neutral electric grid poses significant challenges to conventional paradigms of modern grid planning and operation. Much of the challenge arises from the scale of the decision making and the uncertainty associated with the energy supply and demand. Artificial Intelligence (AI) could potentially have a transformative impact on accelerating the speed and scale of carbon-neutral transition, as many decision making processes in the power grid can be cast as classic, though challenging, machine learning tasks. We point out that to amplify AI's impact on carbon-neutral transition of the electric energy systems, the AI algorithms originally developed for other applications should be tailored in three layers of technology, markets, and policy.",To be published in Patterns (Cell Press),,,cs.AI,"['cs.AI', 'cs.SY', 'eess.SY']","['http://arxiv.org/abs/2211.04584v1', 'http://arxiv.org/pdf/2211.04584v1']",http://arxiv.org/pdf/2211.04584v1
47,http://arxiv.org/abs/2101.09126v1,2021-01-22 14:33:24+00:00,2021-01-22 14:33:24+00:00,Will Artificial Intelligence supersede Earth System and Climate Models?,"['Christopher Irrgang', 'Niklas Boers', 'Maike Sonnewald', 'Elizabeth A. Barnes', 'Christopher Kadow', 'Joanna Staneva', 'Jan Saynisch-Wagner']","We outline a perspective of an entirely new research branch in Earth and climate sciences, where deep neural networks and Earth system models are dismantled as individual methodological approaches and reassembled as learning, self-validating, and interpretable Earth system model-network hybrids. Following this path, we coin the term ""Neural Earth System Modelling"" (NESYM) and highlight the necessity of a transdisciplinary discussion platform, bringing together Earth and climate scientists, big data analysts, and AI experts. We examine the concurrent potential and pitfalls of Neural Earth System Modelling and discuss the open question whether artificial intelligence will not only infuse Earth system modelling, but ultimately render them obsolete.","Perspective paper submitted to Nature Machine Intelligence, 23 pages,
  3 figures",,10.1038/s42256-021-00374-3,stat.ML,"['stat.ML', 'cs.LG', 'physics.ao-ph']","['http://dx.doi.org/10.1038/s42256-021-00374-3', 'http://arxiv.org/abs/2101.09126v1', 'http://arxiv.org/pdf/2101.09126v1']",http://arxiv.org/pdf/2101.09126v1
48,http://arxiv.org/abs/2111.06011v1,2021-11-11 01:48:46+00:00,2021-11-11 01:48:46+00:00,Climate Modeling with Neural Diffusion Equations,"['Jeehyun Hwang', 'Jeongwhan Choi', 'Hwangyong Choi', 'Kookjin Lee', 'Dongeun Lee', 'Noseong Park']","Owing to the remarkable development of deep learning technology, there have been a series of efforts to build deep learning-based climate models. Whereas most of them utilize recurrent neural networks and/or graph neural networks, we design a novel climate model based on the two concepts, the neural ordinary differential equation (NODE) and the diffusion equation. Many physical processes involving a Brownian motion of particles can be described by the diffusion equation and as a result, it is widely used for modeling climate. On the other hand, neural ordinary differential equations (NODEs) are to learn a latent governing equation of ODE from data. In our presented method, we combine them into a single framework and propose a concept, called neural diffusion equation (NDE). Our NDE, equipped with the diffusion equation and one more additional neural network to model inherent uncertainty, can learn an appropriate latent governing equation that best describes a given climate dataset. In our experiments with two real-world and one synthetic datasets and eleven baselines, our method consistently outperforms existing baselines by non-trivial margins.",Accepted by ICDM 2021,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2111.06011v1', 'http://arxiv.org/pdf/2111.06011v1']",http://arxiv.org/pdf/2111.06011v1
49,http://arxiv.org/abs/2209.08151v1,2022-08-15 07:47:38+00:00,2022-08-15 07:47:38+00:00,Efficient Climate Simulation via Machine Learning Method,"['Xin Wang', 'Wei Xue', 'Yilun Han', 'Guangwen Yang']","Hybrid modeling combining data-driven techniques and numerical methods is an emerging and promising research direction for efficient climate simulation. However, previous works lack practical platforms, making developing hybrid modeling a challenging programming problem. Furthermore, the lack of standard data sets and evaluation metrics may hamper researchers from comprehensively comparing various algorithms under a uniform condition. To address these problems, we propose a framework called NeuroClim for hybrid modeling under the real-world scenario, a basic setting to simulate the real climate that we live in. NeuroClim consists of three parts: (1) Platform. We develop a user-friendly platform NeuroGCM for efficiently developing hybrid modeling in climate simulation. (2) Dataset. We provide an open-source dataset for data-driven methods in hybrid modeling. We investigate the characteristics of the data, i.e., heterogeneity and stiffness, which reveals the difficulty of regressing climate simulation data; (3) Metrics. We propose a methodology for quantitatively evaluating hybrid modeling, including the approximation ability of machine learning models and the stability during simulation. We believe that NeuroClim allows researchers to work without high level of climate-related expertise and focus only on machine learning algorithm design, which will accelerate hybrid modeling research in the AI-Climate intersection. The codes and data are released at https://github.com/x-w19/NeuroClim.",Work in progress,,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2209.08151v1', 'http://arxiv.org/pdf/2209.08151v1']",http://arxiv.org/pdf/2209.08151v1
50,http://arxiv.org/abs/2209.13627v1,2022-09-27 18:44:41+00:00,2022-09-27 18:44:41+00:00,A critical appraisal of equity in conversational AI: Evidence from auditing GPT-3's dialogues with different publics on climate change and Black Lives Matter,"['Kaiping Chen', 'Anqi Shao', 'Jirayu Burapacheep', 'Yixuan Li']","Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving. While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society. Despite growing discussions of AI fairness across disciplines, there lacks systemic metrics to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for unpacking the meaning of equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different sub-populations on crucial science and social topics: climate change and the Black Lives Matter (BLM) movement. Our corpus consists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals who vary in gender, race and ethnicity, education level, English as a first language, and opinions toward the issues. We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat. We traced these user experience divides to conversational differences and found that GPT-3 used more negative expressions when it responded to the education and opinion minority groups, compared to its responses to the majority groups. We discuss the implications of our findings for a deliberative conversational AI system that centralizes diversity, equity, and inclusion.",,,,cs.AI,"['cs.AI', 'cs.CL', 'cs.CY', 'cs.HC']","['http://arxiv.org/abs/2209.13627v1', 'http://arxiv.org/pdf/2209.13627v1']",http://arxiv.org/pdf/2209.13627v1
51,http://arxiv.org/abs/2012.06034v1,2020-12-10 23:54:31+00:00,2020-12-10 23:54:31+00:00,Artificial Intelligence & Cooperation,"['Elisa Bertino', 'Finale Doshi-Velez', 'Maria Gini', 'Daniel Lopresti', 'David Parkes']","The rise of Artificial Intelligence (AI) will bring with it an ever-increasing willingness to cede decision-making to machines. But rather than just giving machines the power to make decisions that affect us, we need ways to work cooperatively with AI systems. There is a vital need for research in ""AI and Cooperation"" that seeks to understand the ways in which systems of AIs and systems of AIs with people can engender cooperative behavior. Trust in AI is also key: trust that is intrinsic and trust that can only be earned over time. Here we use the term ""AI"" in its broadest sense, as employed by the recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019), including but certainly not limited to, recent advances in deep learning.   With success, cooperation between humans and AIs can build society just as human-human cooperation has. Whether coming from an intrinsic willingness to be helpful, or driven through self-interest, human societies have grown strong and the human species has found success through cooperation. We cooperate ""in the small"" -- as family units, with neighbors, with co-workers, with strangers -- and ""in the large"" as a global community that seeks cooperative outcomes around questions of commerce, climate change, and disarmament. Cooperation has evolved in nature also, in cells and among animals. While many cases involving cooperation between humans and AIs will be asymmetric, with the human ultimately in control, AI systems are growing so complex that, even today, it is impossible for the human to fully comprehend their reasoning, recommendations, and actions when functioning simply as passive observers.","A Computing Community Consortium (CCC) white paper, 4 pages",,,cs.CY,"['cs.CY', 'cs.AI']","['http://arxiv.org/abs/2012.06034v1', 'http://arxiv.org/pdf/2012.06034v1']",http://arxiv.org/pdf/2012.06034v1
52,http://arxiv.org/abs/2204.02360v1,2022-03-30 21:42:21+00:00,2022-03-30 21:42:21+00:00,"Scientometric Review of Artificial Intelligence for Operations & Maintenance of Wind Turbines: The Past, Present and Future","['Joyjit Chatterjee', 'Nina Dethlefs']","Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.","This is a preprint version of the accepted manuscript in the
  Renewable and Sustainable Energy Reviews journal, shared under a CC-BY-NC-ND
  license. The final published version can be found at:
  https://doi.org/10.1016/j.rser.2021.111051","Renewable and Sustainable Energy Reviews, Volume 144, 2021",10.1016/j.rser.2021.111051,cs.AI,['cs.AI'],"['http://dx.doi.org/10.1016/j.rser.2021.111051', 'http://arxiv.org/abs/2204.02360v1', 'http://arxiv.org/pdf/2204.02360v1']",http://arxiv.org/pdf/2204.02360v1
53,http://arxiv.org/abs/2209.15424v1,2022-09-29 15:36:01+00:00,2022-09-29 15:36:01+00:00,Accurate Long-term Air Temperature Prediction with a Fusion of Artificial Intelligence and Data Reduction Techniques,"['Dušan Fister', 'Jorge Pérez-Aracil', 'César Peláez-Rodríguez', 'Javier Del Ser', 'Sancho Salcedo-Sanz']","In this paper three customised Artificial Intelligence (AI) frameworks, considering Deep Learning (convolutional neural networks), Machine Learning algorithms and data reduction techniques are proposed, for a problem of long-term summer air temperature prediction. Specifically, the prediction of average air temperature in the first and second August fortnights, using input data from previous months, at two different locations, Paris (France) and C\'ordoba (Spain), is considered. The target variable, mainly in the first August fortnight, can contain signals of extreme events such as heatwaves, like the mega-heatwave of 2003, which affected France and the Iberian Peninsula. Thus, an accurate prediction of long-term air temperature may be valuable also for different problems related to climate change, such as attribution of extreme events, and in other problems related to renewable energy. The analysis carried out this work is based on Reanalysis data, which are first processed by a correlation analysis among different prediction variables and the target (average air temperature in August first and second fortnights). An area with the largest correlation is located, and the variables within, after a feature selection process, are the input of different deep learning and ML algorithms. The experiments carried out show a very good prediction skill in the three proposed AI frameworks, both in Paris and C\'ordoba regions.","33 pages, 14 figures, 7 tables, under review",,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.LG', '68T07, 62P12, 68T01, 68T05']","['http://arxiv.org/abs/2209.15424v1', 'http://arxiv.org/pdf/2209.15424v1']",http://arxiv.org/pdf/2209.15424v1
54,http://arxiv.org/abs/2012.09670v1,2020-12-17 15:35:24+00:00,2020-12-17 15:35:24+00:00,RainBench: Towards Global Precipitation Forecasting from Satellite Imagery,"['Christian Schroeder de Witt', 'Catherine Tong', 'Valentina Zantedeschi', 'Daniele De Martini', 'Freddie Kalaitzis', 'Matthew Chantry', 'Duncan Watson-Parris', 'Piotr Bilinski']","Extreme precipitation events, such as violent rainfall and hail storms, routinely ravage economies and livelihoods around the developing world. Climate change further aggravates this issue. Data-driven deep learning approaches could widen the access to accurate multi-day forecasts, to mitigate against such events. However, there is currently no benchmark dataset dedicated to the study of global precipitation forecasts. In this paper, we introduce \textbf{RainBench}, a new multi-modal benchmark dataset for data-driven precipitation forecasting. It includes simulated satellite data, a selection of relevant meteorological data from the ERA5 reanalysis product, and IMERG precipitation data. We also release \textbf{PyRain}, a library to process large precipitation datasets efficiently. We present an extensive analysis of our novel dataset and establish baseline results for two benchmark medium-range precipitation forecasting tasks. Finally, we discuss existing data-driven weather forecasting methodologies and suggest future research avenues.","Work completed during the 2020 Frontier Development Lab research
  accelerator, a private-public partnership with NASA in the US, and ESA in
  Europe. Accepted as a spotlight/long oral talk at both Climate Change and AI,
  as well as AI for Earth Sciences Workshops at NeurIPS 2020",,,cs.LG,"['cs.LG', 'cs.AI', 'physics.ao-ph']","['http://arxiv.org/abs/2012.09670v1', 'http://arxiv.org/pdf/2012.09670v1']",http://arxiv.org/pdf/2012.09670v1
55,http://arxiv.org/abs/2201.10985v1,2022-01-26 14:58:51+00:00,2022-01-26 14:58:51+00:00,Jalisco's multiclass land cover analysis and classification using a novel lightweight convnet with real-world multispectral and relief data,"['Alexander Quevedo', 'Abraham Sánchez', 'Raul Nancláres', 'Diana P. Montoya', 'Juan Pacho', 'Jorge Martínez', 'E. Ulises Moya-Sánchez']","The understanding of global climate change, agriculture resilience, and deforestation control rely on the timely observations of the Land Use and Land Cover Change (LULCC). Recently, some deep learning (DL) methods have been adapted to make an automatic classification of Land Cover (LC) for global and homogeneous data. However, most of these DL models can not apply effectively to real-world data. i.e. a large number of classes, multi-seasonal data, diverse climate regions, high imbalance label dataset, and low-spatial resolution. In this work, we present our novel lightweight (only 89k parameters) Convolution Neural Network (ConvNet) to make LC classification and analysis to handle these problems for the Jalisco region. In contrast to the global approaches, the regional data provide the context-specificity that is required for policymakers to plan the land use and management, conservation areas, or ecosystem services. In this work, we combine three real-world open data sources to obtain 13 channels. Our embedded analysis anticipates the limited performance in some classes and gives us the opportunity to group the most similar, as a result, the test accuracy performance increase from 73 % to 83 %. We hope that this research helps other regional groups with limited data sources or computational resources to attain the United Nations Sustainable Development Goal (SDG) concerning Life on Land.",12 pages,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2201.10985v1', 'http://arxiv.org/pdf/2201.10985v1']",http://arxiv.org/pdf/2201.10985v1
56,http://arxiv.org/abs/2210.12504v1,2022-10-22 17:21:16+00:00,2022-10-22 17:21:16+00:00,Generative Modeling of High-resolution Global Precipitation Forecasts,"['James Duncan', 'Shashank Subramanian', 'Peter Harrington']","Forecasting global precipitation patterns and, in particular, extreme precipitation events is of critical importance to preparing for and adapting to climate change. Making accurate high-resolution precipitation forecasts using traditional physical models remains a major challenge in operational weather forecasting as they incur substantial computational costs and struggle to achieve sufficient forecast skill. Recently, deep-learning-based models have shown great promise in closing the gap with numerical weather prediction (NWP) models in terms of precipitation forecast skill, opening up exciting new avenues for precipitation modeling. However, it is challenging for these deep learning models to fully resolve the fine-scale structures of precipitation phenomena and adequately characterize the extremes of the long-tailed precipitation distribution. In this work, we present several improvements to the architecture and training process of a current state-of-the art deep learning precipitation model (FourCastNet) using a novel generative adversarial network (GAN) to better capture fine scales and extremes. Our improvements achieve superior performance in capturing the extreme percentiles of global precipitation, while comparable to state-of-the-art NWP models in terms of forecast skill at 1--2 day lead times. Together, these improvements set a new state-of-the-art in global precipitation forecasting.","Accepted to NeurIPS 2022 Tackling Climate Change with Machine
  Learning Workshop",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'physics.ao-ph']","['http://arxiv.org/abs/2210.12504v1', 'http://arxiv.org/pdf/2210.12504v1']",http://arxiv.org/pdf/2210.12504v1
57,http://arxiv.org/abs/2211.04755v2,2022-11-10 10:33:16+00:00,2022-11-09 09:17:42+00:00,Towards Global Crop Maps with Transfer Learning,"['Hyun-Woo Jo', 'Alkiviadis Koukos', 'Vasileios Sitokonstantinou', 'Woo-Kyun Lee', 'Charalampos Kontoes']","The continuous increase in global population and the impact of climate change on crop production are expected to affect the food sector significantly. In this context, there is need for timely, large-scale and precise mapping of crops for evidence-based decision making. A key enabler towards this direction are new satellite missions that freely offer big remote sensing data of high spatio-temporal resolution and global coverage. During the previous decade and because of this surge of big Earth observations, deep learning methods have dominated the remote sensing and crop mapping literature. Nevertheless, deep learning models require large amounts of annotated data that are scarce and hard-to-acquire. To address this problem, transfer learning methods can be used to exploit available annotations and enable crop mapping for other regions, crop types and years of inspection. In this work, we have developed and trained a deep learning model for paddy rice detection in South Korea using Sentinel-1 VH time-series. We then fine-tune the model for i) paddy rice detection in France and Spain and ii) barley detection in the Netherlands. Additionally, we propose a modification in the pre-trained weights in order to incorporate extra input features (Sentinel-1 VV). Our approach shows excellent performance when transferring in different areas for the same crop type and rather promising results when transferring in a different area and crop type.","Accepted for publication at Tackling Climate Change with Machine
  Learning: workshop at NeurIPS 2022",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2211.04755v2', 'http://arxiv.org/pdf/2211.04755v2']",http://arxiv.org/pdf/2211.04755v2
58,http://arxiv.org/abs/2012.06246v1,2020-12-11 11:21:00+00:00,2020-12-11 11:21:00+00:00,EarthNet2021: A novel large-scale dataset and challenge for forecasting localized climate impacts,"['Christian Requena-Mesa', 'Vitus Benson', 'Joachim Denzler', 'Jakob Runge', 'Markus Reichstein']","Climate change is global, yet its concrete impacts can strongly vary between different locations in the same region. Seasonal weather forecasts currently operate at the mesoscale (> 1 km). For more targeted mitigation and adaptation, modelling impacts to < 100 m is needed. Yet, the relationship between driving variables and Earth's surface at such local scales remains unresolved by current physical models. Large Earth observation datasets now enable us to create machine learning models capable of translating coarse weather information into high-resolution Earth surface forecasts. Here, we define high-resolution Earth surface forecasting as video prediction of satellite imagery conditional on mesoscale weather forecasts. Video prediction has been tackled with deep learning models. Developing such models requires analysis-ready datasets. We introduce EarthNet2021, a new, curated dataset containing target spatio-temporal Sentinel 2 satellite imagery at 20 m resolution, matched with high-resolution topography and mesoscale (1.28 km) weather variables. With over 32000 samples it is suitable for training deep neural networks. Comparing multiple Earth surface forecasts is not trivial. Hence, we define the EarthNetScore, a novel ranking criterion for models forecasting Earth surface reflectance. For model intercomparison we frame EarthNet2021 as a challenge with four tracks based on different test sets. These allow evaluation of model validity and robustness as well as model applicability to extreme events and the complete annual vegetation cycle. In addition to forecasting directly observable weather impacts through satellite-derived vegetation indices, capable Earth surface models will enable downstream applications such as crop yield prediction, forest health assessments, coastline management, or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech .","4 pages, presented at Tackling Climate Change with Machine Learning
  at NeurIPS2020",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.DB']","['http://arxiv.org/abs/2012.06246v1', 'http://arxiv.org/pdf/2012.06246v1']",http://arxiv.org/pdf/2012.06246v1
59,http://arxiv.org/abs/2107.05101v1,2021-07-11 17:48:23+00:00,2021-07-11 17:48:23+00:00,Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective,['Racine Ly'],"The improvement of computers' capacities, advancements in algorithmic techniques, and the significant increase of available data have enabled the recent developments of Artificial Intelligence (AI) technology. One of its branches, called Machine Learning (ML), has shown strong capacities in mimicking characteristics attributed to human intelligence, such as vision, speech, and problem-solving. However, as previous technological revolutions suggest, their most significant impacts could be mostly expected on other sectors that were not traditional users of that technology. The agricultural sector is vital for African economies; improving yields, mitigating losses, and effective management of natural resources are crucial in a climate change era. Machine Learning is a technology with an added value in making predictions, hence the potential to reduce uncertainties and risk across sectors, in this case, the agricultural sector. The purpose of this paper is to contextualize and discuss barriers to ML-based solutions for African agriculture. In the second section, we provided an overview of ML technology from a historical and technical perspective and its main driving force. In the third section, we provided a brief review of the current use of ML in agriculture. Finally, in section 4, we discuss ML growing interest in Africa and the potential barriers to creating and using ML-based solutions in the agricultural sector.","This paper has been submitted as an internal discussion paper at
  AKADEMIYA2063. It has 13 pages and contains 4 images and 2 tables",,,cs.LG,"['cs.LG', 'cs.AI', '68T01, 97P80', 'H.1.0; I.2.0; J.2']","['http://arxiv.org/abs/2107.05101v1', 'http://arxiv.org/pdf/2107.05101v1']",http://arxiv.org/pdf/2107.05101v1
60,http://arxiv.org/abs/1802.04742v2,2018-05-24 16:35:23+00:00,2018-02-13 17:07:13+00:00,Quantifying Uncertainty in Discrete-Continuous and Skewed Data with Bayesian Deep Learning,"['Thomas Vandal', 'Evan Kodra', 'Jennifer Dy', 'Sangram Ganguly', 'Ramakrishna Nemani', 'Auroop R. Ganguly']","Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S&E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S&E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on `DeepSD', a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S&E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized.",10 Pages,"The 24th ACM SIGKDD International Conference on Knowledge
  Discovery & Data Mining, August 19--23, 2018, London, United Kingdom",10.1145/3219819.3219996,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","['http://dx.doi.org/10.1145/3219819.3219996', 'http://arxiv.org/abs/1802.04742v2', 'http://arxiv.org/pdf/1802.04742v2']",http://arxiv.org/pdf/1802.04742v2
61,http://arxiv.org/abs/1908.05567v1,2019-08-15 14:48:31+00:00,2019-08-15 14:48:31+00:00,Deep reinforcement learning in World-Earth system models to discover sustainable management strategies,"['Felix M. Strnad', 'Wolfram Barfuss', 'Jonathan F. Donges', 'Jobst Heitzig']","Increasingly complex, non-linear World-Earth system models are used for describing the dynamics of the biophysical Earth system and the socio-economic and socio-cultural World of human societies and their interactions. Identifying pathways towards a sustainable future in these models for informing policy makers and the wider public, e.g. pathways leading to a robust mitigation of dangerous anthropogenic climate change, is a challenging and widely investigated task in the field of climate research and broader Earth system science. This problem is particularly difficult when constraints on avoiding transgressions of planetary boundaries and social foundations need to be taken into account. In this work, we propose to combine recently developed machine learning techniques, namely deep reinforcement learning (DRL), with classical analysis of trajectories in the World-Earth system. Based on the concept of the agent-environment interface, we develop an agent that is generally able to act and learn in variable manageable environment models of the Earth system. We demonstrate the potential of our framework by applying DRL algorithms to two stylized World-Earth system models. Conceptually, we explore thereby the feasibility of finding novel global governance policies leading into a safe and just operating space constrained by certain planetary and socio-economic boundaries. The artificially intelligent agent learns that the timing of a specific mix of taxing carbon emissions and subsidies on renewables is of crucial relevance for finding World-Earth system trajectories that are sustainable on the long term.","16 pages, 8 figures","Chaos 29, 123122 (2019)",10.1063/1.5124673,physics.soc-ph,"['physics.soc-ph', 'cs.LG']","['http://dx.doi.org/10.1063/1.5124673', 'http://arxiv.org/abs/1908.05567v1', 'http://arxiv.org/pdf/1908.05567v1']",http://arxiv.org/pdf/1908.05567v1
62,http://arxiv.org/abs/2011.11344v1,2020-11-23 11:54:32+00:00,2020-11-23 11:54:32+00:00,Characterization of Industrial Smoke Plumes from Remote Sensing Data,"['Michael Mommert', 'Mario Sigel', 'Marcel Neuhausler', 'Linus Scheibenreif', 'Damian Borth']","The major driver of global warming has been identified as the anthropogenic release of greenhouse gas (GHG) emissions from industrial activities. The quantitative monitoring of these emissions is mandatory to fully understand their effect on the Earth's climate and to enforce emission regulations on a large scale. In this work, we investigate the possibility to detect and quantify industrial smoke plumes from globally and freely available multi-band image data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can detect smoke plumes of different sizes with an accuracy of 94.3%. The model correctly ignores natural clouds and focuses on those imaging channels that are related to the spectral absorption from aerosols and water vapor, enabling the localization of smoke. We exploit this localization ability and train a U-Net segmentation model on a labeled sub-sample of our data, resulting in an Intersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the detection of any smoke plume of 94.0%; on average, our model can reproduce the area covered by smoke in an image to within 5.6%. The performance of our model is mostly limited by occasional confusion with surface objects, the inability to identify semi-transparent smoke, and human limitations to properly identify smoke based on RGB-only images. Nevertheless, our results enable us to reliably detect and qualitatively estimate the level of smoke activity in order to monitor activity in industrial plants across the globe. Our data set and code base are publicly available.","To be presented at the ""Tackling Climate Change with Machine
  Learning"" workshop at NeurIPS 2020",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2011.11344v1', 'http://arxiv.org/pdf/2011.11344v1']",http://arxiv.org/pdf/2011.11344v1
63,http://arxiv.org/abs/2103.12488v2,2021-10-18 13:39:34+00:00,2021-03-23 12:27:55+00:00,The Digital Agricultural Revolution: a Bibliometric Analysis Literature Review,"['Riccardo Bertoglio', 'Chiara Corbo', 'Filippo M. Renga', 'Matteo Matteucci']","The application of digital technologies in agriculture can improve traditional practices to adapt to climate change, reduce Greenhouse Gases (GHG) emissions, and promote a sustainable intensification for food security. Some authors argued that we are experiencing a Digital Agricultural Revolution (DAR) that will boost sustainable farming. This study aims to find evidence of the ongoing DAR process and clarify its roots, what it means, and where it is heading. We investigated the scientific literature with bibliometric analysis tools to produce an objective and reproducible literature review. We retrieved 4995 articles by querying the Web of Science database in the timespan 2012-2019, and we analyzed the obtained dataset to answer three specific research questions: i) what is the spectrum of the DAR-related terminology?; ii) what are the key articles and the most influential journals, institutions, and countries?; iii) what are the main research streams and the emerging topics? By grouping the authors' keywords reported on publications, we identified five main research streams: Climate-Smart Agriculture (CSA), Site-Specific Management (SSM), Remote Sensing (RS), Internet of Things (IoT), and Artificial Intelligence (AI). To provide a broad overview of each of these topics, we analyzed relevant review articles, and we present here the main achievements and the ongoing challenges. Finally, we showed the trending topics of the last three years (2017, 2018, 2019).",,IEEE Access 9 (2021) 134762-134782,10.1109/ACCESS.2021.3115258,cs.DL,['cs.DL'],"['http://dx.doi.org/10.1109/ACCESS.2021.3115258', 'http://arxiv.org/abs/2103.12488v2', 'http://arxiv.org/pdf/2103.12488v2']",http://arxiv.org/pdf/2103.12488v2
64,http://arxiv.org/abs/2111.00987v1,2021-09-25 12:37:05+00:00,2021-09-25 12:37:05+00:00,Modelling the transition to a low-carbon energy supply,['Alexander Kell'],"A transition to a low-carbon electricity supply is crucial to limit the impacts of climate change. Reducing carbon emissions could help prevent the world from reaching a tipping point, where runaway emissions are likely. Runaway emissions could lead to extremes in weather conditions around the world -- especially in problematic regions unable to cope with these conditions. However, the movement to a low-carbon energy supply can not happen instantaneously due to the existing fossil-fuel infrastructure and the requirement to maintain a reliable energy supply. Therefore, a low-carbon transition is required, however, the decisions various stakeholders should make over the coming decades to reduce these carbon emissions are not obvious. This is due to many long-term uncertainties, such as electricity, fuel and generation costs, human behaviour and the size of electricity demand. A well choreographed low-carbon transition is, therefore, required between all of the heterogenous actors in the system, as opposed to changing the behaviour of a single, centralised actor. The objective of this thesis is to create a novel, open-source agent-based model to better understand the manner in which the whole electricity market reacts to different factors using state-of-the-art machine learning and artificial intelligence methods. In contrast to other works, this thesis looks at both the long-term and short-term impact that different behaviours have on the electricity market by using these state-of-the-art methods.",PhD thesis,,,econ.GN,"['econ.GN', 'cs.AI', 'cs.LG', 'cs.MA', 'q-fin.EC']","['http://arxiv.org/abs/2111.00987v1', 'http://arxiv.org/pdf/2111.00987v1']",http://arxiv.org/pdf/2111.00987v1
65,http://arxiv.org/abs/2207.10330v1,2022-07-21 06:56:46+00:00,2022-07-21 06:56:46+00:00,Reinforcement learning for Energies of the future and carbon neutrality: a Challenge Design,"['Gaëtan Serré', 'Eva Boguslawski', 'Benjamin Donnot', 'Adrien Pavão', 'Isabelle Guyon', 'Antoine Marot']","Current rapid changes in climate increase the urgency to change energy production and consumption management, to reduce carbon and other green-house gas production. In this context, the French electricity network management company RTE (R{\'e}seau de Transport d'{\'E}lectricit{\'e}) has recently published the results of an extensive study outlining various scenarios for tomorrow's French power management. We propose a challenge that will test the viability of such a scenario. The goal is to control electricity transportation in power networks, while pursuing multiple objectives: balancing production and consumption, minimizing energetic losses, and keeping people and equipment safe and particularly avoiding catastrophic failures. While the importance of the application provides a goal in itself, this challenge also aims to push the state-of-the-art in a branch of Artificial Intelligence (AI) called Reinforcement Learning (RL), which offers new possibilities to tackle control problems. In particular, various aspects of the combination of Deep Learning and RL called Deep Reinforcement Learning remain to be harnessed in this application domain. This challenge belongs to a series started in 2019 under the name ""Learning to run a power network"" (L2RPN). In this new edition, we introduce new more realistic scenarios proposed by RTE to reach carbon neutrality by 2050, retiring fossil fuel electricity production, increasing proportions of renewable and nuclear energy and introducing batteries. Furthermore, we provide a baseline using state-of-the-art reinforcement learning algorithm to stimulate the future participants.",,"IEEE SSCI ADPRL, IEEE, Dec 2022, Singapour, Singapore",,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2207.10330v1', 'http://arxiv.org/pdf/2207.10330v1']",http://arxiv.org/pdf/2207.10330v1
66,http://arxiv.org/abs/2108.00853v2,2022-02-08 16:15:54+00:00,2021-07-27 21:37:29+00:00,Sea Ice Forecasting using Attention-based Ensemble LSTM,"['Sahara Ali', 'Yiyi Huang', 'Xin Huang', 'Jianwu Wang']","Accurately forecasting Arctic sea ice from subseasonal to seasonal scales has been a major scientific effort with fundamental challenges at play. In addition to physics-based earth system models, researchers have been applying multiple statistical and machine learning models for sea ice forecasting. Looking at the potential of data-driven sea ice forecasting, we propose an attention-based Long Short Term Memory (LSTM) ensemble method to predict monthly sea ice extent up to 1 month ahead. Using daily and monthly satellite retrieved sea ice data from NSIDC and atmospheric and oceanic variables from ERA5 reanalysis product for 39 years, we show that our multi-temporal ensemble method outperforms several baseline and recently proposed deep learning models. This will substantially improve our ability in predicting future Arctic sea ice changes, which is fundamental for forecasting transporting routes, resource development, coastal erosion, threats to Arctic coastal communities and wildlife.","Accepted by the Tackling Climate Change with Machine Learning
  Workshop at the 2021 International Conference on Machine Learning (ICML 2021)",,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2108.00853v2', 'http://arxiv.org/pdf/2108.00853v2']",http://arxiv.org/pdf/2108.00853v2
67,http://arxiv.org/abs/2211.08268v1,2022-11-15 16:20:39+00:00,2022-11-15 16:20:39+00:00,A Comparative Study of Machine Learning and Deep Learning Techniques for Prediction of Co2 Emission in Cars,"['Samveg Shah', 'Shubham Thakar', 'Kashish Jain', 'Bhavya Shah', 'Sudhir Dhage']","The most recent concern of all people on Earth is the increase in the concentration of greenhouse gas in the atmosphere. The concentration of these gases has risen rapidly over the last century and if the trend continues it can cause many adverse climatic changes. There have been ways implemented to curb this by the government by limiting processes that emit a higher amount of CO2, one such greenhouse gas. However, there is mounting evidence that the CO2 numbers supplied by the government do not accurately reflect the performance of automobiles on the road. Our proposal of using artificial intelligence techniques to improve a previously rudimentary process takes a radical tack, but it fits the bill given the situation. To determine which algorithms and models produce the greatest outcomes, we compared them all and explored a novel method of ensembling them. Further, this can be used to foretell the rise in global temperature and to ground crucial policy decisions like the adoption of electric vehicles. To estimate emissions from vehicles, we used machine learning, deep learning, and ensemble learning on a massive dataset.","Will be published in Springer L.N.N.S. Data set used
  https://www.eea.europa.eu/data-and-maps/data/co2-cars-emission-22",,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2211.08268v1', 'http://arxiv.org/pdf/2211.08268v1']",http://arxiv.org/pdf/2211.08268v1
68,http://arxiv.org/abs/2110.00828v1,2021-10-02 15:51:51+00:00,2021-10-02 15:51:51+00:00,Artificial intelligence for Sustainable Energy: A Contextual Topic Modeling and Content Analysis,"['Tahereh Saheb', 'Mohammad Dehghani']","Parallel to the rising debates over sustainable energy and artificial intelligence solutions, the world is currently discussing the ethics of artificial intelligence and its possible negative effects on society and the environment. In these arguments, sustainable AI is proposed, which aims at advancing the pathway toward sustainability, such as sustainable energy. In this paper, we offered a novel contextual topic modeling combining LDA, BERT, and Clustering. We then combined these computational analyses with content analysis of related scientific publications to identify the main scholarly topics, sub-themes, and cross-topic themes within scientific research on sustainable AI in energy. Our research identified eight dominant topics including sustainable buildings, AI-based DSSs for urban water management, climate artificial intelligence, Agriculture 4, the convergence of AI with IoT, AI-based evaluation of renewable technologies, smart campus and engineering education, and AI-based optimization. We then recommended 14 potential future research strands based on the observed theoretical gaps. Theoretically, this analysis contributes to the existing literature on sustainable AI and sustainable energy, and practically, it intends to act as a general guide for energy engineers and scientists, AI scientists, and social scientists to widen their knowledge of sustainability in AI and energy convergence research.",,,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2110.00828v1', 'http://arxiv.org/pdf/2110.00828v1']",http://arxiv.org/pdf/2110.00828v1
69,http://arxiv.org/abs/1911.03216v1,2019-11-08 12:31:49+00:00,2019-11-08 12:31:49+00:00,AI Ethics for Systemic Issues: A Structural Approach,"['Agnes Schim van der Loeff', 'Iggy Bassi', 'Sachin Kapila', 'Jevgenij Gamper']","The debate on AI ethics largely focuses on technical improvements and stronger regulation to prevent accidents or misuse of AI, with solutions relying on holding individual actors accountable for responsible AI development. While useful and necessary, we argue that this ""agency"" approach disregards more indirect and complex risks resulting from AI's interaction with the socio-economic and political context. This paper calls for a ""structural"" approach to assessing AI's effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts. This is particularly relevant for AI applied to systemic issues such as climate change and food security which require political solutions and global cooperation. To properly address the wide range of AI risks and ensure 'AI for social good', agency-focused policies must be complemented by policies informed by a structural approach.",,NeurIPS AI for Social Good 2019,,cs.AI,"['cs.AI', 'cs.CY']","['http://arxiv.org/abs/1911.03216v1', 'http://arxiv.org/pdf/1911.03216v1']",http://arxiv.org/pdf/1911.03216v1
70,http://arxiv.org/abs/1910.11227v1,2019-10-24 15:32:31+00:00,2019-10-24 15:32:31+00:00,Artificial intelligence for elections: the case of 2019 Argentina primary and presidential election,"['Zhenkun Zhou', 'Hernan A. Makse']","We use a method based on machine learning, big-data analytics, and network theory to process millions of messages posted in Twitter to predict election outcomes. The model has achieved accurate results in the current Argentina primary presidential election on August 11, 2019 by predicting the large difference win of candidate Alberto Fernandez over president Mauricio Macri; a result that none of the traditional pollsters in that country was able to predict, and has led to a major bond market collapse. We apply the model to the upcoming Argentina presidential election on October 27, 2019 yielding the following results: Fernandez 47.5%, Macri 30.9% and third party 21.6%. Our method improves over traditional polling methods which are based on direct interactions with small number of individuals that are plagued by ever declining response rates, currently falling in the low single digits. They provide a reliable polling method that can be applied not only to predict elections but to discover any trend in society, for instance, what people think about climate change, politics or education.",,,,cs.SI,"['cs.SI', 'physics.soc-ph']","['http://arxiv.org/abs/1910.11227v1', 'http://arxiv.org/pdf/1910.11227v1']",http://arxiv.org/pdf/1910.11227v1
71,http://arxiv.org/abs/2005.12401v1,2020-05-22 17:51:13+00:00,2020-05-22 17:51:13+00:00,Wind Speed Prediction and Visualization Using Long Short-Term Memory Networks (LSTM),"['Md Amimul Ehsan', 'Amir Shahirinia', 'Nian Zhang', 'Timothy Oladunni']","Climate change is one of the most concerning issues of this century. Emission from electric power generation is a crucial factor that drives the concern to the next level. Renewable energy sources are widespread and available globally, however, one of the major challenges is to understand their characteristics in a more informative way. This paper proposes the prediction of wind speed that simplifies wind farm planning and feasibility study. Twelve artificial intelligence algorithms were used for wind speed prediction from collected meteorological parameters. The model performances were compared to determine the wind speed prediction accuracy. The results show a deep learning approach, long short-term memory (LSTM) outperforms other models with the highest accuracy of 97.8%.","10th International Conference on Information Science and Technology
  (ICIST 2020)",,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2005.12401v1', 'http://arxiv.org/pdf/2005.12401v1']",http://arxiv.org/pdf/2005.12401v1
72,http://arxiv.org/abs/2010.04328v2,2021-02-08 20:47:24+00:00,2020-10-09 02:20:59+00:00,HydroDeep -- A Knowledge Guided Deep Neural Network for Geo-Spatiotemporal Data Analysis,"['Aishwarya Sarkar', 'Jien Zhang', 'Chaoqun Lu', 'Ali Jannesari']","Due to limited evidence and complex causes of regional climate change, the confidence in predicting fluvial floods remains low. Understanding the fundamental mechanisms intrinsic to geo-spatiotemporal information is crucial to improve the prediction accuracy. This paper demonstrates a hybrid neural network architecture - HydroDeep, that couples a process-based hydro-ecological model with a combination of Deep Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) Network. HydroDeep outperforms the independent CNN's and LSTM's performance by 1.6% and 10.5% respectively in Nash-Sutcliffe efficiency. Also, we show that HydroDeep pre-trained in one region is adept at passing on its knowledge to distant places via unique transfer learning approaches that minimize HydroDeep's training duration for a new region by learning its regional geo-spatiotemporal features in a reduced number of iterations.",,,,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP']","['http://arxiv.org/abs/2010.04328v2', 'http://arxiv.org/pdf/2010.04328v2']",http://arxiv.org/pdf/2010.04328v2
73,http://arxiv.org/abs/2012.02794v1,2020-12-04 16:59:15+00:00,2020-12-04 16:59:15+00:00,Impact of weather factors on migration intention using machine learning algorithms,"['John Aoga', 'Juhee Bae', 'Stefanija Veljanoska', 'Siegfried Nijssen', 'Pierre Schaus']","A growing attention in the empirical literature has been paid to the incidence of climate shocks and change in migration decisions. Previous literature leads to different results and uses a multitude of traditional empirical approaches.   This paper proposes a tree-based Machine Learning (ML) approach to analyze the role of the weather shocks towards an individual's intention to migrate in the six agriculture-dependent-economy countries such as Burkina Faso, Ivory Coast, Mali, Mauritania, Niger, and Senegal. We perform several tree-based algorithms (e.g., XGB, Random Forest) using the train-validation-test workflow to build robust and noise-resistant approaches. Then we determine the important features showing in which direction they are influencing the migration intention. This ML-based estimation accounts for features such as weather shocks captured by the Standardized Precipitation-Evapotranspiration Index (SPEI) for different timescales and various socioeconomic features/covariates.   We find that (i) weather features improve the prediction performance although socioeconomic characteristics have more influence on migration intentions, (ii) country-specific model is necessary, and (iii) international move is influenced more by the longer timescales of SPEIs while general move (which includes internal move) by that of shorter timescales.",,,,econ.GN,"['econ.GN', 'cs.AI', 'q-fin.EC']","['http://arxiv.org/abs/2012.02794v1', 'http://arxiv.org/pdf/2012.02794v1']",http://arxiv.org/pdf/2012.02794v1
74,http://arxiv.org/abs/2012.03690v1,2020-12-07 13:45:08+00:00,2020-12-07 13:45:08+00:00,An Enriched Automated PV Registry: Combining Image Recognition and 3D Building Data,"['Benjamin Rausch', 'Kevin Mayer', 'Marie-Louise Arlt', 'Gunther Gust', 'Philipp Staudt', 'Christof Weinhardt', 'Dirk Neumann', 'Ram Rajagopal']","While photovoltaic (PV) systems are installed at an unprecedented rate, reliable information on an installation level remains scarce. As a result, automatically created PV registries are a timely contribution to optimize grid planning and operations. This paper demonstrates how aerial imagery and three-dimensional building data can be combined to create an address-level PV registry, specifying area, tilt, and orientation angles. We demonstrate the benefits of this approach for PV capacity estimation. In addition, this work presents, for the first time, a comparison between automated and officially-created PV registries. Our results indicate that our enriched automated registry proves to be useful to validate, update, and complement official registries.","Tackling Climate Change with Machine Learning at NeurIPS 2020
  (Spotlight talk)",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2012.03690v1', 'http://arxiv.org/pdf/2012.03690v1']",http://arxiv.org/pdf/2012.03690v1
75,http://arxiv.org/abs/2012.09508v2,2020-12-18 02:17:15+00:00,2020-12-17 11:16:08+00:00,Towards Optimal District Heating Temperature Control in China with Deep Reinforcement Learning,"['Adrien Le-Coz', 'Tahar Nabil', 'Francois Courtot']","Achieving efficiency gains in Chinese district heating networks, thereby reducing their carbon footprint, requires new optimal control methods going beyond current industry tools. Focusing on the secondary network, we propose a data-driven deep reinforcement learning (DRL) approach to address this task. We build a recurrent neural network, trained on simulated data, to predict the indoor temperatures. This model is then used to train two DRL agents, with or without expert guidance, for the optimal control of the supply water temperature. Our tests in a multi-apartment setting show that both agents can ensure a higher thermal comfort and at the same time a smaller energy cost, compared to an optimized baseline strategy.","Accepted at NeurIPS 2020 Workshop Tackling Climate Change with
  Machine Learning",,,eess.SY,"['eess.SY', 'cs.AI', 'cs.LG', 'cs.SY']","['http://arxiv.org/abs/2012.09508v2', 'http://arxiv.org/pdf/2012.09508v2']",http://arxiv.org/pdf/2012.09508v2
76,http://arxiv.org/abs/2012.11903v1,2020-12-22 10:06:47+00:00,2020-12-22 10:06:47+00:00,Modelling Human Routines: Conceptualising Social Practice Theory for Agent-Based Simulation,"['Rijk Mercuur', 'Virginia Dignum', 'Catholijn M. Jonker']","Our routines play an important role in a wide range of social challenges such as climate change, disease outbreaks and coordinating staff and patients in a hospital. To use agent-based simulations (ABS) to understand the role of routines in social challenges we need an agent framework that integrates routines. This paper provides the domain-independent Social Practice Agent (SoPrA) framework that satisfies requirements from the literature to simulate our routines. By choosing the appropriate concepts from the literature on agent theory, social psychology and social practice theory we ensure SoPrA correctly depicts current evidence on routines. By creating a consistent, modular and parsimonious framework suitable for multiple domains we enhance the usability of SoPrA. SoPrA provides ABS researchers with a conceptual, formal and computational framework to simulate routines and gain new insights into social systems.",,,,cs.MA,"['cs.MA', 'cs.AI', 'I.6; I.2']","['http://arxiv.org/abs/2012.11903v1', 'http://arxiv.org/pdf/2012.11903v1']",http://arxiv.org/pdf/2012.11903v1
77,http://arxiv.org/abs/2102.04534v1,2021-02-05 15:12:10+00:00,2021-02-05 15:12:10+00:00,A modular framework for extreme weather generation,"['Bianca Zadrozny', 'Campbell D. Watson', 'Daniela Szwarcman', 'Daniel Civitarese', 'Dario Oliveira', 'Eduardo Rodrigues', 'Jorge Guevara']","Extreme weather events have an enormous impact on society and are expected to become more frequent and severe with climate change. In this context, resilience planning becomes crucial for risk mitigation and coping with these extreme events. Machine learning techniques can play a critical role in resilience planning through the generation of realistic extreme weather event scenarios that can be used to evaluate possible mitigation actions. This paper proposes a modular framework that relies on interchangeable components to produce extreme weather event scenarios. We discuss possible alternatives for each of the components and show initial results comparing two approaches on the task of generating precipitation scenarios.",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2102.04534v1', 'http://arxiv.org/pdf/2102.04534v1']",http://arxiv.org/pdf/2102.04534v1
78,http://arxiv.org/abs/2104.11677v1,2021-04-21 18:13:16+00:00,2021-04-21 18:13:16+00:00,Rapid Detection of Aircrafts in Satellite Imagery based on Deep Neural Networks,"['Arsalan Tahir', 'Muhammad Adil', 'Arslan Ali']","Object detection is one of the fundamental objectives in Applied Computer Vision. In some of the applications, object detection becomes very challenging such as in the case of satellite image processing. Satellite image processing has remained the focus of researchers in domains of Precision Agriculture, Climate Change, Disaster Management, etc. Therefore, object detection in satellite imagery is one of the most researched problems in this domain. This paper focuses on aircraft detection. in satellite imagery using deep learning techniques. In this paper, we used YOLO deep learning framework for aircraft detection. This method uses satellite images collected by different sources as learning for the model to perform detection. Object detection in satellite images is mostly complex because objects have many variations, types, poses, sizes, complex and dense background. YOLO has some limitations for small size objects (less than$\sim$32 pixels per object), therefore we upsample the prediction grid to reduce the coarseness of the model and to accurately detect the densely clustered objects. The improved model shows good accuracy and performance on different unknown images having small, rotating, and dense objects to meet the requirements in real-time.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2104.11677v1', 'http://arxiv.org/pdf/2104.11677v1']",http://arxiv.org/pdf/2104.11677v1
79,http://arxiv.org/abs/2107.06846v1,2021-07-14 17:02:15+00:00,2021-07-14 17:02:15+00:00,Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network,"['Daniel Salles Civitarese', 'Daniela Szwarcman', 'Bianca Zadrozny', 'Campbell Watson']","An impact of climate change is the increase in frequency and intensity of extreme precipitation events. However, confidently predicting the likelihood of extreme precipitation at seasonal scales remains an outstanding challenge. Here, we present an approach to forecasting the quantiles of the maximum daily precipitation in each week up to six months ahead using the temporal fusion transformer (TFT) model. Through experiments in two regions, we compare TFT predictions with those of two baselines: climatology and a calibrated ECMWF SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk at six month lead time, the TFT predictions significantly outperform those from S5 and show an overall small improvement compared to climatology. The TFT also responds positively to departures from normal that climatology cannot.",,,,cs.LG,"['cs.LG', 'cs.AI', 'physics.ao-ph']","['http://arxiv.org/abs/2107.06846v1', 'http://arxiv.org/pdf/2107.06846v1']",http://arxiv.org/pdf/2107.06846v1
80,http://arxiv.org/abs/2109.10085v1,2021-09-21 10:42:24+00:00,2021-09-21 10:42:24+00:00,Heterogeneous Ensemble for ESG Ratings Prediction,"['Tim Krappel', 'Alex Bogun', 'Damian Borth']","Over the past years, topics ranging from climate change to human rights have seen increasing importance for investment decisions. Hence, investors (asset managers and asset owners) who wanted to incorporate these issues started to assess companies based on how they handle such topics. For this assessment, investors rely on specialized rating agencies that issue ratings along the environmental, social and governance (ESG) dimensions. Such ratings allow them to make investment decisions in favor of sustainability. However, rating agencies base their analysis on subjective assessment of sustainability reports, not provided by every company. Furthermore, due to human labor involved, rating agencies are currently facing the challenge to scale up the coverage in a timely manner.   In order to alleviate these challenges and contribute to the overall goal of supporting sustainability, we propose a heterogeneous ensemble model to predict ESG ratings using fundamental data. This model is based on feedforward neural network, CatBoost and XGBoost ensemble members. Given the public availability of fundamental data, the proposed method would allow cost-efficient and scalable creation of initial ESG ratings (also for companies without sustainability reporting). Using our approach we are able to explain 54% of the variation in ratings R2 using fundamental data and outperform prior work in this area.",Accepted to KDD Workshop on Machine Learning in Finance 2021,,,cs.AI,"['cs.AI', 'J.4']","['http://arxiv.org/abs/2109.10085v1', 'http://arxiv.org/pdf/2109.10085v1']",http://arxiv.org/pdf/2109.10085v1
81,http://arxiv.org/abs/2204.05149v1,2022-04-11 14:30:27+00:00,2022-04-11 14:30:27+00:00,"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink","['David Patterson', 'Joseph Gonzalez', 'Urs Hölzle', 'Quoc Le', 'Chen Liang', 'Lluis-Miquel Munguia', 'Daniel Rothchild', 'David So', 'Maud Texier', 'Jeff Dean']","Machine Learning (ML) workloads have rapidly grown in importance, but raised concerns about their carbon footprint. Four best practices can reduce ML training energy by up to 100x and CO2 emissions up to 1000x. By following best practices, overall ML energy use (across research, development, and production) held steady at <15% of Google's total energy use for the past three years. If the whole ML field were to adopt best practices, total carbon emissions from training would reduce. Hence, we recommend that ML papers include emissions explicitly to foster competition on more than just model quality. Estimates of emissions in papers that omitted them have been off 100x-100,000x, so publishing emissions has the added benefit of ensuring accurate accounting. Given the importance of climate change, we must get the numbers right to make certain that we work on its biggest challenges.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.GL']","['http://arxiv.org/abs/2204.05149v1', 'http://arxiv.org/pdf/2204.05149v1']",http://arxiv.org/pdf/2204.05149v1
82,http://arxiv.org/abs/2205.01989v1,2022-05-04 10:43:58+00:00,2022-05-04 10:43:58+00:00,MM-Claims: A Dataset for Multimodal Claim Detection in Social Media,"['Gullal S. Cheema', 'Sherzod Hakimov', 'Abdul Sittar', 'Eric Müller-Budack', 'Christian Otto', 'Ralph Ewerth']","In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.",Accepted to Findings of NAACL 2022,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.MM', 'cs.SI']","['http://arxiv.org/abs/2205.01989v1', 'http://arxiv.org/pdf/2205.01989v1']",http://arxiv.org/pdf/2205.01989v1
83,http://arxiv.org/abs/2206.06369v2,2022-10-28 12:09:41+00:00,2022-06-10 07:23:22+00:00,Towards predicting dynamic stability of power grids with Graph Neural Networks,"['Christian Nauck', 'Michael Lindner', 'Konstantin Schürholt', 'Frank Hellmann']","To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia and volatility in production. However, dynamic stability simulations are intractable and exceedingly expensive for large grids. Graph Neural Networks (GNNs) are a promising method to reduce the computational effort of analyzing dynamic stability of power grids. We provide new datasets of dynamic stability of synthetic power grids and find that GNNs are surprisingly effective at predicting highly non-linear targets from topological information only. We show that large GNNs outperform GNNs from previous work as well as as handcrafted graph features and semi-analytic approximations. Further, we demonstrate GNNs can accurately identify trouble maker-nodes in the power grids. Lastly, we show that GNNs trained on small grids can perform accurately on a large synthetic Texan power grid model, which illustrates the potential of our approach.","10 pages + references and appendix, 8 figures",,,cs.LG,"['cs.LG', 'cs.AI', 'physics.data-an']","['http://arxiv.org/abs/2206.06369v2', 'http://arxiv.org/pdf/2206.06369v2']",http://arxiv.org/pdf/2206.06369v2
84,http://arxiv.org/abs/2208.05419v1,2022-08-08 06:06:31+00:00,2022-08-08 06:06:31+00:00,FourCastNet: Accelerating Global High-Resolution Weather Forecasting using Adaptive Fourier Neural Operators,"['Thorsten Kurth', 'Shashank Subramanian', 'Peter Harrington', 'Jaideep Pathak', 'Morteza Mardani', 'David Hall', 'Andrea Miele', 'Karthik Kashinath', 'Animashree Anandkumar']","Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy due to high computational cost and strict time-to-solution limits. We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCast-Net is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9%of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072GPUs is 67.4minutes, resulting in an 80,000times faster time-to-solution relative to state-of-the-art NWP, in inference. FourCastNet produces accurate instantaneous weather predictions for a week in advance, enables enormous ensembles that better capture weather extremes, and supports higher global forecast resolutions.",,,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.PF']","['http://arxiv.org/abs/2208.05419v1', 'http://arxiv.org/pdf/2208.05419v1']",http://arxiv.org/pdf/2208.05419v1
85,http://arxiv.org/abs/2208.10932v1,2022-08-23 13:03:02+00:00,2022-08-23 13:03:02+00:00,Research Note on Uncertain Probabilities and Abstract Argumentation,"['Pietro Baroni', 'Federico Cerutti', 'Massimiliano Giacomin', 'Lance M. Kaplan', 'Murat Sensoy']","The sixth assessment of the international panel on climate change (IPCC) states that ""cumulative net CO2 emissions over the last decade (2010-2019) are about the same size as the 11 remaining carbon budget likely to limit warming to 1.5C (medium confidence)."" Such reports directly feed the public discourse, but nuances such as the degree of belief and of confidence are often lost. In this paper, we propose a formal account for allowing such degrees of belief and the associated confidence to be used to label arguments in abstract argumentation settings. Differently from other proposals in probabilistic argumentation, we focus on the task of probabilistic inference over a chosen query building upon Sato's distribution semantics which has been already shown to encompass a variety of cases including the semantics of Bayesian networks. Borrowing from the vast literature on such semantics, we examine how such tasks can be dealt with in practice when considering uncertain probabilities, and discuss the connections with existing proposals for probabilistic argumentation.",,,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2208.10932v1', 'http://arxiv.org/pdf/2208.10932v1']",http://arxiv.org/pdf/2208.10932v1
86,http://arxiv.org/abs/2210.00113v1,2022-09-30 22:10:38+00:00,2022-09-30 22:10:38+00:00,"Institutional Foundations of Adaptive Planning: Exploration of Flood Planning in the Lower Rio Grande Valley, Texas, USA","['Ashley D. Ross', 'Ali Nejat', 'Virgie Greb']","Adaptive planning is ideally suited for the deep uncertainties presented by climate change. While there is a robust scholarship on the theory and methods of adaptive planning, this has largely neglected how adaptive planning is affected by existing planning institutions and how to move forward within the constraints of traditional planning organizations. This study asks: How do existing traditional planning institutions support adaptive planning? We explore this for flood planning in the Lower Rio Grande Valley of Texas, United States. We draw on county hazard plan and regional flood plan documents as well as transcripts of regional flood planning meetings to explore the emergent topics of these institutional outputs. Using Natural Language Processing to analyze this large amount of text, we find that hazard plans and discussions developing these plans are largely lacking an adaptive approach.",,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR']","['http://arxiv.org/abs/2210.00113v1', 'http://arxiv.org/pdf/2210.00113v1']",http://arxiv.org/pdf/2210.00113v1
87,http://arxiv.org/abs/2210.08103v1,2022-10-14 20:55:10+00:00,2022-10-14 20:55:10+00:00,High-resolution synthetic residential energy use profiles for the United States,"['Swapna Thorve', 'Young Yun Baek', 'Samarth Swarup', 'Henning Mortveit', 'Achla Marathe', 'Anil Vullikanti', 'Madhav Marathe']","Efficient energy consumption is crucial for achieving sustainable energy goals in the era of climate change and grid modernization. Thus, it is vital to understand how energy is consumed at finer resolutions such as household in order to plan demand-response events or analyze the impacts of weather, electricity prices, electric vehicles, solar, and occupancy schedules on energy consumption. However, availability and access to detailed energy-use data, which would enable detailed studies, has been rare. In this paper, we release a unique, large-scale, synthetic, residential energy-use dataset for the residential sector across the contiguous United States covering millions of households. The data comprise of hourly energy use profiles for synthetic households, disaggregated into Thermostatically Controlled Loads (TCL) and appliance use. The underlying framework is constructed using a bottom-up approach. Diverse open-source surveys and first principles models are used for end-use modeling. Extensive validation of the synthetic dataset has been conducted through comparisons with reported energy-use data. We present a detailed, open, high-resolution, residential energy-use dataset for the United States.","The paper has been conditionally accepted for publication in Nature
  Scientific Data",,,eess.SP,"['eess.SP', 'cs.AI']","['http://arxiv.org/abs/2210.08103v1', 'http://arxiv.org/pdf/2210.08103v1']",http://arxiv.org/pdf/2210.08103v1
88,http://arxiv.org/abs/2210.12104v1,2022-10-21 16:59:06+00:00,2022-10-21 16:59:06+00:00,Towards transparent ANN wind turbine power curve models,['Simon Letzgus'],"Accurate wind turbine power curve models, which translate ambient conditions into turbine power output, are crucial for wind energy to scale and fulfil its proposed role in the global energy transition. Machine learning methods, in particular deep neural networks (DNNs), have shown significant advantages over parametric, physics-informed power curve modelling approaches. Nevertheless, they are often criticised as opaque black boxes with no physical understanding of the system they model, which hinders their application in practice. We apply Shapley values, a popular explainable artificial intelligence (XAI) method, to, for the first time, uncover and validate the strategies learned by DNNs from operational wind turbine data. Our findings show that the trend towards ever larger model architectures, driven by the focus on test-set performance, can result in physically implausible model strategies, similar to the Clever Hans effect observed in classification. We, therefore, call for a more prominent role of XAI methods in model selection and additionally offer a practical strategy to use model explanations for wind turbine condition monitoring.","Submitted to NeurIPS 2022 Workshop Tackling Climate Change with
  Machine Learning",,,cs.LG,"['cs.LG', 'eess.SP']","['http://arxiv.org/abs/2210.12104v1', 'http://arxiv.org/pdf/2210.12104v1']",http://arxiv.org/pdf/2210.12104v1
89,http://arxiv.org/abs/2210.12185v1,2022-10-21 18:25:34+00:00,2022-10-21 18:25:34+00:00,Attention-Based Scattering Network for Satellite Imagery,"['Jason Stock', 'Chuck Anderson']","Multi-channel satellite imagery, from stacked spectral bands or spatiotemporal data, have meaningful representations for various atmospheric properties. Combining these features in an effective manner to create a performant and trustworthy model is of utmost importance to forecasters. Neural networks show promise, yet suffer from unintuitive computations, fusion of high-level features, and may be limited by the quantity of available data. In this work, we leverage the scattering transform to extract high-level features without additional trainable parameters and introduce a separation scheme to bring attention to independent input channels. Experiments show promising results on estimating tropical cyclone intensity and predicting the occurrence of lightning from satellite imagery.","NeurIPS 2022 Workshop - Tackling Climate Change with Machine
  Learning, 4 page limit w/ appendix",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2210.12185v1', 'http://arxiv.org/pdf/2210.12185v1']",http://arxiv.org/pdf/2210.12185v1
90,http://arxiv.org/abs/2210.14706v1,2022-10-26 13:33:58+00:00,2022-10-26 13:33:58+00:00,Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise,"['Wenbo Gong', 'Joel Jennings', 'Cheng Zhang', 'Nick Pawlowski']","Discovering causal relationships between different variables from time series data has been a long-standing challenge for many domains such as climate science, finance, and healthcare. Given the complexity of real-world relationships and the nature of observations in discrete time, causal discovery methods need to consider non-linear relations between variables, instantaneous effects and history-dependent noise (the change of noise distribution due to past actions). However, previous works do not offer a solution addressing all these problems together. In this paper, we propose a novel causal relationship learning framework for time-series data, called Rhino, which combines vector auto-regression, deep learning and variational inference to model non-linear relationships with instantaneous effects while allowing the noise distribution to be modulated by historical observations. Theoretically, we prove the structural identifiability of Rhino. Our empirical results from extensive synthetic experiments and two real-world benchmarks demonstrate better discovery performance compared to relevant baselines, with ablation studies revealing its robustness under model misspecification.","28 pages, 8 figures, 5 tables",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","['http://arxiv.org/abs/2210.14706v1', 'http://arxiv.org/pdf/2210.14706v1']",http://arxiv.org/pdf/2210.14706v1
91,http://arxiv.org/abs/2211.05728v1,2022-11-10 17:52:43+00:00,2022-11-10 17:52:43+00:00,Quantum Power Flows: From Theory to Practice,"['Junyu Liu', 'Han Zheng', 'Masanori Hanada', 'Kanav Setia', 'Dan Wu']","Climate change is becoming one of the greatest challenges to the sustainable development of modern society. Renewable energies with low density greatly complicate the online optimization and control processes, where modern advanced computational technologies, specifically quantum computing, have significant potential to help. In this paper, we discuss applications of quantum computing algorithms toward state-of-the-art smart grid problems. We suggest potential, exponential quantum speedup by the use of the Harrow-Hassidim-Lloyd (HHL) algorithms for sparse matrix inversions in power-flow problems. However, practical implementations of the algorithm are limited by the noise of quantum circuits, the hardness of realizations of quantum random access memories (QRAM), and the depth of the required quantum circuits. We benchmark the hardware and software requirements from the state-of-the-art power-flow algorithms, including QRAM requirements from hybrid phonon-transmon systems, and explicit gate counting used in HHL for explicit realizations. We also develop near-term algorithms of power flow by variational quantum circuits and implement real experiments for 6 qubits with a truncated version of power flows.","30 pages, many figures",,,quant-ph,"['quant-ph', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY']","['http://arxiv.org/abs/2211.05728v1', 'http://arxiv.org/pdf/2211.05728v1']",http://arxiv.org/pdf/2211.05728v1
92,http://arxiv.org/abs/2212.00669v1,2022-10-25 01:04:13+00:00,2022-10-25 01:04:13+00:00,A POMDP Model for Safe Geological Carbon Sequestration,"['Anthony Corso', 'Yizheng Wang', 'Markus Zechner', 'Jef Caers', 'Mykel J. Kochenderfer']","Geological carbon capture and sequestration (CCS), where CO$_2$ is stored in subsurface formations, is a promising and scalable approach for reducing global emissions. However, if done incorrectly, it may lead to earthquakes and leakage of CO$_2$ back to the surface, harming both humans and the environment. These risks are exacerbated by the large amount of uncertainty in the structure of the storage formation. For these reasons, we propose that CCS operations be modeled as a partially observable Markov decision process (POMDP) and decisions be informed using automated planning algorithms. To this end, we develop a simplified model of CCS operations based on a 2D spillpoint analysis that retains many of the challenges and safety considerations of the real-world problem. We show how off-the-shelf POMDP solvers outperform expert baselines for safe CCS planning. This POMDP model can be used as a test bed to drive the development of novel decision-making algorithms for CCS operations.","Accepted at NeurIPS 2022 Workshop on Tackling Climate Change with
  Machine Learning",,,physics.geo-ph,"['physics.geo-ph', 'cs.AI']","['http://arxiv.org/abs/2212.00669v1', 'http://arxiv.org/pdf/2212.00669v1']",http://arxiv.org/pdf/2212.00669v1
93,http://arxiv.org/abs/2212.01939v1,2022-12-04 22:18:38+00:00,2022-12-04 22:18:38+00:00,Winning the CityLearn Challenge: Adaptive Optimization with Evolutionary Search under Trajectory-based Guidance,"['Vanshaj Khattar', 'Ming Jin']","Modern power systems will have to face difficult challenges in the years to come: frequent blackouts in urban areas caused by high power demand peaks, grid instability exacerbated by intermittent renewable generation, and global climate change amplified by rising carbon emissions. While current practices are growingly inadequate, the path to widespread adoption of artificial intelligence (AI) methods is hindered by missing aspects of trustworthiness. The CityLearn Challenge is an exemplary opportunity for researchers from multiple disciplines to investigate the potential of AI to tackle these pressing issues in the energy domain, collectively modeled as a reinforcement learning (RL) task. Multiple real-world challenges faced by contemporary RL techniques are embodied in the problem formulation. In this paper, we present a novel method using the solution function of optimization as policies to compute actions for sequential decision-making, while notably adapting the parameters of the optimization model from online observations. Algorithmically, this is achieved by an evolutionary algorithm under a novel trajectory-based guidance scheme. Formally, the global convergence property is established. Our agent ranked first in the latest 2021 CityLearn Challenge, being able to achieve superior performance in almost all metrics while maintaining some key aspects of interpretability.",,,,eess.SY,"['eess.SY', 'cs.LG', 'cs.NE', 'cs.SY']","['http://arxiv.org/abs/2212.01939v1', 'http://arxiv.org/pdf/2212.01939v1']",http://arxiv.org/pdf/2212.01939v1
94,http://arxiv.org/abs/2109.00100v4,2021-09-07 13:32:08+00:00,2021-08-31 22:41:14+00:00,Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning,"['Snehalkumar', 'S. Gaikwad', 'Shankar Iyer', 'Dalton Lunga', 'Elizabeth Bondi']","Humanitarian challenges, including natural disasters, food insecurity, climate change, racial and gender violence, environmental crises, the COVID-19 coronavirus pandemic, human rights violations, and forced displacements, disproportionately impact vulnerable communities worldwide. According to UN OCHA, 235 million people will require humanitarian assistance in 2021. Despite these growing perils, there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of at-risk populations. Scattered data science efforts exist to address these challenges, but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy, fairness, interpretability, accountability, transparency, and ethics. Biases in data-driven methods carry the risk of amplifying inequalities in high-stakes policy decisions that impact the livelihood of millions of people. Consequently, proclaimed benefits of data-driven innovations remain inaccessible to policymakers, practitioners, and marginalized communities at the core of humanitarian actions and global development. To help fill this gap, we propose the Data-driven Humanitarian Mapping Research Program, which focuses on developing novel data science methodologies that harness human-machine intelligence for high-stakes public policy and resilience planning.   The proceedings of the 2nd Data-driven Humanitarian Mapping workshop at the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. August 15th, 2021","The proceedings of the 2nd Data-driven Humanitarian Mapping workshop
  at the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
  August 15th, 2021",,,cs.CY,"['cs.CY', 'cs.AI', 'econ.GN', 'q-fin.EC']","['http://arxiv.org/abs/2109.00100v4', 'http://arxiv.org/pdf/2109.00100v4']",http://arxiv.org/pdf/2109.00100v4
95,http://arxiv.org/abs/2109.00435v3,2021-09-07 13:31:02+00:00,2021-09-01 15:30:25+00:00,Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning,"['Snehalkumar', 'S. Gaikwad', 'Shankar Iyer', 'Dalton Lunga', 'Yu-Ru Lin']","Humanitarian challenges, including natural disasters, food insecurity, climate change, racial and gender violence, environmental crises, the COVID-19 coronavirus pandemic, human rights violations, and forced displacements, disproportionately impact vulnerable communities worldwide. According to UN OCHA, 235 million people will require humanitarian assistance in 2021 . Despite these growing perils, there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of at-risk populations. Scattered data science efforts exist to address these challenges, but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy, fairness, interpretability, accountability, transparency, and ethics. Biases in data-driven methods carry the risk of amplifying inequalities in high-stakes policy decisions that impact the livelihood of millions of people. Consequently, proclaimed benefits of data-driven innovations remain inaccessible to policymakers, practitioners, and marginalized communities at the core of humanitarian actions and global development. To help fill this gap, we propose the Data-driven Humanitarian Mapping Research Program, which focuses on developing novel data science methodologies that harness human-machine intelligence for high-stakes public policy and resilience planning.   The proceedings of the 1st Data-driven Humanitarian Mapping workshop at the 26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, August 24th, 2020.","The proceedings of the 1st Data-driven Humanitarian Mapping workshop
  at the 26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",,,cs.CY,"['cs.CY', 'cs.AI', 'econ.GN', 'q-fin.EC']","['http://arxiv.org/abs/2109.00435v3', 'http://arxiv.org/pdf/2109.00435v3']",http://arxiv.org/pdf/2109.00435v3
96,http://arxiv.org/abs/2209.01191v1,2022-08-31 18:24:39+00:00,2022-08-31 18:24:39+00:00,Long-term hail risk assessment with deep neural networks,"['Ivan Lukyanenko', 'Mikhail Mozikov', 'Yury Maximov', 'Ilya Makarov']","Hail risk assessment is necessary to estimate and reduce damage to crops, orchards, and infrastructure. Also, it helps to estimate and reduce consequent losses for businesses and, particularly, insurance companies. But hail forecasting is challenging. Data used for designing models for this purpose are tree-dimensional geospatial time series. Hail is a very local event with respect to the resolution of available datasets. Also, hail events are rare - only 1% of targets in observations are marked as ""hail"". Models for nowcasting and short-term hail forecasts are improving. Introducing machine learning models to the meteorology field is not new. There are also various climate models reflecting possible scenarios of climate change in the future. But there are no machine learning models for data-driven forecasting of changes in hail frequency for a given area.   The first possible approach for the latter task is to ignore spatial and temporal structure and develop a model capable of classifying a given vertical profile of meteorological variables as favorable to hail formation or not. Although such an approach certainly neglects important information, it is very light weighted and easily scalable because it treats observations as independent from each other. The more advanced approach is to design a neural network capable to process geospatial data. Our idea here is to combine convolutional layers responsible for the processing of spatial data with recurrent neural network blocks capable to work with temporal structure.   This study compares two approaches and introduces a model suitable for the task of forecasting changes in hail frequency for ongoing decades.",,,,physics.ao-ph,"['physics.ao-ph', 'cs.LG']","['http://arxiv.org/abs/2209.01191v1', 'http://arxiv.org/pdf/2209.01191v1']",http://arxiv.org/pdf/2209.01191v1
97,http://arxiv.org/abs/2002.11485v1,2020-02-23 13:00:52+00:00,2020-02-23 13:00:52+00:00,"A machine-learning software-systems approach to capture social, regulatory, governance, and climate problems",['Christopher A. Tucker'],"This paper will discuss the role of an artificially-intelligent computer system as critique-based, implicit-organizational, and an inherently necessary device, deployed in synchrony with parallel governmental policy, as a genuine means of capturing nation-population complexity in quantitative form, public contentment in societal-cooperative economic groups, regulatory proposition, and governance-effectiveness domains. It will discuss a solution involving a well-known algorithm and proffer an improved mechanism for knowledge-representation, thereby increasing range of utility, scope of influence (in terms of differentiating class sectors) and operational efficiency. It will finish with a discussion of these and other historical implications.","7 pages, 1 figure, 1 table",,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/2002.11485v1', 'http://arxiv.org/pdf/2002.11485v1']",http://arxiv.org/pdf/2002.11485v1
98,http://arxiv.org/abs/2202.03407v2,2022-09-05 22:05:39+00:00,2022-02-07 18:47:15+00:00,Investigating the fidelity of explainable artificial intelligence methods for applications of convolutional neural networks in geoscience,"['Antonios Mamalakis', 'Elizabeth A. Barnes', 'Imme Ebert-Uphoff']","Convolutional neural networks (CNNs) have recently attracted great attention in geoscience due to their ability to capture non-linear system behavior and extract predictive spatiotemporal patterns. Given their black-box nature however, and the importance of prediction explainability, methods of explainable artificial intelligence (XAI) are gaining popularity as a means to explain the CNN decision-making strategy. Here, we establish an intercomparison of some of the most popular XAI methods and investigate their fidelity in explaining CNN decisions for geoscientific applications. Our goal is to raise awareness of the theoretical limitations of these methods and gain insight into the relative strengths and weaknesses to help guide best practices. The considered XAI methods are first applied to an idealized attribution benchmark, where the ground truth of explanation of the network is known a priori, to help objectively assess their performance. Secondly, we apply XAI to a climate-related prediction setting, namely to explain a CNN that is trained to predict the number of atmospheric rivers in daily snapshots of climate simulations. Our results highlight several important issues of XAI methods (e.g., gradient shattering, inability to distinguish the sign of attribution, ignorance to zero input) that have previously been overlooked in our field and, if not considered cautiously, may lead to a distorted picture of the CNN decision-making strategy. We envision that our analysis will motivate further investigation into XAI fidelity and will help towards a cautious implementation of XAI in geoscience, which can lead to further exploitation of CNNs and deep learning for prediction problems.",,"2022, Artificial Intelligence for the Earth Systems (AMS)",10.1175/AIES-D-22-0012.1,physics.geo-ph,"['physics.geo-ph', 'cs.AI', 'cs.LG']","['http://dx.doi.org/10.1175/AIES-D-22-0012.1', 'http://arxiv.org/abs/2202.03407v2', 'http://arxiv.org/pdf/2202.03407v2']",http://arxiv.org/pdf/2202.03407v2
99,http://arxiv.org/abs/1902.09069v1,2019-02-25 02:48:54+00:00,2019-02-25 02:48:54+00:00,Automatic Detection and Compression for Passive Acoustic Monitoring of the African Forest Elephant,"['Johan Bjorck', 'Brendan H. Rappazzo', 'Di Chen', 'Richard Bernstein', 'Peter H. Wrege', 'Carla P. Gomes']","In this work, we consider applying machine learning to the analysis and compression of audio signals in the context of monitoring elephants in sub-Saharan Africa. Earth's biodiversity is increasingly under threat by sources of anthropogenic change (e.g. resource extraction, land use change, and climate change) and surveying animal populations is critical for developing conservation strategies. However, manually monitoring tropical forests or deep oceans is intractable. For species that communicate acoustically, researchers have argued for placing audio recorders in the habitats as a cost-effective and non-invasive method, a strategy known as passive acoustic monitoring (PAM). In collaboration with conservation efforts, we construct a large labeled dataset of passive acoustic recordings of the African Forest Elephant via crowdsourcing, compromising thousands of hours of recordings in the wild. Using state-of-the-art techniques in artificial intelligence we improve upon previously proposed methods for passive acoustic monitoring for classification and segmentation. In real-time detection of elephant calls, network bandwidth quickly becomes a bottleneck and efficient ways to compress the data are needed. Most audio compression schemes are aimed at human listeners and are unsuitable for low-frequency elephant calls. To remedy this, we provide a novel end-to-end differentiable method for compression of audio signals that can be adapted to acoustic monitoring of any species and dramatically improves over naive coding strategies.",,,,cs.SD,"['cs.SD', 'cs.LG', 'eess.AS']","['http://arxiv.org/abs/1902.09069v1', 'http://arxiv.org/pdf/1902.09069v1']",http://arxiv.org/pdf/1902.09069v1
100,http://arxiv.org/abs/2203.06064v1,2022-03-10 13:32:31+00:00,2022-03-10 13:32:31+00:00,Climate Change & Computer Audition: A Call to Action and Overview on Audio Intelligence to Help Save the Planet,"['Björn W. Schuller', 'Alican Akman', 'Yi Chang', 'Harry Coppock', 'Alexander Gebhard', 'Alexander Kathan', 'Esther Rituerto-González', 'Andreas Triantafyllopoulos', 'Florian B. Pokorny']","Among the seventeen Sustainable Development Goals (SDGs) proposed within the 2030 Agenda and adopted by all the United Nations member states, the 13$^{th}$ SDG is a call for action to combat climate change for a better world. In this work, we provide an overview of areas in which audio intelligence -- a powerful but in this context so far hardly considered technology -- can contribute to overcome climate-related challenges. We categorise potential computer audition applications according to the five elements of earth, water, air, fire, and aether, proposed by the ancient Greeks in their five element theory; this categorisation serves as a framework to discuss computer audition in relation to different ecological aspects. Earth and water are concerned with the early detection of environmental changes and, thus, with the protection of humans and animals, as well as the monitoring of land and aquatic organisms. Aerial audio is used to monitor and obtain information about bird and insect populations. Furthermore, acoustic measures can deliver relevant information for the monitoring and forecasting of weather and other meteorological phenomena. The fourth considered element is fire. Due to the burning of fossil fuels, the resulting increase in CO$_2$ emissions and the associated rise in temperature, fire is used as a symbol for man-made climate change and in this context includes the monitoring of noise pollution, machines, as well as the early detection of wildfires. In all these areas, computer audition can help counteract climate change. Aether then corresponds to the technology itself that makes this possible. This work explores these areas and discusses potential applications, while positioning computer audition in relation to methodological alternatives.",,,,cs.SD,"['cs.SD', 'cs.LG']","['http://arxiv.org/abs/2203.06064v1', 'http://arxiv.org/pdf/2203.06064v1']",http://arxiv.org/pdf/2203.06064v1
101,http://arxiv.org/abs/2006.13847v1,2020-06-24 16:20:12+00:00,2020-06-24 16:20:12+00:00,Crop Yield Prediction Integrating Genotype and Weather Variables Using Deep Learning,"['Johnathon Shook', 'Tryambak Gangopadhyay', 'Linjiang Wu', 'Baskar Ganapathysubramanian', 'Soumik Sarkar', 'Asheesh K. Singh']","Accurate prediction of crop yield supported by scientific and domain-relevant insights, can help improve agricultural breeding, provide monitoring across diverse climatic conditions and thereby protect against climatic challenges to crop production including erratic rainfall and temperature variations. We used historical performance records from Uniform Soybean Tests (UST) in North America spanning 13 years of data to build a Long Short Term Memory - Recurrent Neural Network based model to dissect and predict genotype response in multiple-environments by leveraging pedigree relatedness measures along with weekly weather parameters. Additionally, for providing explainability of the important time-windows in the growing season, we developed a model based on temporal attention mechanism. The combination of these two models outperformed random forest (RF), LASSO regression and the data-driven USDA model for yield prediction. We deployed this deep learning framework as a 'hypotheses generation tool' to unravel GxExM relationships. Attention-based time series models provide a significant advancement in interpretability of yield prediction models. The insights provided by explainable models are applicable in understanding how plant breeding programs can adapt their approaches for global climate change, for example identification of superior varieties for commercial release, intelligent sampling of testing environments in variety development, and integrating weather parameters for a targeted breeding approach. Using DL models as hypothesis generation tools will enable development of varieties with plasticity response in variable climatic conditions. We envision broad applicability of this approach (via conducting sensitivity analysis and ""what-if"" scenarios) for soybean and other crop species under different climatic conditions.","18 pages, 9 figures",,10.1371/journal.pone.0252402,cs.LG,"['cs.LG', 'stat.ML']","['http://dx.doi.org/10.1371/journal.pone.0252402', 'http://arxiv.org/abs/2006.13847v1', 'http://arxiv.org/pdf/2006.13847v1']",http://arxiv.org/pdf/2006.13847v1
102,http://arxiv.org/abs/1604.00681v2,2017-02-12 18:38:35+00:00,2016-04-03 19:58:18+00:00,Experimental Assessment of Aggregation Principles in Argumentation-enabled Collective Intelligence,"['Edmond Awad', 'Jean-François Bonnefon', 'Martin Caminada', 'Thomas Malone', 'Iyad Rahwan']","On the Web, there is always a need to aggregate opinions from the crowd (as in posts, social networks, forums, etc.). Different mechanisms have been implemented to capture these opinions such as ""Like"" in Facebook, ""Favorite"" in Twitter, thumbs-up/down, flagging, and so on. However, in more contested domains (e.g. Wikipedia, political discussion, and climate change discussion) these mechanisms are not sufficient since they only deal with each issue independently without considering the relationships between different claims. We can view a set of conflicting arguments as a graph in which the nodes represent arguments and the arcs between these nodes represent the defeat relation. A group of people can then collectively evaluate such graphs. To do this, the group must use a rule to aggregate their individual opinions about the entire argument graph. Here, we present the first experimental evaluation of different principles commonly employed by aggregation rules presented in the literature. We use randomized controlled experiments to investigate which principles people consider better at aggregating opinions under different conditions. Our analysis reveals a number of factors, not captured by traditional formal models, that play an important role in determining the efficacy of aggregation. These results help bring formal models of argumentation closer to real-world application.",,"ACM Transactions on Internet Technology (TOIT), 17(3), 29 (2017)",10.1145/3053371,cs.AI,['cs.AI'],"['http://dx.doi.org/10.1145/3053371', 'http://arxiv.org/abs/1604.00681v2', 'http://arxiv.org/pdf/1604.00681v2']",http://arxiv.org/pdf/1604.00681v2
103,http://arxiv.org/abs/1605.09370v1,2016-05-30 19:57:56+00:00,2016-05-30 19:57:56+00:00,Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data,"['Krzysztof Chalupka', 'Tobias Bischoff', 'Pietro Perona', 'Frederick Eberhardt']","We show that the climate phenomena of El Nino and La Nina arise naturally as states of macro-variables when our recent causal feature learning framework (Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind (ZW) and sea surface temperatures (SST) taken over the equatorial band of the Pacific Ocean. The method identifies these unusual climate states on the basis of the relation between ZW and SST patterns without any input about past occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering the SST fields while disregarding their relationship with ZW patterns, or (ii) clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches. Finally, we propose a new robust and scalable alternative to our original algorithm (Chalupka 2016), which circumvents the need for high-dimensional density learning.",Accepted for plenary presentation at UAI 2016,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'physics.ao-ph']","['http://arxiv.org/abs/1605.09370v1', 'http://arxiv.org/pdf/1605.09370v1']",http://arxiv.org/pdf/1605.09370v1
104,http://arxiv.org/abs/2205.00202v1,2022-04-30 08:35:57+00:00,2022-04-30 08:35:57+00:00,Explainable Artificial Intelligence for Bayesian Neural Networks: Towards trustworthy predictions of ocean dynamics,"['Mariana C. A. Clare', 'Maike Sonnewald', 'Redouane Lguensat', 'Julie Deshayes', 'Venkatramani Balaji']","The trustworthiness of neural networks is often challenged because they lack the ability to express uncertainty and explain their skill. This can be problematic given the increasing use of neural networks in high stakes decision-making such as in climate change applications. We address both issues by successfully implementing a Bayesian Neural Network (BNN), where parameters are distributions rather than deterministic, and applying novel implementations of explainable AI (XAI) techniques. The uncertainty analysis from the BNN provides a comprehensive overview of the prediction more suited to practitioners' needs than predictions from a classical neural network. Using a BNN means we can calculate the entropy (i.e. uncertainty) of the predictions and determine if the probability of an outcome is statistically significant. To enhance trustworthiness, we also spatially apply the two XAI techniques of Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanation (SHAP) values. These XAI methods reveal the extent to which the BNN is suitable and/or trustworthy. Using two techniques gives a more holistic view of BNN skill and its uncertainty, as LRP considers neural network parameters, whereas SHAP considers changes to outputs. We verify these techniques using comparison with intuition from physical theory. The differences in explanation identify potential areas where new physical theory guided studies are needed.","25 pages, 11 figures",,,physics.ao-ph,"['physics.ao-ph', 'cs.LG', '68T07, 86A05, 86A08', 'I.2.6; J.2']","['http://arxiv.org/abs/2205.00202v1', 'http://arxiv.org/pdf/2205.00202v1']",http://arxiv.org/pdf/2205.00202v1
105,http://arxiv.org/abs/2209.07928v1,2022-09-06 18:32:08+00:00,2022-09-06 18:32:08+00:00,The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,"['Paulo Pirozelli', 'Ais B. R. Castro', 'Ana Luiza C. de Oliveira', 'André S. Oliveira', 'Flávio N. Cação', 'Igor C. Silveira', 'João G. M. Campos', 'Laura C. Motheo', 'Leticia F. Figueiredo', 'Lucas F. A. O. Pellicer', 'Marcelo A. José', 'Marcos M. José', 'Pedro de M. Ligabue', 'Ricardo S. Grava', 'Rodrigo M. Tavares', 'Vinícius B. Matos', 'Yan V. Sym', 'Anna H. R. Costa', 'Anarosa A. F. Brandão', 'Denis D. Mauá', 'Fabio G. Cozman', 'Sarajane M. Peres']","We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",,"AI: Modeling Oceans and Climate Change (IJCAI-ECAI), 2022",,cs.AI,"['cs.AI', 'cs.CL', 'cs.SY', 'eess.SY']","['http://arxiv.org/abs/2209.07928v1', 'http://arxiv.org/pdf/2209.07928v1']",http://arxiv.org/pdf/2209.07928v1
106,http://arxiv.org/abs/2010.14300v1,2020-10-27 14:02:00+00:00,2020-10-27 14:02:00+00:00,Ice Monitoring in Swiss Lakes from Optical Satellites and Webcams using Machine Learning,"['Manu Tom', 'Rajanie Prabha', 'Tianyu Wu', 'Emmanuel Baltsavias', 'Laura Leal-Taixe', 'Konrad Schindler']","Continuous observation of climate indicators, such as trends in lake freezing, is important to understand the dynamics of the local and global climate system. Consequently, lake ice has been included among the Essential Climate Variables (ECVs) of the Global Climate Observing System (GCOS), and there is a need to set up operational monitoring capabilities. Multi-temporal satellite images and publicly available webcam streams are among the viable data sources to monitor lake ice. In this work we investigate machine learning-based image analysis as a tool to determine the spatio-temporal extent of ice on Swiss Alpine lakes as well as the ice-on and ice-off dates, from both multispectral optical satellite images (VIIRS and MODIS) and RGB webcam images. We model lake ice monitoring as a pixel-wise semantic segmentation problem, i.e., each pixel on the lake surface is classified to obtain a spatially explicit map of ice cover. We show experimentally that the proposed system produces consistently good results when tested on data from multiple winters and lakes. Our satellite-based method obtains mean Intersection-over-Union (mIoU) scores >93%, for both sensors. It also generalises well across lakes and winters with mIoU scores >78% and >80% respectively. On average, our webcam approach achieves mIoU values of 87% (approx.) and generalisation scores of 71% (approx.) and 69% (approx.) across different cameras and winters respectively. Additionally, we put forward a new benchmark dataset of webcam images (Photi-LakeIce) which includes data from two winters and three cameras.",Accepted for publication in MDPI Remote Sensing Journal,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2010.14300v1', 'http://arxiv.org/pdf/2010.14300v1']",http://arxiv.org/pdf/2010.14300v1
107,http://arxiv.org/abs/2112.08453v1,2021-12-15 19:57:38+00:00,2021-12-15 19:57:38+00:00,"The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences","['Amy McGovern', 'Imme Ebert-Uphoff', 'David John Gagne II', 'Ann Bostrom']","Given the growing use of Artificial Intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help {\it reduce} climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences.",,,10.1017/eds.2022.5,cs.CY,"['cs.CY', 'cs.AI', 'cs.LG', 'K.4.0; I.2.0']","['http://dx.doi.org/10.1017/eds.2022.5', 'http://arxiv.org/abs/2112.08453v1', 'http://arxiv.org/pdf/2112.08453v1']",http://arxiv.org/pdf/2112.08453v1
108,http://arxiv.org/abs/1711.04710v2,2017-11-17 17:31:54+00:00,2017-11-13 17:17:29+00:00,Spatio-Temporal Data Mining: A Survey of Problems and Methods,"['Gowtham Atluri', 'Anuj Karpatne', 'Vipin Kumar']","Large volumes of spatio-temporal data are increasingly collected and studied in diverse domains including, climate science, social sciences, neuroscience, epidemiology, transportation, mobile health, and Earth sciences. Spatio-temporal data differs from relational data for which computational approaches are developed in the data mining community for multiple decades, in that both spatial and temporal attributes are available in addition to the actual measurements/attributes. The presence of these attributes introduces additional challenges that needs to be dealt with. Approaches for mining spatio-temporal data have been studied for over a decade in the data mining community. In this article we present a broad survey of this relatively young field of spatio-temporal data mining. We discuss different types of spatio-temporal data and the relevant data mining questions that arise in the context of analyzing each of these datasets. Based on the nature of the data mining problem studied, we classify literature on spatio-temporal data mining into six major categories: clustering, predictive learning, change detection, frequent pattern mining, anomaly detection, and relationship mining. We discuss the various forms of spatio-temporal data mining problems in each of these categories.",Accepted for publication at ACM Computing Surveys,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.DB']","['http://arxiv.org/abs/1711.04710v2', 'http://arxiv.org/pdf/1711.04710v2']",http://arxiv.org/pdf/1711.04710v2
109,http://arxiv.org/abs/1907.01821v1,2019-07-03 09:53:05+00:00,2019-07-03 09:53:05+00:00,Super-Resolution of PROBA-V Images Using Convolutional Neural Networks,"['Marcus Märtens', 'Dario Izzo', 'Andrej Krzic', 'Daniël Cox']","ESA's PROBA-V Earth observation satellite enables us to monitor our planet at a large scale, studying the interaction between vegetation and climate and provides guidance for important decisions on our common global future. However, the interval at which high resolution images are recorded spans over several days, in contrast to the availability of lower resolution images which is often daily. We collect an extensive dataset of both, high and low resolution images taken by PROBA-V instruments during monthly periods to investigate Multi Image Super-resolution, a technique to merge several low resolution images to one image of higher quality. We propose a convolutional neural network that is able to cope with changes in illumination, cloud coverage and landscape features which are challenges introduced by the fact that the different images are taken over successive satellite passages over the same region. Given a bicubic upscaling of low resolution images taken under optimal conditions, we find the Peak Signal to Noise Ratio of the reconstructed image of the network to be higher for a large majority of different scenes. This shows that applied machine learning has the potential to enhance large amounts of previously collected earth observation data during multiple satellite passes.","To appear in Special Issue on Applications of Artificial Intelligence
  in Aerospace Engineering in the Journal ""Astrodynamics""",,,cs.CV,"['cs.CV', 'eess.IV']","['http://arxiv.org/abs/1907.01821v1', 'http://arxiv.org/pdf/1907.01821v1']",http://arxiv.org/pdf/1907.01821v1
110,http://arxiv.org/abs/1907.13305v2,2019-09-20 11:08:16+00:00,2019-07-31 04:48:56+00:00,An Implementation of a Non-monotonic Logic in an Embedded Computer for a Motor-glider,"['José Luis Vilchis Medina', 'Pierre Siegel', 'Vincent Risch', 'Andrei Doncescu']","In this article we present an implementation of non-monotonic reasoning in an embedded system. As a part of an autonomous motor-glider, it simulates piloting decisions of an airplane. A real pilot must take care not only about the information arising from the cockpit (airspeed, altitude, variometer, compass...) but also from outside the cabin. Throughout a flight, a pilot is constantly in communication with the control tower to follow orders, because there is an airspace regulation to respect. In addition, if the control tower sends orders while the pilot has an emergency, he may have to violate these orders and airspace regulations to solve his problem (e.g. emergency landing). On the other hand, climate changes constantly (wind, snow, hail..) and can affect the sensors. All these cases easily lead to contradictions. Switching to reasoning under uncertainty, a pilot must make decisions to carry out a flight. The objective of this implementation is to validate a non-monotonic model which allows to solve the question of incomplete and contradictory information. We formalize the problem using default logic, a non-monotonic logic which allows to find fixed-points in the face of contradictions. For the implementation, the Prolog language is used in an embedded computer running at 1 GHz single core with 512 Mb of RAM and 0.8 watts of energy consumption.","In Proceedings ICLP 2019, arXiv:1909.07646","EPTCS 306, 2019, pp. 323-329",10.4204/EPTCS.306.37,cs.AI,['cs.AI'],"['http://dx.doi.org/10.4204/EPTCS.306.37', 'http://arxiv.org/abs/1907.13305v2', 'http://arxiv.org/pdf/1907.13305v2']",http://arxiv.org/pdf/1907.13305v2
111,http://arxiv.org/abs/2006.01601v1,2020-05-28 06:54:43+00:00,2020-05-28 06:54:43+00:00,Optimizing carbon tax for decentralized electricity markets using an agent-based model,"['Alexander J. M. Kell', 'A. Stephen McGough', 'Matthew Forshaw']","Averting the effects of anthropogenic climate change requires a transition from fossil fuels to low-carbon technology. A way to achieve this is to decarbonize the electricity grid. However, further efforts must be made in other fields such as transport and heating for full decarbonization. This would reduce carbon emissions due to electricity generation, and also help to decarbonize other sources such as automotive and heating by enabling a low-carbon alternative. Carbon taxes have been shown to be an efficient way to aid in this transition. In this paper, we demonstrate how to to find optimal carbon tax policies through a genetic algorithm approach, using the electricity market agent-based model ElecSim. To achieve this, we use the NSGA-II genetic algorithm to minimize average electricity price and relative carbon intensity of the electricity mix. We demonstrate that it is possible to find a range of carbon taxes to suit differing objectives. Our results show that we are able to minimize electricity cost to below \textsterling10/MWh as well as carbon intensity to zero in every case. In terms of the optimal carbon tax strategy, we found that an increasing strategy between 2020 and 2035 was preferable. Each of the Pareto-front optimal tax strategies are at least above \textsterling81/tCO2 for every year. The mean carbon tax strategy was \textsterling240/tCO2.","Accepted at The Eleventh ACM International Conference on Future
  Energy Systems (e-Energy'20) AMLIES Workshop",,10.1145/3396851.3402369,eess.SY,"['eess.SY', 'cs.AI', 'cs.LG', 'cs.SY']","['http://dx.doi.org/10.1145/3396851.3402369', 'http://arxiv.org/abs/2006.01601v1', 'http://arxiv.org/pdf/2006.01601v1']",http://arxiv.org/pdf/2006.01601v1
112,http://arxiv.org/abs/2007.07352v1,2020-07-09 13:19:13+00:00,2020-07-09 13:19:13+00:00,"Degrees of individual and groupwise backward and forward responsibility in extensive-form games with ambiguity, and their application to social choice problems","['Jobst Heitzig', 'Sarah Hiller']","Many real-world situations of ethical relevance, in particular those of large-scale social choice such as mitigating climate change, involve not only many agents whose decisions interact in complicated ways, but also various forms of uncertainty, including quantifiable risk and unquantifiable ambiguity. In such problems, an assessment of individual and groupwise moral responsibility for ethically undesired outcomes or their responsibility to avoid such is challenging and prone to the risk of under- or overdetermination of responsibility. In contrast to existing approaches based on strict causation or certain deontic logics that focus on a binary classification of `responsible' vs `not responsible', we here present several different quantitative responsibility metrics that assess responsibility degrees in units of probability. For this, we use a framework based on an adapted version of extensive-form game trees and an axiomatic approach that specifies a number of potentially desirable properties of such metrics, and then test the developed candidate metrics by their application to a number of paradigmatic social choice situations. We find that while most properties one might desire of such responsibility metrics can be fulfilled by some variant, an optimal metric that clearly outperforms others has yet to be found.","38 pages, 6 figures, 2 tables",,,econ.TH,"['econ.TH', 'cs.AI', '91B06, 91A35, 90B50, 91A18', 'F.4.3; G.3']","['http://arxiv.org/abs/2007.07352v1', 'http://arxiv.org/pdf/2007.07352v1']",http://arxiv.org/pdf/2007.07352v1
113,http://arxiv.org/abs/2011.06125v4,2022-09-24 19:03:41+00:00,2020-11-11 23:55:33+00:00,Hurricane Forecasting: A Novel Multimodal Machine Learning Framework,"['Léonard Boussioux', 'Cynthia Zeng', 'Théo Guénais', 'Dimitris Bertsimas']","This paper describes a novel machine learning (ML) framework for tropical cyclone intensity and track forecasting, combining multiple ML techniques and utilizing diverse data sources. Our multimodal framework, called Hurricast, efficiently combines spatial-temporal data with statistical data by extracting features with deep-learning encoder-decoder architectures and predicting with gradient-boosted trees. We evaluate our models in the North Atlantic and Eastern Pacific basins on 2016-2019 for 24-hour lead time track and intensity forecasts and show they achieve comparable mean absolute error and skill to current operational forecast models while computing in seconds. Furthermore, the inclusion of Hurricast into an operational forecast consensus model could improve over the National Hurricane Center's official forecast, thus highlighting the complementary properties with existing approaches. In summary, our work demonstrates that utilizing machine learning techniques to combine different data sources can lead to new opportunities in tropical cyclone forecasting.","Published by the AMS' Weather and Forecasting journal; Spotlight talk
  at NeurIPS 2021, Tackling Climate Change with AI ;
  https://journals.ametsoc.org/view/journals/wefo/37/6/WAF-D-21-0091.1.xml","2022, Weather and Forecasting, 37(6), 817-831",10.1175/WAF-D-21-0091.1,cs.LG,"['cs.LG', 'cs.AI', 'physics.ao-ph']","['http://dx.doi.org/10.1175/WAF-D-21-0091.1', 'http://arxiv.org/abs/2011.06125v4', 'http://arxiv.org/pdf/2011.06125v4']",http://arxiv.org/pdf/2011.06125v4
114,http://arxiv.org/abs/2011.07227v1,2020-11-14 06:20:21+00:00,2020-11-14 06:20:21+00:00,OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep Learning on Remotely Sensed Imagery,"['Hao Sheng', 'Jeremy Irvin', 'Sasankh Munukutla', 'Shawn Zhang', 'Christopher Cross', 'Kyle Story', 'Rose Rustowicz', 'Cooper Elsworth', 'Zutao Yang', 'Mark Omara', 'Ritesh Gautam', 'Robert B. Jackson', 'Andrew Y. Ng']","At least a quarter of the warming that the Earth is experiencing today is due to anthropogenic methane emissions. There are multiple satellites in orbit and planned for launch in the next few years which can detect and quantify these emissions; however, to attribute methane emissions to their sources on the ground, a comprehensive database of the locations and characteristics of emission sources worldwide is essential. In this work, we develop deep learning algorithms that leverage freely available high-resolution aerial imagery to automatically detect oil and gas infrastructure, one of the largest contributors to global methane emissions. We use the best algorithm, which we call OGNet, together with expert review to identify the locations of oil refineries and petroleum terminals in the U.S. We show that OGNet detects many facilities which are not present in four standard public datasets of oil and gas infrastructure. All detected facilities are associated with characteristics known to contribute to methane emissions, including the infrastructure type and the number of storage tanks. The data curated and produced in this study is freely available at http://stanfordmlgroup.github.io/projects/ognet .","Tackling Climate Change with Machine Learning at NeurIPS 2020
  (Spotlight talk)",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2011.07227v1', 'http://arxiv.org/pdf/2011.07227v1']",http://arxiv.org/pdf/2011.07227v1
115,http://arxiv.org/abs/2106.03242v1,2021-06-06 20:42:00+00:00,2021-06-06 20:42:00+00:00,Highlighting the Importance of Reducing Research Bias and Carbon Emissions in CNNs,"['Ahmed Badar', 'Arnav Varma', 'Adrian Staniec', 'Mahmoud Gamal', 'Omar Magdy', 'Haris Iqbal', 'Elahe Arani', 'Bahram Zonooz']","Convolutional neural networks (CNNs) have become commonplace in addressing major challenges in computer vision. Researchers are not only coming up with new CNN architectures but are also researching different techniques to improve the performance of existing architectures. However, there is a tendency to over-emphasize performance improvement while neglecting certain important variables such as simplicity, versatility, the fairness of comparisons, and energy efficiency. Overlooking these variables in architectural design and evaluation has led to research bias and a significantly negative environmental impact. Furthermore, this can undermine the positive impact of research in using deep learning models to tackle climate change. Here, we perform an extensive and fair empirical study of a number of proposed techniques to gauge the utility of each technique for segmentation and classification. Our findings restate the importance of favoring simplicity over complexity in model design (Occam's Razor). Furthermore, our results indicate that simple standardized practices can lead to a significant reduction in environmental impact with little drop in performance. We highlight that there is a need to rethink the design and evaluation of CNNs to alleviate the issue of research bias and carbon emissions.",,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2106.03242v1', 'http://arxiv.org/pdf/2106.03242v1']",http://arxiv.org/pdf/2106.03242v1
116,http://arxiv.org/abs/2107.08369v4,2021-10-25 15:49:50+00:00,2021-07-18 05:42:10+00:00,Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning,"['Sayak Paul', 'Siddha Ganju']","Floods wreak havoc throughout the world, causing billions of dollars in damages, and uprooting communities, ecosystems and economies. The NASA Impact Flood Detection competition tasked participants with predicting flooded pixels after training with synthetic aperture radar (SAR) images in a supervised setting. We propose a semi-supervised learning pseudo-labeling scheme that derives confidence estimates from U-Net ensembles, progressively improving accuracy. Concretely, we use a cyclical approach involving multiple stages (1) training an ensemble model of multiple U-Net architectures with the provided high confidence hand-labeled data and, generated pseudo labels or low confidence labels on the entire unlabeled test dataset, and then, (2) filter out quality generated labels and, (3) combine the generated labels with the previously available high confidence hand-labeled dataset. This assimilated dataset is used for the next round of training ensemble models and the cyclical process is repeated until the performance improvement plateaus. We post process our results with Conditional Random Fields. Our approach sets a new state-of-the-art on the Sentinel-1 dataset with 0.7654 IoU, an impressive improvement over the 0.60 IoU baseline. Our method, which we release with all the code and models, can also be used as an open science benchmark for the Sentinel-1 dataset.","Equal authorship. Accepted to the Tackling Climate Change with
  Machine Learning workshop at NeurIPS 2021. Code and models are available at
  https://git.io/JW3P8",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2107.08369v4', 'http://arxiv.org/pdf/2107.08369v4']",http://arxiv.org/pdf/2107.08369v4
117,http://arxiv.org/abs/2108.00887v2,2021-08-09 14:41:22+00:00,2021-07-29 18:22:34+00:00,A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs,"['Karla Saldana Ochoa', 'Tina Comes']","Along with climate change, more frequent extreme events, such as flooding and tropical cyclones, threaten the livelihoods and wellbeing of poor and vulnerable populations. One of the most immediate needs of people affected by a disaster is finding shelter. While the proliferation of data on disasters is already helping to save lives, identifying damages in buildings, assessing shelter needs, and finding appropriate places to establish emergency shelters or settlements require a wide range of data to be combined rapidly. To address this gap and make a headway in comprehensive assessments, this paper proposes a machine learning workflow that aims to fuse and rapidly analyse multimodal data. This workflow is built around open and online data to ensure scalability and broad accessibility. Based on a database of 19 characteristics for more than 200 disasters worldwide, a fusion approach at the decision level was used. This technique allows the collected multimodal data to share a common semantic space that facilitates the prediction of individual variables. Each fused numerical vector was fed into an unsupervised clustering algorithm called Self-Organizing-Maps (SOM). The trained SOM serves as a predictor for future cases, allowing predicting consequences such as total deaths, total people affected, and total damage, and provides specific recommendations for assessments in the shelter and housing sector. To achieve such prediction, a satellite image from before the disaster and the geographic and demographic conditions are shown to the trained model, which achieved a prediction accuracy of 62 %",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2108.00887v2', 'http://arxiv.org/pdf/2108.00887v2']",http://arxiv.org/pdf/2108.00887v2
118,http://arxiv.org/abs/2108.08052v2,2021-11-02 18:13:12+00:00,2021-08-18 09:00:24+00:00,Moser Flow: Divergence-based Generative Modeling on Manifolds,"['Noam Rozen', 'Aditya Grover', 'Maximilian Nickel', 'Yaron Lipman']","We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.",,,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2108.08052v2', 'http://arxiv.org/pdf/2108.08052v2']",http://arxiv.org/pdf/2108.08052v2
119,http://arxiv.org/abs/2109.05083v1,2021-09-10 19:30:37+00:00,2021-09-10 19:30:37+00:00,"Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks",['Samarth Shah'],"Wildfires are uncontrolled fires in the environment that can be caused by humans or nature. In 2020 alone, wildfires in California have burned 4.2 million acres, damaged 10,500 buildings or structures, and killed more than 31 people, exacerbated by climate change and a rise in average global temperatures. This also means there has been an increase in the costs of extinguishing these treacherous wildfires. The objective of the research is to detect forest fires in their earlier stages to prevent them from spreading, prevent them from causing damage to a variety of things, and most importantly, reduce or eliminate the chances of someone dying from a wildfire. A fire detection system should be efficient and accurate with respect to extinguishing wildfires in their earlier stages to prevent the spread of them along with their consequences. Computer Vision is potentially a more reliable, fast, and widespread method we need. The current research in the field of preliminary fire detection has several problems related to unrepresentative data being used to train models and their existing varied amounts of label imbalance in the classes of their dataset. We propose a more representative and evenly distributed data through better settings, lighting, atmospheres, etc., and class distribution in the entire dataset. After thoroughly examining the results of this research, it can be inferred that they supported the datasets strengths by being a viable resource when tested in the real world on unfamiliar data. This is evident since as the model trains on the dataset, it is able to generalize on it, hence confirming this is a viable Machine Learning setting that has practical impact.","12 pages, 7 figures",,,cs.CV,"['cs.CV', 'cs.AI', '68T45', 'I.4.1; I.4.6; I.4.7; E.1; I.2.10']","['http://arxiv.org/abs/2109.05083v1', 'http://arxiv.org/pdf/2109.05083v1']",http://arxiv.org/pdf/2109.05083v1
120,http://arxiv.org/abs/2110.03585v1,2021-10-07 16:09:01+00:00,2021-10-07 16:09:01+00:00,To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and Autoencoders,"['Michael Bosello', 'Carlo Falcomer', 'Claudio Rossi', 'Giovanni Pau']","Electric Vehicles (EVs) are spreading fast as they promise to provide better performances and comfort, but above all, to help facing climate change. Despite their success, their cost is still a challenge. One of the most expensive components of EVs is lithium-ion batteries, which became the standard for energy storage in a wide range of applications. Precisely estimating the Remaining Useful Life (RUL) of battery packs can open to their reuse and thus help to reduce the cost of EVs and improve sustainability. A correct RUL estimation can be used to quantify the residual market value of the battery pack. The customer can then decide to sell the battery when it still has a value, i.e., before it exceeds its end of life of the target application and can still be reused in a second domain without compromising safety and reliability. In this paper, we propose to use a Deep Learning approach based on LSTMs and Autoencoders to estimate the RUL of li-ion batteries. Compared to what has been proposed so far in the literature, we employ measures to ensure the applicability of the method also in the real deployed application. Such measures include (1) avoid using non-measurable variables as input, (2) employ appropriate datasets with wide variability and different conditions, (3) do not use cycles to define the RUL.",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2110.03585v1', 'http://arxiv.org/pdf/2110.03585v1']",http://arxiv.org/pdf/2110.03585v1
121,http://arxiv.org/abs/2111.01692v2,2021-11-23 10:10:28+00:00,2021-11-02 15:50:01+00:00,Efficient Hierarchical Bayesian Inference for Spatio-temporal Regression Models in Neuroimaging,"['Ali Hashemi', 'Yijing Gao', 'Chang Cai', 'Sanjay Ghosh', 'Klaus-Robert Müller', 'Srikantan S. Nagarajan', 'Stefan Haufe']","Several problems in neuroimaging and beyond require inference on the parameters of multi-task sparse hierarchical regression models. Examples include M/EEG inverse problems, neural encoding models for task-based fMRI analyses, and climate science. In these domains, both the model parameters to be inferred and the measurement noise may exhibit a complex spatio-temporal structure. Existing work either neglects the temporal structure or leads to computationally demanding inference schemes. Overcoming these limitations, we devise a novel flexible hierarchical Bayesian framework within which the spatio-temporal dynamics of model parameters and noise are modeled to have Kronecker product covariance structure. Inference in our framework is based on majorization-minimization optimization and has guaranteed convergence properties. Our highly efficient algorithms exploit the intrinsic Riemannian geometry of temporal autocovariance matrices. For stationary dynamics described by Toeplitz matrices, the theory of circulant embeddings is employed. We prove convex bounding properties and derive update rules of the resulting algorithms. On both synthetic and real neural data from M/EEG, we demonstrate that our methods lead to improved performance.","Accepted to the 35th Conference on Neural Information Processing
  Systems (NeurIPS 2021)",,,stat.ML,"['stat.ML', 'cs.AI', 'cs.LG', 'eess.SP', 'stat.AP']","['http://arxiv.org/abs/2111.01692v2', 'http://arxiv.org/pdf/2111.01692v2']",http://arxiv.org/pdf/2111.01692v2
122,http://arxiv.org/abs/2112.13845v1,2021-12-24 10:16:28+00:00,2021-12-24 10:16:28+00:00,Raw Produce Quality Detection with Shifted Window Self-Attention,"['Oh Joon Kwon', 'Byungsoo Kim', 'Youngduck Choi']","Global food insecurity is expected to worsen in the coming decades with the accelerated rate of climate change and the rapidly increasing population. In this vein, it is important to remove inefficiencies at every level of food production. The recent advances in deep learning can help reduce such inefficiencies, yet their application has not yet become mainstream throughout the industry, inducing economic costs at a massive scale. To this point, modern techniques such as CNNs (Convolutional Neural Networks) have been applied to RPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's successful debut in the vision among other modalities led us to expect a better performance with these Transformer-based models in RPQD. In this work, we exclusively investigate the recent state-of-the-art Swin (Shifted Windows) Transformer which computes self-attention in both intra- and inter-window fashion. We compare Swin Transformer against CNN models on four RPQD image datasets, each containing different kinds of raw produce: fruits and vegetables, fish, pork, and beef. We observe that Swin Transformer not only achieves better or competitive performance but also is data- and compute-efficient, making it ideal for actual deployment in real-world setting. To the best of our knowledge, this is the first large-scale empirical study on RPQD task, which we hope will gain more attention in future works.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2112.13845v1', 'http://arxiv.org/pdf/2112.13845v1']",http://arxiv.org/pdf/2112.13845v1
123,http://arxiv.org/abs/2201.10526v1,2022-01-24 17:51:42+00:00,2022-01-24 17:51:42+00:00,MonarchNet: Differentiating Monarch Butterflies from Butterflies Species with Similar Phenotypes,['Thomas Y. Chen'],"In recent years, the monarch butterfly's iconic migration patterns have come under threat from a number of factors, from climate change to pesticide use. To track trends in their populations, scientists as well as citizen scientists must identify individuals accurately. This is uniquely key for the study of monarch butterflies because there exist other species of butterfly, such as viceroy butterflies, that are ""look-alikes"" (coined by the Convention on International Trade in Endangered Species of Wild Fauna and Flora), having similar phenotypes. To tackle this problem and to aid in more efficient identification, we present MonarchNet, the first comprehensive dataset consisting of butterfly imagery for monarchs and five look-alike species. We train a baseline deep-learning classification model to serve as a tool for differentiating monarch butterflies and its various look-alikes. We seek to contribute to the study of biodiversity and butterfly ecology by providing a novel method for computational classification of these particular butterfly species. The ultimate aim is to help scientists track monarch butterfly population and migration trends in the most precise and efficient manner possible.","5 pages, 2 figures, Proceedings of NeurIPS 2020 - Learning Meaningful
  Representations of Life (LMRL) Workshop. The FASEB Journal","CVPR 2021 Workshop on CV4Animals (Computer Vision for Animal
  Behavior Tracking and Modeling)",10.1096/fasebj.2021.35.S1.05504,cs.CV,"['cs.CV', 'cs.AI', 'q-bio.PE', 'stat.AP', 'I.4.9; I.2.1; I.2.10']","['http://dx.doi.org/10.1096/fasebj.2021.35.S1.05504', 'http://arxiv.org/abs/2201.10526v1', 'http://arxiv.org/pdf/2201.10526v1']",http://arxiv.org/pdf/2201.10526v1
124,http://arxiv.org/abs/2202.01340v2,2022-07-01 00:11:54+00:00,2022-01-31 23:53:19+00:00,An Artificial Intelligence Dataset for Solar Energy Locations in India,"['Anthony Ortiz', 'Dhaval Negandhi', 'Sagar R Mysorekar', 'Joseph Kiesecker', 'Shivaprakash K Nagaraju', 'Caleb Robinson', 'Priyal Bhatia', 'Aditi Khurana', 'Jane Wang', 'Felipe Oviedo', 'Juan Lavista Ferres']","Rapid development of renewable energy sources, particularly solar photovoltaics (PV), is critical to mitigate climate change. As a result, India has set ambitious goals to install 500 gigawatts of solar energy capacity by 2030. Given the large footprint projected to meet renewables energy targets, the potential for land use conflicts over environmental values is high. To expedite development of solar energy, land use planners will need access to up-to-date and accurate geo-spatial information of PV infrastructure. In this work, we developed a spatially explicit machine learning model to map utility-scale solar projects across India using freely available satellite imagery with a mean accuracy of 92%. Our model predictions were validated by human experts to obtain a dataset of 1363 solar PV farms. Using this dataset, we measure the solar footprint across India and quantified the degree of landcover modification associated with the development of PV infrastructure. Our analysis indicates that over 74% of solar development In India was built on landcover types that have natural ecosystem preservation, or agricultural value.",Accepted for publication in Nature Scientific Data,,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2202.01340v2', 'http://arxiv.org/pdf/2202.01340v2']",http://arxiv.org/pdf/2202.01340v2
125,http://arxiv.org/abs/2203.15558v1,2022-03-28 14:08:49+00:00,2022-03-28 14:08:49+00:00,Wildfire risk forecast: An optimizable fire danger index,"['Eduardo Rodrigues', 'Bianca Zadrozny', 'Campbell Watson']","Wildfire events have caused severe losses in many places around the world and are expected to increase with climate change. Throughout the years many technologies have been developed to identify fire events early on and to simulate fire behavior once they have started. Another particularly helpful technology is fire risk indices, which use weather forcing to make advanced predictions of the risk of fire. Predictions of fire risk indices can be used, for instance, to allocate resources in places with high risk. These indices have been developed over the years as empirical models with parameters that were estimated in lab experiments and field tests. These parameters, however, may not fit well all places where these models are used. In this paper we propose a novel implementation of one index (NFDRS IC) as a differentiable function in which one can optimize its internal parameters via gradient descent. We leverage existing machine learning frameworks (PyTorch) to construct our model. This approach has two benefits: (1) the NFDRS IC parameters can be improved for each region using actual observed fire events, and (2) the internal variables remain intact for interpretations by specialists instead of meaningless hidden layers as in traditional neural networks. In this paper we evaluate our strategy with actual fire events for locations in the USA and Europe.","6 pages, 5 figures",,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2203.15558v1', 'http://arxiv.org/pdf/2203.15558v1']",http://arxiv.org/pdf/2203.15558v1
126,http://arxiv.org/abs/2204.01142v2,2022-04-14 12:31:21+00:00,2022-04-03 19:40:11+00:00,"Proceedings of TDA: Applications of Topological Data Analysis to Data Science, Artificial Intelligence, and Machine Learning Workshop at SDM 2022","['R. W. R. Darling', 'John A. Emanuello', 'Emilie Purvine', 'Ahmad Ridley']","Topological Data Analysis (TDA) is a rigorous framework that borrows techniques from geometric and algebraic topology, category theory, and combinatorics in order to study the ""shape"" of such complex high-dimensional data. Research in this area has grown significantly over the last several years bringing a deeply rooted theory to bear on practical applications in areas such as genomics, natural language processing, medicine, cybersecurity, energy, and climate change. Within some of these areas, TDA has also been used to augment AI and ML techniques.   We believe there is further utility to be gained in this space that can be facilitated by a workshop bringing together experts (both theorists and practitioners) and non-experts. Currently there is an active community of pure mathematicians with research interests in developing and exploring the theoretical and computational aspects of TDA. Applied mathematicians and other practitioners are also present in community but do not represent a majority. This speaks to the primary aim of this workshop which is to grow a wider community of interest in TDA. By fostering meaningful exchanges between these groups, from across the government, academia, and industry, we hope to create new synergies that can only come through building a mutual comprehensive awareness of the problem and solution spaces.",,,,math.AT,"['math.AT', 'cs.CG', 'cs.LG', 'math.CO']","['http://arxiv.org/abs/2204.01142v2', 'http://arxiv.org/pdf/2204.01142v2']",http://arxiv.org/pdf/2204.01142v2
127,http://arxiv.org/abs/2204.03059v2,2022-07-13 17:10:04+00:00,2022-04-03 19:10:00+00:00,Semantic Sensor Network Ontology based Decision Support System for Forest Fire Management,"['Ritesh Chandra', 'Kumar Abhishek', 'Sonali Agarwal', 'Navjot Singh']","The forests are significant assets for every country. When it gets destroyed, it may negatively impact the environment, and forest fire is one of the primary causes. Fire weather indices are widely used to measure fire danger and are used to issue bushfire warnings. It can also be used to predict the demand for emergency management resources. Sensor networks have grown in popularity in data collection and processing capabilities for a variety of applications in industries such as medical, environmental monitoring, home automation etc. Semantic sensor networks can collect various climatic circumstances like wind speed, temperature, and relative humidity. However, estimating fire weather indices is challenging due to the various issues involved in processing the data streams generated by the sensors. Hence, the importance of forest fire detection has increased day by day. The underlying Semantic Sensor Network (SSN) ontologies are built to allow developers to create rules for calculating fire weather indices and also the convert dataset into Resource Description Framework (RDF). This research describes the various steps involved in developing rules for calculating fire weather indices. Besides, this work presents a Web-based mapping interface to help users visualize the changes in fire weather indices over time. With the help of the inference rule, it designed a decision support system using the SSN ontology and query on it through SPARQL. The proposed fire management system acts according to the situation, supports reasoning and the general semantics of the open-world followed by all the ontologies",Ontology and Semantic Modeling,,,cs.AI,"['cs.AI', 'cs.CY']","['http://arxiv.org/abs/2204.03059v2', 'http://arxiv.org/pdf/2204.03059v2']",http://arxiv.org/pdf/2204.03059v2
128,http://arxiv.org/abs/2207.11186v1,2022-07-20 18:37:25+00:00,2022-07-20 18:37:25+00:00,Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images,"['Akshay Iyer', 'Linh Nguyen', 'Shweta Khushu']","Wind energy is expected to be one of the leading ways to achieve the goals of the Paris Agreement but it in turn heavily depends on effective management of its operations and maintenance (O&M) costs. Blade failures account for one-third of all O&M costs thus making accurate detection of blade damages, especially cracks, very important for sustained operations and cost savings. Traditionally, damage inspection has been a completely manual process thus making it subjective, error-prone, and time-consuming. Hence in this work, we bring more objectivity, scalability, and repeatability in our damage inspection process, using deep learning, to miss fewer cracks. We build a deep learning model trained on a large dataset of blade damages, collected by our drone-based inspection, to correctly detect cracks. Our model is already in production and has processed more than a million damages with a recall of 0.96. We also focus on model interpretability using class activation maps to get a peek into the model workings. The model not only performs as good as human experts but also better in certain tricky cases. Thus, in this work, we aim to increase wind energy adoption by decreasing one of its major hurdles - the O\&M costs resulting from missing blade failures like cracks.","NeurIPS 2021 Workshop on Tackling Climate Change with Machine
  Learning",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']","['http://arxiv.org/abs/2207.11186v1', 'http://arxiv.org/pdf/2207.11186v1']",http://arxiv.org/pdf/2207.11186v1
129,http://arxiv.org/abs/2211.03525v1,2022-10-25 07:55:20+00:00,2022-10-25 07:55:20+00:00,Dynamic weights enabled Physics-Informed Neural Network for simulating the mobility of Engineered Nano-particles in a contaminated aquifer,"['Shikhar Nilabh', 'Fidel Grandia']","Numerous polluted groundwater sites across the globe require an active remediation strategy to restore natural environmental conditions and local ecosystem. The Engineered Nano-particles (ENPs) have emerged as an efficient reactive agent for the in-situ degradation of groundwater contaminants. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool for understanding the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets. This work uses a dynamic, weight-enabled Physics-Informed Neural Network (dw-PINN) framework to model the nano-particle behavior within an aquifer. The result from the forward model demonstrates the effective capability of dw-PINN in accurately predicting the ENPs mobility. The model verification step shows that the relative mean square error (MSE) of the predicted ENPs concentration using dw-PINN converges to a minimum value of $1.3{e^{-5}}$. In the subsequent step, the result from the inverse model estimates the governing parameters of ENPs mobility with reasonable accuracy. The research demonstrates the tool's capability to provide predictive insights for developing an efficient groundwater remediation strategy.","5 pages, 3 Figures, Conference paper accepted in NeurIPS 2022
  Workshop: Tackling Climate Change with Machine Learning",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.NA', 'math.NA']","['http://arxiv.org/abs/2211.03525v1', 'http://arxiv.org/pdf/2211.03525v1']",http://arxiv.org/pdf/2211.03525v1
130,http://arxiv.org/abs/2207.06282v1,2022-07-13 15:27:51+00:00,2022-07-13 15:27:51+00:00,DiverGet: A Search-Based Software Testing Approach for Deep Neural Network Quantization Assessment,"['Ahmed Haj Yahmed', 'Houssem Ben Braiek', 'Foutse Khomh', 'Sonia Bouzidi', 'Rania Zaatour']","Quantization is one of the most applied Deep Neural Network (DNN) compression strategies, when deploying a trained DNN model on an embedded system or a cell phone. This is owing to its simplicity and adaptability to a wide range of applications and circumstances, as opposed to specific Artificial Intelligence (AI) accelerators and compilers that are often designed only for certain specific hardware (e.g., Google Coral Edge TPU). With the growing demand for quantization, ensuring the reliability of this strategy is becoming a critical challenge. Traditional testing methods, which gather more and more genuine data for better assessment, are often not practical because of the large size of the input space and the high similarity between the original DNN and its quantized counterpart. As a result, advanced assessment strategies have become of paramount importance. In this paper, we present DiverGet, a search-based testing framework for quantization assessment. DiverGet defines a space of metamorphic relations that simulate naturally-occurring distortions on the inputs. Then, it optimally explores these relations to reveal the disagreements among DNNs of different arithmetic precision. We evaluate the performance of DiverGet on state-of-the-art DNNs applied to hyperspectral remote sensing images. We chose the remote sensing DNNs as they're being increasingly deployed at the edge (e.g., high-lift drones) in critical domains like climate change research and astronomy. Our results show that DiverGet successfully challenges the robustness of established quantization techniques against naturally-occurring shifted data, and outperforms its most recent concurrent, DiffChaser, with a success rate that is (on average) four times higher.","Accepted for publication in The Empirical Software Engineering
  Journal (EMSE)",,10.1007/s10664-022-10202-w,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","['http://dx.doi.org/10.1007/s10664-022-10202-w', 'http://arxiv.org/abs/2207.06282v1', 'http://arxiv.org/pdf/2207.06282v1']",http://arxiv.org/pdf/2207.06282v1
131,http://arxiv.org/abs/1711.04518v1,2017-11-13 10:58:58+00:00,2017-11-13 10:58:58+00:00,A Supervised Learning Concept for Reducing User Interaction in Passenger Cars,"['Marius Stärk', 'Damian Backes', 'Christian Kehl']",In this article an automation system for human-machine-interfaces (HMI) for setpoint adjustment using supervised learning is presented. We use HMIs of multi-modal thermal conditioning systems in passenger cars as example for a complex setpoint selection system. The goal is the reduction of interaction complexity up to full automation. The approach is not limited to climate control applications but can be extended to other setpoint-based HMIs.,"4 pages, 9 figures, concept only",,,cs.SY,"['cs.SY', 'cs.AI', 'cs.HC', 'cs.LG', 'cs.NE']","['http://arxiv.org/abs/1711.04518v1', 'http://arxiv.org/pdf/1711.04518v1']",http://arxiv.org/pdf/1711.04518v1
132,http://arxiv.org/abs/1807.10965v2,2018-07-31 01:08:37+00:00,2018-07-28 18:26:28+00:00,Ontology-Grounded Topic Modeling for Climate Science Research,"['Jennifer Sleeman', 'Tim Finin', 'Milton Halem']","In scientific disciplines where research findings have a strong impact on society, reducing the amount of time it takes to understand, synthesize and exploit the research is invaluable. Topic modeling is an effective technique for summarizing a collection of documents to find the main themes among them and to classify other documents that have a similar mixture of co-occurring words. We show how grounding a topic model with an ontology, extracted from a glossary of important domain phrases, improves the topics generated and makes them easier to understand. We apply and evaluate this method to the climate science domain. The result improves the topics generated and supports faster research understanding, discovery of social networks among researchers, and automatic ontology generation.","To appear in Proc. of Semantic Web for Social Good Workshop of the
  Int. Semantic Web Conf., Oct 2018 and published as part of the book ""Emerging
  Topics in Semantic Technologies. ISWC 2018 Satellite Events"", E. Demidova,
  A.J. Zaveri, E. Simperl (Eds.), ISBN: 978-3-89838-736-1, 2018, AKA Verlag
  Berlin, (edited authors)",,,cs.CL,"['cs.CL', 'cs.AI', 'I.2.4; I.2.6; I.2.7']","['http://arxiv.org/abs/1807.10965v2', 'http://arxiv.org/pdf/1807.10965v2']",http://arxiv.org/pdf/1807.10965v2
133,http://arxiv.org/abs/1901.02039v1,2019-01-07 19:56:19+00:00,2019-01-07 19:56:19+00:00,Spherical CNNs on Unstructured Grids,"['Chiyu ""Max"" Jiang', 'Jingwei Huang', 'Karthik Kashinath', 'Prabhat', 'Philip Marcus', 'Matthias Niessner']","We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.","Accepted as a conference paper at ICLR 2019. Codes available at
  https://github.com/maxjiang93/ugscnn",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/1901.02039v1', 'http://arxiv.org/pdf/1901.02039v1']",http://arxiv.org/pdf/1901.02039v1
134,http://arxiv.org/abs/2203.08753v1,2022-03-16 17:09:49+00:00,2022-03-16 17:09:49+00:00,Behaviour in social media for floods and heat waves in disaster response via Artificial Intelligence,"['Victor Ponce-López', 'Catalina Spataru']","This paper analyses social media data in multiple disaster-related collections of floods and heat waves in the UK. The proposed method uses machine learning classifiers based on deep bidirectional neural networks trained on benchmark datasets of disaster responses and extreme events. The resulting models are applied to perform sentiment and qualitative analysis of inferred topics in text data. We further analyse a set of behavioural indicators and match them with climate variables via decoding synoptical records to analyse thermal comfort. We highlight the advantages of aligning behavioural indicators along with climate variables to provide with additional valuable information to be considered especially in different phases of a disaster and applicable to extreme weather periods. The positiveness of messages is around 8% for disaster, 1% for disaster and medical response, 7% for disaster and humanitarian related messages. This shows the reliability of such data for our case studies. We show the transferability of this approach to be applied to any social media data collection.","36 pages, 12 figures",,,cs.SI,['cs.SI'],"['http://arxiv.org/abs/2203.08753v1', 'http://arxiv.org/pdf/2203.08753v1']",http://arxiv.org/pdf/2203.08753v1
135,http://arxiv.org/abs/2003.00646v2,2020-08-19 14:24:18+00:00,2020-03-02 03:59:38+00:00,A review of machine learning applications in wildfire science and management,"['Piyush Jain', 'Sean C P Coogan', 'Sriram Ganapathi Subramanian', 'Mark Crowley', 'Steve Taylor', 'Mike D Flannigan']","Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then the field has rapidly progressed congruently with the wide adoption of machine learning (ML) in the environmental sciences. Here, we present a scoping review of ML in wildfire science and management. Our objective is to improve awareness of ML among wildfire scientists and managers, as well as illustrate the challenging range of problems in wildfire science available to data scientists. We first present an overview of popular ML approaches used in wildfire science to date, and then review their use in wildfire science within six problem domains: 1) fuels characterization, fire detection, and mapping; 2) fire weather and climate change; 3) fire occurrence, susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6) fire management. We also discuss the advantages and limitations of various ML approaches and identify opportunities for future advances in wildfire science and management within a data science context. We identified 298 relevant publications, where the most frequently used ML methods included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. There exists opportunities to apply more current ML methods (e.g., deep learning and agent based learning) in wildfire science. However, despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods requires sophisticated knowledge for their application. Finally, we stress that the wildfire research and management community plays an active role in providing relevant, high quality data for use by practitioners of ML methods.","83 pages, 4 figures, 3 tables","Environmental Reviews. 28(4): 478-505, 2020",10.1139/er-2020-0019,cs.LG,"['cs.LG', 'stat.ML']","['http://dx.doi.org/10.1139/er-2020-0019', 'http://arxiv.org/abs/2003.00646v2', 'http://arxiv.org/pdf/2003.00646v2']",http://arxiv.org/pdf/2003.00646v2
136,http://arxiv.org/abs/1106.0223v1,2011-06-01 16:17:03+00:00,2011-06-01 16:17:03+00:00,Decentralized Markets versus Central Control: A Comparative Study,"['H. Akkermans', 'F. Ygge']","Multi-Agent Systems (MAS) promise to offer solutions to problems where established, older paradigms fall short. In order to validate such claims that are repeatedly made in software agent publications, empirical in-depth studies of advantages and weaknesses of multi-agent solutions versus conventional ones in practical applications are needed. Climate control in large buildings is one application area where multi-agent systems, and market-oriented programming in particular, have been reported to be very successful, although central control solutions are still the standard practice. We have therefore constructed and implemented a variety of market designs for this problem, as well as different standard control engineering solutions. This article gives a detailed analysis and comparison, so as to learn about differences between standard versus agent approaches, and yielding new insights about benefits and limitations of computational markets. An important outcome is that ""local information plus market communication produces global control"".",,"Journal Of Artificial Intelligence Research, Volume 11, pages
  301-333, 1999",10.1613/jair.627,cs.MA,"['cs.MA', 'cs.AI']","['http://dx.doi.org/10.1613/jair.627', 'http://arxiv.org/abs/1106.0223v1', 'http://arxiv.org/pdf/1106.0223v1']",http://arxiv.org/pdf/1106.0223v1
137,http://arxiv.org/abs/2104.11079v3,2022-03-21 21:29:54+00:00,2021-04-19 18:59:26+00:00,Randomized Algorithms for Scientific Computing (RASC),"['Aydin Buluc', 'Tamara G. Kolda', 'Stefan M. Wild', 'Mihai Anitescu', 'Anthony DeGennaro', 'John Jakeman', 'Chandrika Kamath', 'Ramakrishnan Kannan', 'Miles E. Lopes', 'Per-Gunnar Martinsson', 'Kary Myers', 'Jelani Nelson', 'Juan M. Restrepo', 'C. Seshadhri', 'Draguna Vrabie', 'Brendt Wohlberg', 'Stephen J. Wright', 'Chao Yang', 'Peter Zwart']","Randomized algorithms have propelled advances in artificial intelligence and represent a foundational research area in advancing AI for Science. Future advancements in DOE Office of Science priority areas such as climate science, astrophysics, fusion, advanced materials, combustion, and quantum computing all require randomized algorithms for surmounting challenges of complexity, robustness, and scalability. This report summarizes the outcomes of that workshop, ""Randomized Algorithms for Scientific Computing (RASC),"" held virtually across four days in December 2020 and January 2021.",,,10.2172/1807223,cs.AI,"['cs.AI', 'cs.CE']","['http://dx.doi.org/10.2172/1807223', 'http://arxiv.org/abs/2104.11079v3', 'http://arxiv.org/pdf/2104.11079v3']",http://arxiv.org/pdf/2104.11079v3
138,http://arxiv.org/abs/2105.02368v2,2021-05-17 00:41:11+00:00,2021-05-05 23:32:43+00:00,Physics-informed Spline Learning for Nonlinear Dynamics Discovery,"['Fangzheng Sun', 'Yang Liu', 'Hao Sun']","Dynamical systems are typically governed by a set of linear/nonlinear differential equations. Distilling the analytical form of these equations from very limited data remains intractable in many disciplines such as physics, biology, climate science, engineering and social science. To address this fundamental challenge, we propose a novel Physics-informed Spline Learning (PiSL) framework to discover parsimonious governing equations for nonlinear dynamics, based on sparsely sampled noisy data. The key concept is to (1) leverage splines to interpolate locally the dynamics, perform analytical differentiation and build the library of candidate terms, (2) employ sparse representation of the governing equations, and (3) use the physics residual in turn to inform the spline learning. The synergy between splines and discovered underlying physics leads to the robust capacity of dealing with high-level data scarcity and noise. A hybrid sparsity-promoting alternating direction optimization strategy is developed for systematically pruning the sparse coefficients that form the structure and explicit expression of the governing equations. The efficacy and superiority of the proposed method have been demonstrated by multiple well-known nonlinear dynamical systems, in comparison with two state-of-the-art methods.",,"The 30th International Joint Conference on Artificial Intelligence
  (IJCAI-2021)",,cs.LG,"['cs.LG', 'cs.AI', 'nlin.CD']","['http://arxiv.org/abs/2105.02368v2', 'http://arxiv.org/pdf/2105.02368v2']",http://arxiv.org/pdf/2105.02368v2
139,http://arxiv.org/abs/2210.01235v1,2022-10-03 21:24:04+00:00,2022-10-03 21:24:04+00:00,CaiRL: A High-Performance Reinforcement Learning Environment Toolkit,"['Per-Arne Andersen', 'Morten Goodwin', 'Ole-Christoffer Granmo']","This paper addresses the dire need for a platform that efficiently provides a framework for running reinforcement learning (RL) experiments. We propose the CaiRL Environment Toolkit as an efficient, compatible, and more sustainable alternative for training learning agents and propose methods to develop more efficient environment simulations.   There is an increasing focus on developing sustainable artificial intelligence. However, little effort has been made to improve the efficiency of running environment simulations. The most popular development toolkit for reinforcement learning, OpenAI Gym, is built using Python, a powerful but slow programming language. We propose a toolkit written in C++ with the same flexibility level but works orders of magnitude faster to make up for Python's inefficiency. This would drastically cut climate emissions.   CaiRL also presents the first reinforcement learning toolkit with a built-in JVM and Flash support for running legacy flash games for reinforcement learning research. We demonstrate the effectiveness of CaiRL in the classic control benchmark, comparing the execution speed to OpenAI Gym. Furthermore, we illustrate that CaiRL can act as a drop-in replacement for OpenAI Gym to leverage significantly faster training speeds because of the reduced environment computation time.",Published in 2022 IEEE Conference on Games (CoG),IEEE 2022,10.1109/CoG51982.2022.9893661,cs.LG,"['cs.LG', 'cs.AI']","['http://dx.doi.org/10.1109/CoG51982.2022.9893661', 'http://arxiv.org/abs/2210.01235v1', 'http://arxiv.org/pdf/2210.01235v1']",http://arxiv.org/pdf/2210.01235v1
140,http://arxiv.org/abs/1507.03638v2,2016-02-16 21:43:03+00:00,2015-07-13 22:19:41+00:00,Experimental analysis of data-driven control for a building heating system,"['Giuseppe Tommaso Costanzo', 'Sandro Iacovella', 'Frederik Ruelens', 'T. Leurs', 'Bert Claessens']","Driven by the opportunity to harvest the flexibility related to building climate control for demand response applications, this work presents a data-driven control approach building upon recent advancements in reinforcement learning. More specifically, model assisted batch reinforcement learning is applied to the setting of building climate control subjected to a dynamic pricing. The underlying sequential decision making problem is cast on a markov decision problem, after which the control algorithm is detailed. In this work, fitted Q-iteration is used to construct a policy from a batch of experimental tuples. In those regions of the state space where the experimental sample density is low, virtual support samples are added using an artificial neural network. Finally, the resulting policy is shaped using domain knowledge. The control approach has been evaluated quantitatively using a simulation and qualitatively in a living lab. From the quantitative analysis it has been found that the control approach converges in approximately 20 days to obtain a control policy with a performance within 90% of the mathematical optimum. The experimental analysis confirms that within 10 to 20 days sensible policies are obtained that can be used for different outside temperature regimes.","12 pages, 8 figures, pending for publication in Elsevier SEGAN",,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/1507.03638v2', 'http://arxiv.org/pdf/1507.03638v2']",http://arxiv.org/pdf/1507.03638v2
141,http://arxiv.org/abs/1704.08111v3,2020-04-30 23:58:09+00:00,2017-04-23 21:16:40+00:00,A Popperian Falsification of Artificial Intelligence -- Lighthill Defended,['Steven Meyer'],The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British mathematical physicist James Lighthill. How Lighthill's arguments continue to apply to current AI is explained. It is argued that AI should use the Popperian scientific method in which it is the duty of scientists to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method and discusses Paul Nurse's application of the method to cell biology that also involves questions of mechanism and behavior. It is shown how Lighthill's falsifying arguments especially combinatorial explosion continue to apply to modern AI. Various skeptical arguments against the assumptions of AI mostly by physicists especially against Hilbert's philosophical programme that defined knowledge and truth as provable formal sentences. John von Neumann's arguments from natural complexity against neural networks and evolutionary algorithms are discussed. Next the game of chess is discussed to show how modern chess experts have reacted to computer chess programs. It is shown that currently chess masters can defeat any chess program using Kasperov's arguments from his 1997 Deep Blue match and aftermath. The game of 'go' and climate models are discussed to show computer applications where combinatorial explosion may not apply. The paper concludes by advocating studying computation as Peter Naur's Dataology.,"12 pages. Version improves discussion of chess and adds sections on
  when combinatorial explosion may not apply",,,cs.AI,"['cs.AI', 'I.1; I.1.2']","['http://arxiv.org/abs/1704.08111v3', 'http://arxiv.org/pdf/1704.08111v3']",http://arxiv.org/pdf/1704.08111v3
142,http://arxiv.org/abs/2010.00330v3,2021-08-25 14:26:33+00:00,2020-09-30 13:09:48+00:00,Workflow Provenance in the Lifecycle of Scientific Machine Learning,"['Renan Souza', 'Leonardo G. Azevedo', 'Vítor Lourenço', 'Elton Soares', 'Raphael Thiago', 'Rafael Brandão', 'Daniel Civitarese', 'Emilio Vital Brazil', 'Marcio Moreno', 'Patrick Valduriez', 'Marta Mattoso', 'Renato Cerqueira', 'Marco A. S. Netto']","Machine Learning (ML) has already fundamentally changed several businesses. More recently, it has also been profoundly impacting the computational science and engineering domains, like geoscience, climate science, and health science. In these domains, users need to perform comprehensive data analyses combining scientific data and ML models to provide for critical requirements, such as reproducibility, model explainability, and experiment data understanding. However, scientific ML is multidisciplinary, heterogeneous, and affected by the physical constraints of the domain, making such analyses even more challenging. In this work, we leverage workflow provenance techniques to build a holistic view to support the lifecycle of scientific ML. We contribute with (i) characterization of the lifecycle and taxonomy for data analyses; (ii) design principles to build this view, with a W3C PROV compliant data representation and a reference system architecture; and (iii) lessons learned after an evaluation in an Oil & Gas case using an HPC cluster with 393 nodes and 946 GPUs. The experiments show that the principles enable queries that integrate domain semantics with ML models while keeping low overhead (<1%), high scalability, and an order of magnitude of query acceleration under certain workloads against without our representation.","21 pages, 10 figures, text overlap with arXiv:1910.04223, a workshop
  paper being extended in this journal paper",Concurrency Computation Practice Experience. 2021;e6544,10.1002/cpe.6544,cs.DB,"['cs.DB', 'cs.AI', 'cs.DC', 'cs.LG', '65Y05, 68P15', 'I.2; H.2; C.4; J.2']","['http://dx.doi.org/10.1002/cpe.6544', 'http://arxiv.org/abs/2010.00330v3', 'http://arxiv.org/pdf/2010.00330v3']",http://arxiv.org/pdf/2010.00330v3
143,http://arxiv.org/abs/2206.11669v3,2022-11-11 06:33:43+00:00,2022-06-23 12:49:36+00:00,Short-range forecasts of global precipitation using deep learning-augmented numerical weather prediction,"['Manmeet Singh', 'Vaisakh S B', 'Nachiketa Acharya', 'Aditya Grover', 'Suryachandra A Rao', 'Bipin Kumar', 'Zong-Liang Yang', 'Dev Niyogi']","Precipitation governs Earth's hydroclimate, and its daily spatiotemporal fluctuations have major socioeconomic effects. Advances in Numerical weather prediction (NWP) have been measured by the improvement of forecasts for various physical fields such as temperature and pressure; however, large biases exist in precipitation prediction. We augment the output of the well-known NWP model CFSv2 with deep learning to create a hybrid model that improves short-range global precipitation at 1-, 2-, and 3-day lead times. To hybridise, we address the sphericity of the global data by using modified DLWP-CS architecture which transforms all the fields to cubed-sphere projection. Dynamical model precipitation and surface temperature outputs are fed into a modified DLWP-CS (UNET) to forecast ground truth precipitation. While CFSv2's average bias is +5 to +7 mm/day over land, the multivariate deep learning model decreases it to within -1 to +1 mm/day. Hurricane Katrina in 2005, Hurricane Ivan in 2004, China floods in 2010, India floods in 2005, and Myanmar storm Nargis in 2008 are used to confirm the substantial enhancement in the skill for the hybrid dynamical-deep learning model. CFSv2 typically shows a moderate to large bias in the spatial pattern and overestimates the precipitation at short-range time scales. The proposed deep learning augmented NWP model can address these biases and vastly improve the spatial pattern and magnitude of predicted precipitation. Deep learning enhanced CFSv2 reduces mean bias by 8x over important land regions for 1 day lead compared to CFSv2. The spatio-temporal deep learning system opens pathways to further the precision and accuracy in global short-range precipitation forecasts.","Accepted at Tackling Climate Change with Machine Learning: workshop
  at NeurIPS 2022",,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.CV']","['http://arxiv.org/abs/2206.11669v3', 'http://arxiv.org/pdf/2206.11669v3']",http://arxiv.org/pdf/2206.11669v3
144,http://arxiv.org/abs/2209.14875v1,2022-09-29 15:40:12+00:00,2022-09-29 15:40:12+00:00,Accelerating Laboratory Automation Through Robot Skill Learning For Sample Scraping,"['Gabriella Pizzuto', 'Hetong Wang', 'Hatem Fakhruldeen', 'Bei Peng', 'Kevin S. Luck', 'Andrew I. Cooper']","The potential use of robotics for laboratory experiments offers an attractive route to alleviate scientists from tedious tasks while accelerating the process of obtaining new materials, where topical issues such as climate change and disease risks worldwide would greatly benefit. While some experimental workflows can already benefit from automation, it is common that sample preparation is still carried out manually due to the high level of motor function required when dealing with heterogeneous systems, e.g., different tools, chemicals, and glassware. A fundamental workflow in chemical fields is crystallisation, where one application is polymorph screening, i.e., obtaining a three dimensional molecular structure from a crystal. For this process, it is of utmost importance to recover as much of the sample as possible since synthesising molecules is both costly in time and money. To this aim, chemists have to scrape vials to retrieve sample contents prior to imaging plate transfer. Automating this process is challenging as it goes beyond robotic insertion tasks due to a fundamental requirement of having to execute fine-granular movements within a constrained environment that is the sample vial. Motivated by how human chemists carry out this process of scraping powder from vials, our work proposes a model-free reinforcement learning method for learning a scraping policy, leading to a fully autonomous sample scraping procedure. To realise that, we first create a simulation environment with a Panda Franka Emika robot using a laboratory scraper which is inserted into a simulated vial, to demonstrate how a scraping policy can be learned successfully. We then evaluate our method on a real robotic manipulator in laboratory settings, and show that our method can autonomously scrape powder across various setups.","7 pages, 7 figures, submitted to IEEE International Conference on
  Robotics and Automation (ICRA 2023). Video:
  https://www.youtube.com/watch?v=0QQ3VPy0W6E",,,cs.RO,"['cs.RO', 'cs.AI']","['http://arxiv.org/abs/2209.14875v1', 'http://arxiv.org/pdf/2209.14875v1']",http://arxiv.org/pdf/2209.14875v1
145,http://arxiv.org/abs/2210.10582v2,2022-10-23 19:13:43+00:00,2022-09-04 01:50:30+00:00,"Avoiding the ""Great Filter"": Extraterrestrial Life and Humanity's Future in the Universe","['Jonathan H. Jiang', 'Philip E. Rosen', 'Kelly Lu', 'Kristen A. Fahy', 'Piotr Obacz']","Our Universe is a vast, tantalizing enigma - a mystery that has aroused humankind's innate curiosity for eons. Begging questions on alien lifeforms have been thus far unfruitful, even with the bounding advancements we have embarked upon in recent years. Coupled with logical assumption and calculations such as made by Dr. Frank Drake starting in the early 1960s, evidence in the millions should exist in our galaxy alone, and yet we've produced no clear affirmation in practice. So, where is everybody? In one sense, the seeming silence of the Universe past terra firma reveals layers of stubborn human limitation. Even as ambitious programs such as SETI aim to solve these knotty challenges, the results have turned up rather pessimistic possibilities. An existential disaster may lay in wait as our society advances exponentially towards space exploration, acting as the Great Filter: a phenomenon that wipes out civilizations before they can encounter each other, which may explain the cosmic silence. In this article, we propose several possible doomsday-type scenarios, including anthropogenic and natural hazards, both of which can be prevented with reforms in individual, institutional and intrinsic behaviors. We take into account multiple calamity candidates: nuclear warfare, pathogens and pandemics, artificial intelligence, asteroid and comet impacts, and climate change. Each of these categories have various influences but lack critical adjustment to accommodate to their high risk. We have long ignored the quickly encroaching Great Filter, even as it threatens to consume us entirely, especially as our rate of progress correlates directly to the severity of our fall. This indicates a necessary period of introspection, followed by appropriate refinements to properly approach our predicament, and see our way through it.",,,,physics.pop-ph,"['physics.pop-ph', 'physics.soc-ph']","['http://arxiv.org/abs/2210.10582v2', 'http://arxiv.org/pdf/2210.10582v2']",http://arxiv.org/pdf/2210.10582v2
146,http://arxiv.org/abs/2211.04257v1,2022-11-05 17:48:59+00:00,2022-11-05 17:48:59+00:00,Toward Human-AI Co-creation to Accelerate Material Discovery,"['Dmitry Zubarev', 'Carlos Raoni Mendes', 'Emilio Vital Brazil', 'Renato Cerqueira', 'Kristin Schmidt', 'Vinicius Segura', 'Juliana Jansen Ferreira', 'Dan Sanders']","There is an increasing need in our society to achieve faster advances in Science to tackle urgent problems, such as climate changes, environmental hazards, sustainable energy systems, pandemics, among others. In certain domains like chemistry, scientific discovery carries the extra burden of assessing risks of the proposed novel solutions before moving to the experimental stage. Despite several recent advances in Machine Learning and AI to address some of these challenges, there is still a gap in technologies to support end-to-end discovery applications, integrating the myriad of available technologies into a coherent, orchestrated, yet flexible discovery process. Such applications need to handle complex knowledge management at scale, enabling knowledge consumption and production in a timely and efficient way for subject matter experts (SMEs). Furthermore, the discovery of novel functional materials strongly relies on the development of exploration strategies in the chemical space. For instance, generative models have gained attention within the scientific community due to their ability to generate enormous volumes of novel molecules across material domains. These models exhibit extreme creativity that often translates in low viability of the generated candidates. In this work, we propose a workbench framework that aims at enabling the human-AI co-creation to reduce the time until the first discovery and the opportunity costs involved. This framework relies on a knowledge base with domain and process knowledge, and user-interaction components to acquire knowledge and advise the SMEs. Currently,the framework supports four main activities: generative modeling, dataset triage, molecule adjudication, and risk assessment.","9 pages, 5 figures, NeurIPS 2022 WS: AI4Science",,,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.QM']","['http://arxiv.org/abs/2211.04257v1', 'http://arxiv.org/pdf/2211.04257v1']",http://arxiv.org/pdf/2211.04257v1
147,http://arxiv.org/abs/2210.17517v1,2022-10-31 17:41:26+00:00,2022-10-31 17:41:26+00:00,Lila: A Unified Benchmark for Mathematical Reasoning,"['Swaroop Mishra', 'Matthew Finlayson', 'Pan Lu', 'Leonard Tang', 'Sean Welleck', 'Chitta Baral', 'Tanmay Rajpurohit', 'Oyvind Tafjord', 'Ashish Sabharwal', 'Peter Clark', 'Ashwin Kalyan']","Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), while the best performing model only obtains 60.40%, indicating the room for improvement in general mathematical reasoning and understanding.",EMNLP 2022,,,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","['http://arxiv.org/abs/2210.17517v1', 'http://arxiv.org/pdf/2210.17517v1']",http://arxiv.org/pdf/2210.17517v1
148,http://arxiv.org/abs/1806.06474v1,2018-06-18 01:28:59+00:00,2018-06-18 01:28:59+00:00,"Population Growth, Energy Use, and the Implications for the Search for Extraterrestrial Intelligence","['Brendan Mullan', 'Jacob Haqq-Misra']","Von Hoerner (1975) examined the effects of human population growth and predicted agricultural, environmental, and other problems from observed growth rate trends. Using straightforward calculations, VH75 predicted the ""doomsday"" years for these scenarios (2020-2050), when we as a species should run out of space or food, or induce catastrophic anthropogenic climate change through thermodynamically unavoidable direct heating of the planet. Now that over four decades have passed, in this paper we update VH75. We perform similar calculations as that work, with improved data and trends in population growth, food production, energy use, and climate change. For many of the impacts noted in VH75 our work amounts to pushing the ""doomsday"" horizon back to the 2300s-2400s (or much further for population-driven interstellar colonization). This is largely attributable to using worldwide data that exhibit smaller growth rates of population and energy use in the last few decades. While population-related catastrophes appear less likely than in VH75, our continued growth in energy use provides insight into possible future issues. We find that, if historic trends continue, direct heating of the Earth will be a substantial contributor to climate change by about 2260, regardless of the energy source used, coincident with our transition to a Kardashev type-I civilization. We also determine that either an increase of Earth's global mean temperature of 12K will occur or an unreasonably high fraction of the planet will need to be covered by solar collectors by about 2400 to keep pace with our growth in energy use. We further discuss the implications in terms of interstellar expansion, the transition to type II and III civilizations, SETI, and the Fermi Paradox. We conclude that the ""sustainability solution"" to the Fermi Paradox is a compelling possibility.","Accepted for publication in Futures, special issue: Detectability of
  Future Earth; 30 pages, 3 figures",Futures 106:4-17. 2019,10.1016/j.futures.2018.06.009,physics.pop-ph,['physics.pop-ph'],"['http://dx.doi.org/10.1016/j.futures.2018.06.009', 'http://arxiv.org/abs/1806.06474v1', 'http://arxiv.org/pdf/1806.06474v1']",http://arxiv.org/pdf/1806.06474v1
149,http://arxiv.org/abs/2011.11313v3,2020-12-08 09:24:20+00:00,2020-11-23 10:24:17+00:00,Deep-learning based down-scaling of summer monsoon rainfall data over Indian region,"['Bipin Kumar', 'Rajib Chattopadhyay', 'Manmeet Singh', 'Niraj Chaudhari', 'Karthik Kodari', 'Amit Barve']","Downscaling is necessary to generate high-resolution observation data to validate the climate model forecast or monitor rainfall at the micro-regional level operationally. Dynamical and statistical downscaling models are often used to get information at high-resolution gridded data over larger domains. As rainfall variability is dependent on the complex Spatio-temporal process leading to non-linear or chaotic Spatio-temporal variations, no single downscaling method can be considered efficient enough. In data with complex topographies, quasi-periodicities, and non-linearities, deep Learning (DL) based methods provide an efficient solution in downscaling rainfall data for regional climate forecasting and real-time rainfall observation data at high spatial resolutions. In this work, we employed three deep learning-based algorithms derived from the super-resolution convolutional neural network (SRCNN) methods, to precipitation data, in particular, IMD and TRMM data to produce 4x-times high-resolution downscaled rainfall data during the summer monsoon season. Among the three algorithms, namely SRCNN, stacked SRCNN, and DeepSD, employed here, the best spatial distribution of rainfall amplitude and minimum root-mean-square error is produced by DeepSD based downscaling. Hence, the use of the DeepSD algorithm is advocated for future use. We found that spatial discontinuity in amplitude and intensity rainfall patterns is the main obstacle in the downscaling of precipitation. Furthermore, we applied these methods for model data postprocessing, in particular, ERA5 data. Downscaled ERA5 rainfall data show a much better distribution of spatial covariance and temporal variance when compared with observation.",,,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'physics.data-an']","['http://arxiv.org/abs/2011.11313v3', 'http://arxiv.org/pdf/2011.11313v3']",http://arxiv.org/pdf/2011.11313v3
150,http://arxiv.org/abs/2201.00417v1,2022-01-02 20:55:18+00:00,2022-01-02 20:55:18+00:00,Non-local parameterization of atmospheric subgrid processes with neural networks,"['Peidong Wang', 'Janni Yuval', ""Paul A. O'Gorman""]","Subgrid processes in global climate models are represented by parameterizations that are a major source of uncertainties in simulations of climate. In recent years, it has been suggested that new machine-learning parameterizations learned from high-resolution model output data could be superior to traditional parameterizations. Currently, both traditional and machine-learning parameterizations of subgrid processes in the atmosphere are based on a single-column approach. Namely, the information used by these parameterizations is taken from a single atmospheric column. However, a single-column approach might not be ideal for the parameterization problem since certain atmospheric phenomena, such as organized convective systems, can cross multiple grid boxes and involve slantwise circulations that are not purely vertical. Here we train neural networks using non-local inputs spanning over 3$\times$3 columns of inputs. We find that including the non-local inputs substantially improves the prediction of subgrid tendencies of a range of subgrid processes. The improvement is especially notable for cases associated with mid-latitude fronts and convective instability. Using an explainable artificial intelligence technique called layer-wise relevance propagation, we find that non-local inputs from zonal and meridional winds contain information that helps to improve the performance of the neural network parameterization. Our results imply that use of non-local inputs has the potential to substantially improve both traditional and machine-learning parameterizations.","31 pages, 17 figures (7 figures in the main file)",,10.1029/2022MS002984,physics.ao-ph,['physics.ao-ph'],"['http://dx.doi.org/10.1029/2022MS002984', 'http://arxiv.org/abs/2201.00417v1', 'http://arxiv.org/pdf/2201.00417v1']",http://arxiv.org/pdf/2201.00417v1
151,http://arxiv.org/abs/2204.08465v1,2022-04-15 21:14:07+00:00,2022-04-15 21:14:07+00:00,Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data,"['Ian Zhou', 'Justin Lipman', 'Mehran Abolhasan', 'Negin Shariati']","The weather phenomenon of frost poses great threats to agriculture. Since it damages the crops and plants from upstream of the supply chain, the potential impact of frosts is significant for agriculture-related industries. As recent frost prediction methods are based on on-site historical data and sensors, extra development and deployment time are required for data collection in any new site. The aim of this article is to eliminate the dependency on on-site historical data and sensors for frost prediction methods. In this article, a frost prediction method based on spatial interpolation is proposed. The models use climate data from existing weather stations, digital elevation models surveys, and normalized difference vegetation index data to estimate a target site's next hour minimum temperature. The proposed method utilizes ensemble learning to increase the model accuracy. Ensemble methods include averaging and weighted averaging. Climate datasets are obtained from 75 weather stations across New South Wales and Australian Capital Territory areas of Australia. The models are constructed with five-fold validation, splitting the weather stations into five testing dataset folds. For each fold, the other stations act as training datasets. After the models are constructed, three experiments are conducted. The first experiment compares the results generated by models between different folds. Then, the second experiment compares the accuracy of different methods. The final experiment reveals the effect of available stations on the proposed models. The results show that the proposed method reached a detection rate up to 92.55%. This method could be implemented as an alternative solution when on-site historical datasets are scarce.","13 pages, 13 figures",,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2204.08465v1', 'http://arxiv.org/pdf/2204.08465v1']",http://arxiv.org/pdf/2204.08465v1
152,http://arxiv.org/abs/2209.06848v1,2022-08-15 12:42:20+00:00,2022-08-15 12:42:20+00:00,"Urban precipitation downscaling using deep learning: a smart city application over Austin, Texas, USA","['Manmeet Singh', 'Nachiketa Acharya', 'Sajad Jamshidi', 'Junfeng Jiao', 'Zong-Liang Yang', 'Marc Coudert', 'Zach Baumer', 'Dev Niyogi']","Urban downscaling is a link to transfer the knowledge from coarser climate information to city scale assessments. These high-resolution assessments need multiyear climatology of past data and future projections, which are complex and computationally expensive to generate using traditional numerical weather prediction models. The city of Austin, Texas, USA has seen tremendous growth in the past decade. Systematic planning for the future requires the availability of fine resolution city-scale datasets. In this study, we demonstrate a novel approach generating a general purpose operator using deep learning to perform urban downscaling. The algorithm employs an iterative super-resolution convolutional neural network (Iterative SRCNN) over the city of Austin, Texas, USA. We show the development of a high-resolution gridded precipitation product (300 m) from a coarse (10 km) satellite-based product (JAXA GsMAP). High resolution gridded datasets of precipitation offer insights into the spatial distribution of heavy to low precipitation events in the past. The algorithm shows improvement in the mean peak-signal-to-noise-ratio and mutual information to generate high resolution gridded product of size 300 m X 300 m relative to the cubic interpolation baseline. Our results have implications for developing high-resolution gridded-precipitation urban datasets and the future planning of smart cities for other cities and other climatic variables.",,,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.CV', 'cs.LG']","['http://arxiv.org/abs/2209.06848v1', 'http://arxiv.org/pdf/2209.06848v1']",http://arxiv.org/pdf/2209.06848v1
153,http://arxiv.org/abs/2112.03235v2,2022-11-27 07:20:27+00:00,2021-12-06 18:45:31+00:00,Simulation Intelligence: Towards a New Generation of Scientific Methods,"['Alexander Lavin', 'David Krakauer', 'Hector Zenil', 'Justin Gottschlich', 'Tim Mattson', 'Johann Brehmer', 'Anima Anandkumar', 'Sanjay Choudry', 'Kamil Rocki', 'Atılım Güneş Baydin', 'Carina Prunkl', 'Brooks Paige', 'Olexandr Isayev', 'Erik Peterson', 'Peter L. McMahon', 'Jakob Macke', 'Kyle Cranmer', 'Jiaxin Zhang', 'Haruko Wainwright', 'Adi Hanuka', 'Manuela Veloso', 'Samuel Assefa', 'Stephan Zheng', 'Avi Pfeffer']","The original ""Seven Motifs"" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the ""Nine Motifs of Simulation Intelligence"", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.",,,,cs.AI,"['cs.AI', 'cs.CE', 'cs.LG', 'cs.MS']","['http://arxiv.org/abs/2112.03235v2', 'http://arxiv.org/pdf/2112.03235v2']",http://arxiv.org/pdf/2112.03235v2
154,http://arxiv.org/abs/2206.03264v1,2022-05-16 20:10:20+00:00,2022-05-16 20:10:20+00:00,Intelligent Energy Management Systems -- A Review,"['Stavros Mischos', 'Eleanna Dalagdi', 'Dimitris Vrakas']","Climate change has become a major problem for humanity in the last two decades. One of the reasons that caused it, is our daily energy waste. People consume electricity in order to use home/work appliances and devices and also reach certain levels of comfort while working or being at home. However, even though the environmental impact of this behavior is not immediately observed, it leads to increased CO2 emissions coming from energy generation from power plants. Confronting such a problem efficiently will affect both the environment and our society. Monitoring energy consumption in real-time, changing energy wastage behavior of occupants and using automations with incorporated energy savings scenarios, are ways to decrease global energy footprint. In this review, we study intelligent systems for energy management in residential, commercial and educational buildings, classifying them in two major categories depending on whether they provide direct or indirect control. The article also discusses what the strengths and weaknesses are, which optimization techniques do they use and finally, provide insights about how these systems can be improved in the future.",,,,cs.CY,['cs.CY'],"['http://arxiv.org/abs/2206.03264v1', 'http://arxiv.org/pdf/2206.03264v1']",http://arxiv.org/pdf/2206.03264v1
155,http://arxiv.org/abs/2209.05251v1,2022-09-07 16:40:45+00:00,2022-09-07 16:40:45+00:00,Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks,"['Lorenzo Bonicelli', 'Angelo Porrello', 'Stefano Vincenzi', 'Carla Ippoliti', 'Federica Iapaolo', 'Annamaria Conte', 'Simone Calderara']","The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities.   In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbouring places, and further extend these modules to consider multiple relations, such as the difference in temperature and soil moisture between two sites, as well as the geographical distance. Moreover, we inject time-related information directly into the model to take into account the seasonality of virus spread.   We design an experimental setting that combines satellite images - from Landsat and Sentinel missions - with ground truth observations of WNV circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention Network (MAGAT) consistently leads to higher performance when paired with an appropriate pre-training stage. Finally, we assess the importance of each component of MAGAT in our ablation studies.","11 pages, 4 figures. This work has been submitted to the IEEE
  Transactions On Geoscience And Remote Sensing for possible publication",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2209.05251v1', 'http://arxiv.org/pdf/2209.05251v1']",http://arxiv.org/pdf/2209.05251v1
156,http://arxiv.org/abs/2211.02409v1,2022-11-04 12:37:27+00:00,2022-11-04 12:37:27+00:00,The Sustainable Development Goals and Aerospace Engineering: A critical note through Artificial Intelligence,"['Alejandro Sánchez-Roncero', 'Òscar Garibo-i-Ortsa', 'J. Alberto Conejero', 'Hamidreza Eivazi', 'Fermín Mallor', 'Emelie Rosenberg', 'Francesco Fuso-Nerini', 'Javier García-Martínez', 'Ricardo Vinuesa', 'Sergio Hoyas']","The 2030 Agenda of the United Nations (UN) revolves around the Sustainable Development Goals (SDGs). A critical step towards that objective is identifying whether scientific production aligns with the SDGs' achievement. To assess this, funders and research managers need to manually estimate the impact of their funding agenda on the SDGs, focusing on accuracy, scalability, and objectiveness. With this objective in mind, in this work, we develop ASDG, an easy-to-use artificial-intelligence (AI)-based model for automatically identifying the potential impact of scientific papers on the UN SDGs. As a demonstrator of ASDG, we analyze the alignment of recent aerospace publications with the SDGs. The Aerospace data set analyzed in this paper consists of approximately 820,000 papers published in English from 2011 to 2020 and indexed in the Scopus database. The most-contributed SDGs are 7 (on clean energy), 9 (on industry), 11 (on sustainable cities) and 13 (on climate action). The establishment of the SDGs by the UN in the middle of the 2010 decade did not significantly affect the data. However, we find clear discrepancies among countries, likely indicative of different priorities. Also, different trends can be seen in the most and least cited papers, with clear differences in some SDGs. Finally, the number of abstracts the code cannot identify is decreasing with time, possibly showing the scientific community's awareness of SDG.",,,,cs.DL,"['cs.DL', 'physics.data-an']","['http://arxiv.org/abs/2211.02409v1', 'http://arxiv.org/pdf/2211.02409v1']",http://arxiv.org/pdf/2211.02409v1
157,http://arxiv.org/abs/1902.06778v1,2018-12-26 18:38:28+00:00,2018-12-26 18:38:28+00:00,Using an Ancillary Neural Network to Capture Weekends and Holidays in an Adjoint Neural Network Architecture for Intelligent Building Management,"['Zhicheng Ding', 'Mehmet Kerem Turkcan', 'Albert Boulanger']","The US EIA estimated in 2017 about 39\% of total U.S. energy consumption was by the residential and commercial sectors. Therefore, Intelligent Building Management (IBM) solutions that minimize consumption while maintaining tenant comfort are an important component in addressing climate change. A forecasting capability for accurate prediction of indoor temperatures in a planning horizon of 24 hours is essential to IBM. It should predict the indoor temperature in both short-term (e.g. 15 minutes) and long-term (e.g. 24 hours) periods accurately including weekends, major holidays, and minor holidays. Other requirements include the ability to predict the maximum and the minimum indoor temperatures precisely and provide the confidence for each prediction. To achieve these requirements, we propose a novel adjoint neural network architecture for time series prediction that uses an ancillary neural network to capture weekend and holiday information. We studied four long short-term memory (LSTM) based time series prediction networks within this architecture. We observed that the ancillary neural network helps to improve the prediction accuracy, the maximum and the minimum temperature prediction and model reliability for all networks tested.","9 pages, 11 figures, 2 tables",,,cs.LG,"['cs.LG', 'cs.NE']","['http://arxiv.org/abs/1902.06778v1', 'http://arxiv.org/pdf/1902.06778v1']",http://arxiv.org/pdf/1902.06778v1
158,http://arxiv.org/abs/cs/0405012v1,2004-05-05 00:36:17+00:00,2004-05-05 00:36:17+00:00,Is Neural Network a Reliable Forecaster on Earth? A MARS Query!,"['Ajith Abraham', 'Dan Steinberg']","Long-term rainfall prediction is a challenging task especially in the modern world where we are facing the major environmental problem of global warming. In general, climate and rainfall are highly non-linear phenomena in nature exhibiting what is known as the butterfly effect. While some regions of the world are noticing a systematic decrease in annual rainfall, others notice increases in flooding and severe storms. The global nature of this phenomenon is very complicated and requires sophisticated computer modeling and simulation to predict accurately. In this paper, we report a performance analysis for Multivariate Adaptive Regression Splines (MARS)and artificial neural networks for one month ahead prediction of rainfall. To evaluate the prediction efficiency, we made use of 87 years of rainfall data in Kerala state, the southern part of the Indian peninsula situated at latitude -longitude pairs (8o29'N - 76o57' E). We used an artificial neural network trained using the scaled conjugate gradient algorithm. The neural network and MARS were trained with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that MARS is a good forecasting tool and performed better than the considered neural network.",,"Bio-Inspired Applications of Connectionism, Lecture Notes in
  Computer Science. Volume. 2085, Springer Verlag Germany, Jose Mira and
  Alberto Prieto (Eds.), ISBN 3540422374, Spain, pp.679-686, 2001",,cs.AI,"['cs.AI', 'I.2.0']","['http://arxiv.org/abs/cs/0405012v1', 'http://arxiv.org/pdf/cs/0405012v1']",http://arxiv.org/pdf/cs/0405012v1
159,http://arxiv.org/abs/cs/0102014v1,2001-02-18 19:17:18+00:00,2001-02-18 19:17:18+00:00,On the predictability of Rainfall in Kerala- An application of ABF Neural Network,"['Ninan Sajeeth Philip', 'K. Babu Joseph']","Rainfall in Kerala State, the southern part of Indian Peninsula in particular is caused by the two monsoons and the two cyclones every year. In general, climate and rainfall are highly nonlinear phenomena in nature giving rise to what is known as the `butterfly effect'. We however attempt to train an ABF neural network on the time series rainfall data and show for the first time that in spite of the fluctuations resulting from the nonlinearity in the system, the trends in the rainfall pattern in this corner of the globe have remained unaffected over the past 87 years from 1893 to 1980. We also successfully filter out the chaotic part of the system and illustrate that its effects are marginal over long term predictions.",,,,cs.NE,"['cs.NE', 'cs.AI', 'A0']","['http://arxiv.org/abs/cs/0102014v1', 'http://arxiv.org/pdf/cs/0102014v1']",http://arxiv.org/pdf/cs/0102014v1
160,http://arxiv.org/abs/q-bio/0511046v1,2005-11-28 19:47:28+00:00,2005-11-28 19:47:28+00:00,Improving ecological niche models by data mining large environmental datasets for surrogate models,['David R. B. Stockwell'],"WhyWhere is a new ecological niche modeling (ENM) algorithm for mapping and explaining the distribution of species. The algorithm uses image processing methods to efficiently sift through large amounts of data to find the few variables that best predict species occurrence. The purpose of this paper is to describe and justify the main parameterizations and to show preliminary success at rapidly providing accurate, scalable, and simple ENMs. Preliminary results for 6 species of plants and animals in different regions indicate a significant (p<0.01) 14% increase in accuracy over the GARP algorithm using models with few, typically two, variables. The increase is attributed to access to additional data, particularly monthly vs. annual climate averages. WhyWhere is also 6 times faster than GARP on large data sets. A data mining based approach with transparent access to remote data archives is a new paradigm for ENM, particularly suited to finding correlates in large databases of fine resolution surfaces. Software for WhyWhere is freely available, both as a service and in a desktop downloadable form from the web site http://biodi.sdsc.edu/ww_home.html.","16 pages, 4 figures, to appear in Ecological Modelling",,,q-bio.QM,"['q-bio.QM', 'cs.AI']","['http://arxiv.org/abs/q-bio/0511046v1', 'http://arxiv.org/pdf/q-bio/0511046v1']",http://arxiv.org/pdf/q-bio/0511046v1
161,http://arxiv.org/abs/1704.07899v2,2017-09-05 11:02:03+00:00,2017-04-25 20:24:17+00:00,Reinforcement Learning-based Thermal Comfort Control for Vehicle Cabins,"['James Brusey', 'Diana Hintea', 'Elena Gaura', 'Neil Beloe']","Vehicle climate control systems aim to keep passengers thermally comfortable. However, current systems control temperature rather than thermal comfort and tend to be energy hungry, which is of particular concern when considering electric vehicles. This paper poses energy-efficient vehicle comfort control as a Markov Decision Process, which is then solved numerically using Sarsa({\lambda}) and an empirically validated, single-zone, 1D thermal model of the cabin. The resulting controller was tested in simulation using 200 randomly selected scenarios and found to exceed the performance of bang-bang, proportional, simple fuzzy logic, and commercial controllers with 23%, 43%, 40%, 56% increase, respectively. Compared to the next best performing controller, energy consumption is reduced by 13% while the proportion of time spent thermally comfortable is increased by 23%. These results indicate that this is a viable approach that promises to translate into substantial comfort and energy improvements in the car.",,,,cs.AI,['cs.AI'],"['http://arxiv.org/abs/1704.07899v2', 'http://arxiv.org/pdf/1704.07899v2']",http://arxiv.org/pdf/1704.07899v2
162,http://arxiv.org/abs/1810.13075v2,2019-01-31 00:11:32+00:00,2018-10-31 02:21:19+00:00,Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles,"['Xiaowei Jia', 'Jared Willard', 'Anuj Karpatne', 'Jordan Read', 'Jacob Zwart', 'Michael Steinbach', 'Vipin Kumar']","This paper proposes a physics-guided recurrent neural network model (PGRNN) that combines RNNs and physics-based models to leverage their complementary strengths and improve the modeling of physical processes. Specifically, we show that a PGRNN can improve prediction accuracy over that of physical models, while generating outputs consistent with physical laws, and achieving good generalizability. Standard RNNs, even when producing superior prediction accuracy, often produce physically inconsistent results and lack generalizability. We further enhance this approach by using a pre-training method that leverages the simulated data from a physics-based model to address the scarcity of observed data. The PGRNN has the flexibility to incorporate additional physical constraints and we incorporate a density-depth relationship. Both enhancements further improve PGRNN performance. Although we present and evaluate this methodology in the context of modeling the dynamics of temperature in lakes, it is applicable more widely to a range of scientific and engineering disciplines where mechanistic (also known as process-based) models are used, e.g., power engineering, climate science, materials science, computational chemistry, and biomedicine.",,,,physics.comp-ph,"['physics.comp-ph', 'cs.AI']","['http://arxiv.org/abs/1810.13075v2', 'http://arxiv.org/pdf/1810.13075v2']",http://arxiv.org/pdf/1810.13075v2
163,http://arxiv.org/abs/1905.05888v1,2019-05-14 23:53:57+00:00,2019-05-14 23:53:57+00:00,Generative Design in Minecraft: Chronicle Challenge,"['Christoph Salge', 'Christian Guckelsberger', 'Michael Cerny Green', 'Rodrigo Canaan', 'Julian Togelius']","We introduce the Chronicle Challenge as an optional addition to the Settlement Generation Challenge in Minecraft. One of the foci of the overall competition is adaptive procedural content generation (PCG), an arguably under-explored problem in computational creativity. In the base challenge, participants must generate new settlements that respond to and ideally interact with existing content in the world, such as the landscape or climate. The goal is to understand the underlying creative process, and to design better PCG systems. The Chronicle Challenge in particular focuses on the generation of a narrative based on the history of a generated settlement, expressed in natural language. We discuss the unique features of the Chronicle Challenge in comparison to other competitions, clarify the characteristics of a chronicle eligible for submission and describe the evaluation criteria. We furthermore draw on simulation-based approaches in computational storytelling as examples to how this challenge could be approached.","5 pages, 1 Figure, accepted as late-breaking paper at ICCC 2019, 10th
  International Conference on Computational Creativity",,,cs.AI,"['cs.AI', 'cs.CL', 'cs.CY']","['http://arxiv.org/abs/1905.05888v1', 'http://arxiv.org/pdf/1905.05888v1']",http://arxiv.org/pdf/1905.05888v1
164,http://arxiv.org/abs/1906.04595v1,2019-06-10 01:18:35+00:00,2019-06-10 01:18:35+00:00,Evaluating aleatoric and epistemic uncertainties of time series deep learning models for soil moisture predictions,"['Kuai Fang', 'Chaopeng Shen', 'Daniel Kifer']","Soil moisture is an important variable that determines floods, vegetation health, agriculture productivity, and land surface feedbacks to the atmosphere, etc. Accurately modeling soil moisture has important implications in both weather and climate models. The recently available satellite-based observations give us a unique opportunity to build data-driven models to predict soil moisture instead of using land surface models, but previously there was no uncertainty estimate. We tested Monte Carlo dropout (MCD) with an aleatoric term for our long short-term memory models for this problem, and asked if the uncertainty terms behave as they were argued to. We show that the method successfully captures the predictive error after tuning a hyperparameter on a representative training dataset. We show the MCD uncertainty estimate, as previously argued, does detect dissimilarity.",,Water Resources Research (2020),10.1029/2020WR028095,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","['http://dx.doi.org/10.1029/2020WR028095', 'http://arxiv.org/abs/1906.04595v1', 'http://arxiv.org/pdf/1906.04595v1']",http://arxiv.org/pdf/1906.04595v1
165,http://arxiv.org/abs/2007.01019v1,2020-07-02 11:07:53+00:00,2020-07-02 11:07:53+00:00,Higher-order Logic as Lingua Franca -- Integrating Argumentative Discourse and Deep Logical Analysis,"['David Fuenmayor', 'Christoph Benzmüller']","We present an approach towards the deep, pluralistic logical analysis of argumentative discourse that benefits from the application of state-of-the-art automated reasoning technology for classical higher-order logic. Thanks to its expressivity this logic can adopt the status of a uniform \textit{lingua franca} allowing the encoding of both formalized arguments (their deep logical structure) and dialectical interactions (their attack and support relations). We illustrate this by analyzing an excerpt from an argumentative debate on climate engineering.   Another, novel contribution concerns the definition of abstract, language-theoretical foundations for the characterization and assessment of shallow semantical embeddings (SSEs) of non-classical logics in classical higher-order logic, which constitute a pillar stone of our approach.   The novel perspective we draw enables more concise and more elegant characterizations of semantical embeddings of logics and logic combinations, which is demonstrated with several examples.","35 pages, 1 figure",,,cs.AI,"['cs.AI', 'cs.LO', '03B60, 03B15, 68T27, 68T30, 68T15', 'I.2.3; I.2.4; I.2.0; F.4']","['http://arxiv.org/abs/2007.01019v1', 'http://arxiv.org/pdf/2007.01019v1']",http://arxiv.org/pdf/2007.01019v1
166,http://arxiv.org/abs/2009.08002v2,2020-11-27 08:08:46+00:00,2020-09-17 01:17:13+00:00,Planting trees at the right places: Recommending suitable sites for growing trees using algorithm fusion,"['Pushpendra Rana', 'Lav R Varshney']","Large-scale planting of trees has been proposed as a low-cost natural solution for carbon mitigation, but is hampered by poor selection of plantation sites, especially in developing countries. To aid in site selection, we develop the ePSA (e-Plantation Site Assistant) recommendation system based on algorithm fusion that combines physics-based/traditional forestry science knowledge with machine learning. ePSA assists forest range officers by identifying blank patches inside forest areas and ranking each such patch based on their tree growth potential. Experiments, user studies, and deployment results characterize the utility of the recommender system in shaping the long-term success of tree plantations as a nature climate solution for carbon mitigation in northern India and beyond.","26 pages, 4 figures, 2 tables, 2 supplemental tables",,,cs.CY,"['cs.CY', 'cs.AI']","['http://arxiv.org/abs/2009.08002v2', 'http://arxiv.org/pdf/2009.08002v2']",http://arxiv.org/pdf/2009.08002v2
167,http://arxiv.org/abs/2012.14516v2,2021-01-27 16:21:31+00:00,2020-12-28 22:52:57+00:00,Synergy between Observation Systems Oceanic in Turbulent Regions,"['Van-Khoa Nguyen', 'Santiago Agudelo']","Ocean dynamics constitute a source of incertitude in determining the ocean's role in complex climatic phenomena. Current observation systems have limitations in achieving sufficiently statistical precision for three-dimensional oceanic data. It is crucial knowledge to describe the behavior of internal ocean structures. We present the data-driven approaches which explore latent class regressions and deep regression neural networks in modeling ocean dynamics in the extensions of Gulf Stream and Kuroshio currents. The obtained results show a promising data-driven direction for understanding the ocean's characteristics, including salinity and temperature, in both spatial and temporal dimensions in the turbulent regions. Our source codes are publicly available at https://github.com/v18nguye/gulfstream-lrm and at https://github.com/sagudelor/Kuroshio.",,,,physics.ao-ph,"['physics.ao-ph', 'cs.AI']","['http://arxiv.org/abs/2012.14516v2', 'http://arxiv.org/pdf/2012.14516v2']",http://arxiv.org/pdf/2012.14516v2
168,http://arxiv.org/abs/2103.07919v1,2021-03-14 13:04:04+00:00,2021-03-14 13:04:04+00:00,Simulation Studies on Deep Reinforcement Learning for Building Control with Human Interaction,"['Donghwan Lee', 'Niao He', 'Seungjae Lee', 'Panagiota Karava', 'Jianghai Hu']","The building sector consumes the largest energy in the world, and there have been considerable research interests in energy consumption and comfort management of buildings. Inspired by recent advances in reinforcement learning (RL), this paper aims at assessing the potential of RL in building climate control problems with occupant interaction. We apply a recent RL approach, called DDPG (deep deterministic policy gradient), for the continuous building control tasks and assess its performance with simulation studies in terms of its ability to handle (a) the partial state observability due to sensor limitations; (b) complex stochastic system with high-dimensional state-spaces, which are jointly continuous and discrete; (c) uncertainties due to ambient weather conditions, occupant's behavior, and comfort feelings. Especially, the partial observability and uncertainty due to the occupant interaction significantly complicate the control problem. Through simulation studies, the policy learned by DDPG demonstrates reasonable performance and computational tractability.",,,,cs.AI,"['cs.AI', 'cs.SY', 'eess.SY']","['http://arxiv.org/abs/2103.07919v1', 'http://arxiv.org/pdf/2103.07919v1']",http://arxiv.org/pdf/2103.07919v1
169,http://arxiv.org/abs/2104.03226v1,2021-04-07 16:24:39+00:00,2021-04-07 16:24:39+00:00,Evaluation of Time Series Forecasting Models for Estimation of PM2.5 Levels in Air,"['Satvik Garg', 'Himanshu Jindal']","Air contamination in urban areas has risen consistently over the past few years. Due to expanding industrialization and increasing concentration of toxic gases in the climate, the air is getting more poisonous step by step at an alarming rate. Since the arrival of the Coronavirus pandemic, it is getting more critical to lessen air contamination to reduce its impact. The specialists and environmentalists are making a valiant effort to gauge air contamination levels. However, its genuinely unpredictable to mimic subatomic communication in the air, which brings about off base outcomes. There has been an ascent in using machine learning and deep learning models to foresee the results on time series data. This study adopts ARIMA, FBProphet, and deep learning models such as LSTM, 1D CNN, to estimate the concentration of PM2.5 in the environment. Our predicted results convey that all adopted methods give comparative outcomes in terms of average root mean squared error. However, the LSTM outperforms all other models with reference to mean absolute percentage error.","8 pages, This paper is accepted and presented in the IEEE 6th I2CT
  2021 conference. The final version of this paper will appear in the
  conference proceedings",,10.1109/I2CT51068.2021.9418215,cs.LG,"['cs.LG', 'cs.AI']","['http://dx.doi.org/10.1109/I2CT51068.2021.9418215', 'http://arxiv.org/abs/2104.03226v1', 'http://arxiv.org/pdf/2104.03226v1']",http://arxiv.org/pdf/2104.03226v1
170,http://arxiv.org/abs/2105.00557v1,2021-05-02 21:40:39+00:00,2021-05-02 21:40:39+00:00,Hard Encoding of Physics for Learning Spatiotemporal Dynamics,"['Chengping Rao', 'Hao Sun', 'Yang Liu']","Modeling nonlinear spatiotemporal dynamical systems has primarily relied on partial differential equations (PDEs). However, the explicit formulation of PDEs for many underexplored processes, such as climate systems, biochemical reaction and epidemiology, remains uncertain or partially unknown, where very limited measurement data is yet available. To tackle this challenge, we propose a novel deep learning architecture that forcibly encodes known physics knowledge to facilitate learning in a data-driven manner. The coercive encoding mechanism of physics, which is fundamentally different from the penalty-based physics-informed learning, ensures the network to rigorously obey given physics. Instead of using nonlinear activation functions, we propose a novel elementwise product operation to achieve the nonlinearity of the model. Numerical experiment demonstrates that the resulting physics-encoded learning paradigm possesses remarkable robustness against data noise/scarcity and generalizability compared with some state-of-the-art models for data-driven modeling.",,ICLR 2021 SimDL Workshop,,cs.LG,"['cs.LG', 'cs.AI', 'physics.comp-ph']","['http://arxiv.org/abs/2105.00557v1', 'http://arxiv.org/pdf/2105.00557v1']",http://arxiv.org/pdf/2105.00557v1
171,http://arxiv.org/abs/2108.04232v2,2022-02-21 06:03:55+00:00,2021-08-07 05:50:54+00:00,GANmapper: geographical data translation,"['Abraham Noah Wu', 'Filip Biljecki']","We present a new method to create spatial data using a generative adversarial network (GAN). Our contribution uses coarse and widely available geospatial data to create maps of less available features at the finer scale in the built environment, bypassing their traditional acquisition techniques (e.g. satellite imagery or land surveying). In the work, we employ land use data and road networks as input to generate building footprints and conduct experiments in 9 cities around the world. The method, which we implement in a tool we release openly, enables the translation of one geospatial dataset to another with high fidelity and morphological accuracy. It may be especially useful in locations missing detailed and high-resolution data and those that are mapped with uncertain or heterogeneous quality, such as much of OpenStreetMap. The quality of the results is influenced by the urban form and scale. In most cases, the experiments suggest promising performance as the method tends to truthfully indicate the locations, amount, and shape of buildings. The work has the potential to support several applications, such as energy, climate, and urban morphology studies in areas previously lacking required data or inpainting geospatial data in regions with incomplete data.",,International Journal of Geographical Information Science 2022,10.1080/13658816.2022.2041643,cs.CV,"['cs.CV', 'cs.AI']","['http://dx.doi.org/10.1080/13658816.2022.2041643', 'http://arxiv.org/abs/2108.04232v2', 'http://arxiv.org/pdf/2108.04232v2']",http://arxiv.org/pdf/2108.04232v2
172,http://arxiv.org/abs/2108.06670v1,2021-08-15 06:57:36+00:00,2021-08-15 06:57:36+00:00,Deep Geospatial Interpolation Networks,"['Sumit Kumar Varshney', 'Jeetu Kumar', 'Aditya Tiwari', 'Rishabh Singh', 'Venkata M. V. Gunturi', 'Narayanan C. Krishnan']","Interpolation in Spatio-temporal data has applications in various domains such as climate, transportation, and mining. Spatio-Temporal interpolation is highly challenging due to the complex spatial and temporal relationships. However, traditional techniques such as Kriging suffer from high running time and poor performance on data that exhibit high variance across space and time dimensions. To this end, we propose a novel deep neural network called as Deep Geospatial Interpolation Network(DGIN), which incorporates both spatial and temporal relationships and has significantly lower training time. DGIN consists of three major components: Spatial Encoder to capture the spatial dependencies, Sequential module to incorporate the temporal dynamics, and an Attention block to learn the importance of the temporal neighborhood around the gap. We evaluate DGIN on the MODIS reflectance dataset from two different regions. Our experimental results indicate that DGIN has two advantages: (a) it outperforms alternative approaches (has lower MSE with p-value < 0.01) and, (b) it has significantly low execution time than Kriging.",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2108.06670v1', 'http://arxiv.org/pdf/2108.06670v1']",http://arxiv.org/pdf/2108.06670v1
173,http://arxiv.org/abs/2110.05357v2,2022-03-16 03:33:15+00:00,2021-10-11 15:37:58+00:00,Graph-Guided Network for Irregularly Sampled Multivariate Time Series,"['Xiang Zhang', 'Marko Zeman', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']","In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings.",Accepted by ICLR 2022; https://github.com/mims-harvard/Raindrop,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2110.05357v2', 'http://arxiv.org/pdf/2110.05357v2']",http://arxiv.org/pdf/2110.05357v2
174,http://arxiv.org/abs/2207.11417v1,2022-07-23 05:01:03+00:00,2022-07-23 05:01:03+00:00,Multiscale Neural Operator: Learning Fast and Grid-independent PDE Solvers,"['Björn Lütjens', 'Catherine H. Crawford', 'Campbell D Watson', 'Christopher Hill', 'Dava Newman']","Numerical simulations in climate, chemistry, or astrophysics are computationally too expensive for uncertainty quantification or parameter-exploration at high-resolution. Reduced-order or surrogate models are multiple orders of magnitude faster, but traditional surrogates are inflexible or inaccurate and pure machine learning (ML)-based surrogates too data-hungry. We propose a hybrid, flexible surrogate model that exploits known physics for simulating large-scale dynamics and limits learning to the hard-to-model term, which is called parametrization or closure and captures the effect of fine- onto large-scale dynamics. Leveraging neural operators, we are the first to learn grid-independent, non-local, and flexible parametrizations. Our \textit{multiscale neural operator} is motivated by a rich literature in multiscale modeling, has quasilinear runtime complexity, is more accurate or flexible than state-of-the-art parametrizations and demonstrated on the chaotic equation multiscale Lorenz96.","Presented at International Conference on Machine Learning Workshop AI
  for Science, 2022",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE', 'cs.DC']","['http://arxiv.org/abs/2207.11417v1', 'http://arxiv.org/pdf/2207.11417v1']",http://arxiv.org/pdf/2207.11417v1
175,http://arxiv.org/abs/2209.11770v1,2022-09-23 12:37:11+00:00,2022-09-23 12:37:11+00:00,Toward Smart Doors: A Position Paper,"['Luigi Capogrosso', 'Geri Skenderi', 'Federico Girella', 'Franco Fummi', 'Marco Cristani']","Conventional automatic doors cannot distinguish between people wishing to pass through the door and people passing by the door, so they often open unnecessarily. This leads to the need to adopt new systems in both commercial and non-commercial environments: smart doors. In particular, a smart door system predicts the intention of people near the door based on the social context of the surrounding environment and then makes rational decisions about whether or not to open the door. This work proposes the first position paper related to smart doors, without bells and whistles. We first point out that the problem not only concerns reliability, climate control, safety, and mode of operation. Indeed, a system to predict the intention of people near the door also involves a deeper understanding of the social context of the scene through a complex combined analysis of proxemics and scene reasoning. Furthermore, we conduct an exhaustive literature review about automatic doors, providing a novel system formulation. Also, we present an analysis of the possible future application of smart doors, a description of the ethical shortcomings, and legislative issues.",2nd International Workshop on Industrial Machine Learning @ ICPR 2022,,,cs.HC,"['cs.HC', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2209.11770v1', 'http://arxiv.org/pdf/2209.11770v1']",http://arxiv.org/pdf/2209.11770v1
176,http://arxiv.org/abs/2107.13870v1,2021-07-29 10:11:45+00:00,2021-07-29 10:11:45+00:00,Artificial Intelligence Hybrid Deep Learning Model for Groundwater Level Prediction Using MLP-ADAM,"['Pejman Zarafshan', 'Saman Javadi', 'Abbas Roozbahani', 'Seyed Mehdi Hashemy', 'Payam Zarafshan', 'Hamed Etezadi']","Groundwater is the largest storage of freshwater resources, which serves as the major inventory for most of the human consumption through agriculture, industrial, and domestic water supply. In the fields of hydrological, some researchers applied a neural network to forecast rainfall intensity in space-time and introduced the advantages of neural networks compared to numerical models. Then, many researches have been conducted applying data-driven models. Some of them extended an Artificial Neural Networks (ANNs) model to forecast groundwater level in semi-confined glacial sand and gravel aquifer under variable state, pumping extraction and climate conditions with significant accuracy. In this paper, a multi-layer perceptron is applied to simulate groundwater level. The adaptive moment estimation optimization algorithm is also used to this matter. The root mean squared error, mean absolute error, mean squared error and the coefficient of determination ( ) are used to evaluate the accuracy of the simulated groundwater level. Total value of and RMSE are 0.9458 and 0.7313 respectively which are obtained from the model output. Results indicate that deep learning algorithms can demonstrate a high accuracy prediction. Although the optimization of parameters is insignificant in numbers, but due to the value of time in modelling setup, it is highly recommended to apply an optimization algorithm in modelling.",,,,cs.LG,"['cs.LG', 'physics.geo-ph']","['http://arxiv.org/abs/2107.13870v1', 'http://arxiv.org/pdf/2107.13870v1']",http://arxiv.org/pdf/2107.13870v1
177,http://arxiv.org/abs/1911.06440v2,2019-12-11 19:01:02+00:00,2019-11-15 01:21:46+00:00,Resilience of Urban Transport Network-of-Networks under Intense Flood Hazards Exacerbated by Targeted Attacks,"['Nishant Yadav', 'Samrat Chatterjee', 'Auroop R. Ganguly']","Natural hazards including floods can trigger catastrophic failures in interdependent urban transport network-of-networks (NoNs). Population growth has enhanced transportation demand while urbanization and climate change have intensified urban floods. However, despite the clear need to develop actionable insights for improving the resilience of critical urban lifelines, the theory and methods remain underdeveloped. Furthermore, as infrastructure systems become more intelligent, security experts point to the growing threat of targeted cyber-physical attacks during natural hazards. Here we develop a hypothesis-driven resilience framework for urban transport NoNs, which we demonstrate on the London Rail Network (LRN). We find that topological attributes designed for maximizing efficiency rather than robustness render the network more vulnerable to compound natural-targeted threats including cascading failures. Our results suggest that an organizing principle for post-disruption recovery may be developed with network science principles. Our findings and frameworks can generalize to urban lifelines and more generally to real-world spatial networks.",,,,physics.soc-ph,['physics.soc-ph'],"['http://arxiv.org/abs/1911.06440v2', 'http://arxiv.org/pdf/1911.06440v2']",http://arxiv.org/pdf/1911.06440v2
178,http://arxiv.org/abs/2011.02773v1,2020-11-05 11:55:42+00:00,2020-11-05 11:55:42+00:00,"The Gray Rhino of Pandemic Preparedness: Proactive digital, data, and organizational infrastructure to help humanity build resilience in the face of pandemics",['Abhishek Gupta'],"COVID-19 has exposed glaring holes in our existing digital, data, and organizational practices. Researchers ensconced in epidemiological and human health work have repeatedly pointed out how urban encroachment, climate change, and other human-triggered activities and patterns are going to make zoonotic pandemics more frequent and commonplace. The Gray Rhino mindset provides a useful reframing (as opposed to viewing pandemics such as the current one as a Black Swan event) that can help us recover faster from these (increasingly) frequent occurrences and build resiliency in our digital, data, and organizational infrastructure. Mitigating the social and economic impacts of pandemics can be eased through building infrastructure that elucidate leading indicators via passive intelligence gathering so that responses to containing the spread of pandemics are not blanket measures; instead, they can be fine-grained allowing for more efficient utilization of scarce resources and minimizing disruption to our way of life.",5 pages,,,cs.CY,['cs.CY'],"['http://arxiv.org/abs/2011.02773v1', 'http://arxiv.org/pdf/2011.02773v1']",http://arxiv.org/pdf/2011.02773v1
179,http://arxiv.org/abs/2012.04407v2,2022-01-23 10:04:08+00:00,2020-12-08 12:55:29+00:00,Enhanced spatio-temporal electric load forecasts using less data with active deep learning,"['Arsam Aryandoust', 'Anthony Patt', 'Stefan Pfenninger']","An effective way to oppose global warming and mitigate climate change is to electrify our energy sectors and supply their electric power from renewable wind and solar. Spatio-temporal predictions of electric load become increasingly important for planning this transition, while deep learning prediction models provide increasingly accurate predictions for it. The data used for training deep learning models, however, is usually collected at random using a passive learning approach. This naturally results in a large demand for data and associated costs for sensors like smart meters, posing a large barrier for electric utilities in decarbonizing their grids. Here, we test active learning where we leverage additional computation for collecting a more informative subset of data. We show how electric utilities can apply active learning to better distribute smart meters and collect their data for more accurate predictions of load with about half the data compared to when applying passive learning.",,"Nature Machine Intelligence 4, 977-991 (2022)",10.1038/s42256-022-00552-x,cs.LG,"['cs.LG', 'stat.ML']","['http://dx.doi.org/10.1038/s42256-022-00552-x', 'http://arxiv.org/abs/2012.04407v2', 'http://arxiv.org/pdf/2012.04407v2']",http://arxiv.org/pdf/2012.04407v2
180,http://arxiv.org/abs/2102.11558v1,2021-02-23 08:58:37+00:00,2021-02-23 08:58:37+00:00,EscapeWildFire: Assisting People to Escape Wildfires in Real-Time,"['Andreas Kamilaris', 'Jean-Baptiste Filippi', 'Chirag Padubidri', 'Jesper Provoost', 'Savvas Karatsiolis', 'Ian Cole', 'Wouter Couwenbergh', 'Evi Demetriou']","Over the past couple of decades, the number of wildfires and area of land burned around the world has been steadily increasing, partly due to climatic changes and global warming. Therefore, there is a high probability that more people will be exposed to and endangered by forest fires. Hence there is an urgent need to design pervasive systems that effectively assist people and guide them to safety during wildfires. This paper presents EscapeWildFire, a mobile application connected to a backend system which models and predicts wildfire geographical progression, assisting citizens to escape wildfires in real-time. A small pilot indicates the correctness of the system. The code is open-source; fire authorities around the world are encouraged to adopt this approach.","6th IEEE International Workshop on Pervasive Context-Aware Smart
  Cities and Intelligent Transport System (PerAwareCity), Proc. of PerCom 2021,
  Kassel, Germany, March, 2021",,,cs.CY,"['cs.CY', 'cs.CV']","['http://arxiv.org/abs/2102.11558v1', 'http://arxiv.org/pdf/2102.11558v1']",http://arxiv.org/pdf/2102.11558v1
181,http://arxiv.org/abs/2110.12896v1,2021-10-20 12:47:18+00:00,2021-10-20 12:47:18+00:00,Classification of PS and ABS Black Plastics for WEEE Recycling Applications,"['Anton Persson', 'Niklas Dymne', 'Fernando Alonso-Fernandez']","Pollution and climate change are some of the biggest challenges that humanity is facing. In such a context, efficient recycling is a crucial tool for a sustainable future. This work is aimed at creating a system that can classify different types of plastics by using picture analysis, in particular, black plastics of the type Polystyrene (PS) and Acrylonitrile Butadiene Styrene (ABS). They are two common plastics from Waste from Electrical and Electronic Equipment (WEEE). For this purpose, a Convolutional Neural Network has been tested and retrained, obtaining a validation accuracy of 95%. Using a separate test set, average accuracy goes down to 86.6%, but a further look at the results shows that the ABS type is correctly classified 100% of the time, so it is the PS type that accumulates all the errors. Overall, this demonstrates the feasibility of classifying black plastics using CNN machine learning techniques. It is believed that if a more diverse and extensive image dataset becomes available, a system with higher reliability that generalizes well could be developed using the proposed methodology.","Published at 8th Intl. Conference on Soft Computing & Machine
  Intelligence, ISCMI, Cairo, Egypt 26-27 November 2021",,,cs.LG,"['cs.LG', 'cs.CV']","['http://arxiv.org/abs/2110.12896v1', 'http://arxiv.org/pdf/2110.12896v1']",http://arxiv.org/pdf/2110.12896v1
182,http://arxiv.org/abs/2209.03226v1,2022-09-07 15:29:26+00:00,2022-09-07 15:29:26+00:00,On the Importance of Quantifying Visibility for Autonomous Vehicles under Extreme Precipitation,"['Clément Courcelle', 'Dominic Baril', 'François Pomerleau', 'Johann Laconte']","In the context of autonomous driving, vehicles are inherently bound to encounter more extreme weather during which public safety must be ensured. As climate is quickly changing, the frequency of heavy snowstorms is expected to increase and become a major threat to safe navigation. While there is much literature aiming to improve navigation resiliency to winter conditions, there is a lack of standard metrics to quantify the loss of visibility of lidar sensors related to precipitation. This chapter proposes a novel metric to quantify the lidar visibility loss in real time, relying on the notion of visibility from the meteorology research field. We evaluate this metric on the Canadian Adverse Driving Conditions (CADC) dataset, correlate it with the performance of a state-of-the-art lidar-based localization algorithm, and evaluate the benefit of filtering point clouds before the localization process. We show that the Iterative Closest Point (ICP) algorithm is surprisingly robust against snowfalls, but abrupt events, such as snow gusts, can greatly hinder its accuracy. We discuss such events and demonstrate the need for better datasets focusing on these extreme events to quantify their effect.","Submitted to Intelligent Vehicles and Transportation Volume 3 - De
  Gruyter",,,cs.RO,['cs.RO'],"['http://arxiv.org/abs/2209.03226v1', 'http://arxiv.org/pdf/2209.03226v1']",http://arxiv.org/pdf/2209.03226v1
183,http://arxiv.org/abs/1311.0759v1,2013-11-04 16:30:07+00:00,2013-11-04 16:30:07+00:00,Virulence as a Model for Interplanetary and Interstellar Colonisation - Parasitism or Mutualism,"['Jonathan Starling', 'Duncan Forgan']","In the light of current scientific assessments of human-induced climate change, we investigate an experimental model to inform how resource-use strategies may influence interplanetary and interstellar colonisation by intelligent civilisations. In doing so, we seek to provide an additional aspect for refining the famed Fermi Paradox. The model described is necessarily simplistic, and the intent is to simply obtain some general insights to inform and inspire additional models. We model the relationship between an intelligent civilisation and its host planet as symbiotic, where the the relationship between the symbiont and the host species (the civilisation and the planets ecology, respectively) determines the fitness and ultimate survival of both organisms.   We perform a series of Monte Carlo Realisation simulations, where civilisations pursue a variety of different relationships/strategies with their host planet, from mutualism to parasitism, and can consequently 'infect' other planets/hosts. We find that parasitic civilisations are generally less effective at survival than mutualist civilisations, provided that interstellar colonisation is inefficient (the maximum velocity of colonisation/infection is low). However, as the colonisation velocity is increased, the strategy of parasitism becomes more successful, until they dominate the 'population'. This is in accordance with predictions based on island biogeography and r/K selection theory. While heavily assumption dependent, we contend that this provides a fertile approach for further application of insights from theoretical ecology for extraterrestrial colonisation - while also potentially offering insights for understanding the human-Earth relationship and the potential for extraterrestrial human colonisation.","18 pages, 7 figures, published in the International Journal of
  Astrobiology",,10.1017/S1473550413000347,physics.pop-ph,"['physics.pop-ph', 'astro-ph.IM']","['http://dx.doi.org/10.1017/S1473550413000347', 'http://arxiv.org/abs/1311.0759v1', 'http://arxiv.org/pdf/1311.0759v1']",http://arxiv.org/pdf/1311.0759v1
184,http://arxiv.org/abs/1901.00761v1,2019-01-03 14:20:44+00:00,2019-01-03 14:20:44+00:00,"Robotic Tankette for Intelligent BioEnergy Agriculture: Design, Development and Field Tests","['Marco F. S. Xaud', 'Antonio C. Leite', 'Evelyn S. Barbosa', 'Henrique D. Faria', 'Gabriel S. M. Loureiro', 'Pål J. From']","In recent years, the use of robots in agriculture has been increasing mainly due to the high demand of productivity, precision and efficiency, which follow the climate change effects and world population growth. Unlike conventional agriculture, sugarcane farms are usually regions with dense vegetation, gigantic areas, and subjected to extreme weather conditions, such as intense heat, moisture and rain. TIBA - Tankette for Intelligent BioEnergy Agriculture - is the first result of an R&D project which strives to develop an autonomous mobile robotic system for carrying out a number of agricultural tasks in sugarcane fields. The proposed concept consists of a semi-autonomous, low-cost, dust and waterproof tankette-type vehicle, capable of infiltrating dense vegetation in plantation tunnels and carry several sensing systems, in order to perform mapping of hard-to-access areas and collecting samples. This paper presents an overview of the robot mechanical design, the embedded electronics and software architecture, and the construction of a first prototype. Preliminary results obtained in field tests validate the proposed conceptual design and bring about several challenges and potential applications for robot autonomous navigation, as well as to build a new prototype with additional functionality.","9 pages, 15 figures","The XXII Brazilian Conference on Automation. SBA, 2018",,cs.RO,"['cs.RO', '68T40 (Primary) 70B15 (Secondary)']","['http://arxiv.org/abs/1901.00761v1', 'http://arxiv.org/pdf/1901.00761v1']",http://arxiv.org/pdf/1901.00761v1
185,http://arxiv.org/abs/2007.10981v1,2020-07-21 17:58:11+00:00,2020-07-21 17:58:11+00:00,Forecasting Brazilian and American COVID-19 cases based on artificial intelligence coupled with climatic exogenous variables,"['Ramon Gomes da Silva', 'Matheus Henrique Dal Molin Ribeiro', 'Viviana Cocco Mariani', 'Leandro dos Santos Coelho']","The novel coronavirus disease (COVID-19) is a public health problem once according to the World Health Organization up to June 10th, 2020, more than 7.1 million people were infected, and more than 400 thousand have died worldwide. In the current scenario, the Brazil and the United States of America present a high daily incidence of new cases and deaths. It is important to forecast the number of new cases in a time window of one week, once this can help the public health system developing strategic planning to deals with the COVID-19. In this paper, Bayesian regression neural network, cubist regression, k-nearest neighbors, quantile random forest, and support vector regression, are used stand-alone, and coupled with the recent pre-processing variational mode decomposition (VMD) employed to decompose the time series into several intrinsic mode functions. All Artificial Intelligence techniques are evaluated in the task of time-series forecasting with one, three, and six-days-ahead the cumulative COVID-19 cases in five Brazilian and American states up to April 28th, 2020. Previous cumulative COVID-19 cases and exogenous variables as daily temperature and precipitation were employed as inputs for all forecasting models. The hybridization of VMD outperformed single forecasting models regarding the accuracy, specifically when the horizon is six-days-ahead, achieving better accuracy in 70% of the cases. Regarding the exogenous variables, the importance ranking as predictor variables is past cases, temperature, and precipitation. Due to the efficiency of evaluated models to forecasting cumulative COVID-19 cases up to six-days-ahead, the adopted models can be recommended as a promising models for forecasting and be used to assist in the development of public policies to mitigate the effects of COVID-19 outbreak.","24 pages, 6 figures. Published paper","Chaos, Solitons & Fractals. 139 (2020) 110027",10.1016/j.chaos.2020.110027,q-bio.PE,"['q-bio.PE', 'cs.LG']","['http://dx.doi.org/10.1016/j.chaos.2020.110027', 'http://arxiv.org/abs/2007.10981v1', 'http://arxiv.org/pdf/2007.10981v1']",http://arxiv.org/pdf/2007.10981v1
186,http://arxiv.org/abs/1612.08544v2,2017-11-13 17:42:12+00:00,2016-12-27 09:14:16+00:00,Theory-guided Data Science: A New Paradigm for Scientific Discovery from Data,"['Anuj Karpatne', 'Gowtham Atluri', 'James Faghmous', 'Michael Steinbach', 'Arindam Banerjee', 'Auroop Ganguly', 'Shashi Shekhar', 'Nagiza Samatova', 'Vipin Kumar']","Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",,"IEEE Transactions on Knowledge and Data Engineering, 29(10),
  pp.2318-2331. 2017",10.1109/TKDE.2017.2720168,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","['http://dx.doi.org/10.1109/TKDE.2017.2720168', 'http://arxiv.org/abs/1612.08544v2', 'http://arxiv.org/pdf/1612.08544v2']",http://arxiv.org/pdf/1612.08544v2
187,http://arxiv.org/abs/1506.01326v1,2015-06-03 17:45:01+00:00,2015-06-03 17:45:01+00:00,Probabilistic Numerics and Uncertainty in Computations,"['Philipp Hennig', 'Michael A Osborne', 'Mark Girolami']","We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.","Author Generated Postprint. 17 pages, 4 Figures, 1 Table",,10.1098/rspa.2015.0142,math.NA,"['math.NA', 'cs.AI', 'cs.LG', 'stat.CO', 'stat.ML']","['http://dx.doi.org/10.1098/rspa.2015.0142', 'http://arxiv.org/abs/1506.01326v1', 'http://arxiv.org/pdf/1506.01326v1']",http://arxiv.org/pdf/1506.01326v1
188,http://arxiv.org/abs/2002.06036v1,2020-02-14 13:52:04+00:00,2020-02-14 13:52:04+00:00,A comparison of different types of Niching Genetic Algorithms for variable selection in solar radiation estimation,"['Jorge Bustos', 'Victor A. Jimenez', 'Adrian Will']","Variable selection problems generally present more than a single solution and, sometimes, it is worth to find as many solutions as possible. The use of Evolutionary Algorithms applied to this kind of problem proves to be one of the best methods to find optimal solutions. Moreover, there are variants designed to find all or almost all local optima, known as Niching Genetic Algorithms (NGA). There are several different NGA methods developed in order to achieve this task. The present work compares the behavior of eight different niching techniques, applied to a climatic database of four weather stations distributed in Tucuman, Argentina. The goal is to find different sets of input variables that have been used as the input variable by the estimation method. Final results were evaluated based on low estimation error and low dispersion error, as well as a high number of different results and low computational time. A second experiment was carried out to study the capability of the method to identify critical variables. The best results were obtained with Deterministic Crowding. In contrast, Steady State Worst Among Most Similar and Probabilistic Crowding showed good results but longer processing times and less ability to determine the critical factors.","10 pages, two columns, 9 figures, non-published paper",,,cs.NE,"['cs.NE', 'cs.AI', '68T20 (Primary)', 'G.1.6; G.3; I.2.0']","['http://arxiv.org/abs/2002.06036v1', 'http://arxiv.org/pdf/2002.06036v1']",http://arxiv.org/pdf/2002.06036v1
189,http://arxiv.org/abs/2011.11188v1,2020-11-23 03:09:58+00:00,2020-11-23 03:09:58+00:00,Integrating Deep Learning in Domain Sciences at Exascale,"['Rick Archibald', 'Edmond Chow', ""Eduardo D'Azevedo"", 'Jack Dongarra', 'Markus Eisenbach', 'Rocco Febbo', 'Florent Lopez', 'Daniel Nichols', 'Stanimire Tomov', 'Kwai Wong', 'Junqi Yin']","This paper presents some of the current challenges in designing deep learning artificial intelligence (AI) and integrating it with traditional high-performance computing (HPC) simulations. We evaluate existing packages for their ability to run deep learning models and applications on large-scale HPC systems efficiently, identify challenges, and propose new asynchronous parallelization and optimization techniques for current large-scale heterogeneous systems and upcoming exascale systems. These developments, along with existing HPC AI software capabilities, have been integrated into MagmaDNN, an open-source HPC deep learning framework. Many deep learning frameworks are targeted at data scientists and fall short in providing quality integration into existing HPC workflows. This paper discusses the necessities of an HPC deep learning framework and how those needs can be provided (e.g., as in MagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA and its modular memory management, MPI, CuBLAS, CuDNN, MKL, and HIP. Advancements are also illustrated through the use of algorithmic enhancements in reduced- and mixed-precision, as well as asynchronous optimization methods. Finally, we present illustrations and potential solutions for enhancing traditional compute- and data-intensive applications at ORNL and UTK with AI. The approaches and future challenges are illustrated in materials science, imaging, and climate applications.",,,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2011.11188v1', 'http://arxiv.org/pdf/2011.11188v1']",http://arxiv.org/pdf/2011.11188v1
190,http://arxiv.org/abs/2102.13597v1,2021-02-26 17:08:30+00:00,2021-02-26 17:08:30+00:00,Evolution of collective fairness in complex networks through degree-based role assignment,"['Andreia Sofia Teixeira', 'Francisco C. Santos', 'Alexandre P. Francisco', 'Fernando P. Santos']","From social contracts to climate agreements, individuals engage in groups that must collectively reach decisions with varying levels of equality and fairness. These dilemmas also pervade Distributed Artificial Intelligence, in domains such as automated negotiation, conflict resolution or resource allocation. As evidenced by the well-known Ultimatum Game -- where a Proposer has to divide a resource with a Responder -- payoff-maximizing outcomes are frequently at odds with fairness. Eliciting equality in populations of self-regarding agents requires judicious interventions. Here we use knowledge about agents' social networks to implement fairness mechanisms, in the context of Multiplayer Ultimatum Games. We focus on network-based role assignment and show that preferentially attributing the role of Proposer to low-connected nodes increases the fairness levels in a population. We evaluate the effectiveness of low-degree Proposer assignment considering networks with different average connectivity, group sizes, and group voting rules when accepting proposals (e.g. majority or unanimity). We further show that low-degree Proposer assignment is efficient, not only optimizing fairness, but also the average payoff level in the population. Finally, we show that stricter voting rules (i.e., imposing an accepting consensus as requirement for collectives to accept a proposal) attenuates the unfairness that results from situations where high-degree nodes (hubs) are the natural candidates to play as Proposers. Our results suggest new routes to use role assignment and voting mechanisms to prevent unfair behaviors from spreading on complex networks.",,,,physics.soc-ph,"['physics.soc-ph', 'cs.GT']","['http://arxiv.org/abs/2102.13597v1', 'http://arxiv.org/pdf/2102.13597v1']",http://arxiv.org/pdf/2102.13597v1
191,http://arxiv.org/abs/2104.07237v2,2021-05-05 15:36:44+00:00,2021-04-15 05:07:11+00:00,Skilled and Mobile: Survey Evidence of AI Researchers' Immigration Preferences,"['Remco Zwetsloot', 'Baobao Zhang', 'Noemi Dreksler', 'Lauren Kahn', 'Markus Anderljung', 'Allan Dafoe', 'Michael C. Horowitz']","Countries, companies, and universities are increasingly competing over top-tier artificial intelligence (AI) researchers. Where are these researchers likely to immigrate and what affects their immigration decisions? We conducted a survey $(n = 524)$ of the immigration preferences and motivations of researchers that had papers accepted at one of two prestigious AI conferences: the Conference on Neural Information Processing Systems (NeurIPS) and the International Conference on Machine Learning (ICML). We find that the U.S. is the most popular destination for AI researchers, followed by the U.K., Canada, Switzerland, and France. A country's professional opportunities stood out as the most common factor that influences immigration decisions of AI researchers, followed by lifestyle and culture, the political climate, and personal relations. The destination country's immigration policies were important to just under half of the researchers surveyed, while around a quarter noted current immigration difficulties to be a deciding factor. Visa and immigration difficulties were perceived to be a particular impediment to conducting AI research in the U.S., the U.K., and Canada. Implications of the findings for the future of AI talent policies and governance are discussed.","Accepted for poster presentation at the 2021 AAAI/ACM Conference on
  AI, Ethics, and Society",,10.1145/3461702.3462617,cs.CY,"['cs.CY', 'K.7.4']","['http://dx.doi.org/10.1145/3461702.3462617', 'http://arxiv.org/abs/2104.07237v2', 'http://arxiv.org/pdf/2104.07237v2']",http://arxiv.org/pdf/2104.07237v2
192,http://arxiv.org/abs/2105.03804v1,2021-05-09 00:34:37+00:00,2021-05-09 00:34:37+00:00,Slash or burn: Power line and vegetation classification for wildfire prevention,"['Austin Park', 'Farzaneh Rajabi', 'Ross Weber']","Electric utilities are struggling to manage increasing wildfire risk in a hotter and drier climate. Utility transmission and distribution lines regularly ignite destructive fires when they make contact with surrounding vegetation. Trimming vegetation to maintain the separation from utility assets is as critical to safety as it is difficult. Each utility has tens of thousands of linear miles to manage, poor knowledge of where those assets are located, and no way to prioritize trimming. Feature-enhanced convolutional neural networks (CNNs) have proven effective in this problem space. Histograms of oriented gradients (HOG) and Hough transforms are used to increase the salience of the linear structures like power lines and poles. Data is frequently taken from drone or satellite footage, but Google Street View offers an even more scalable and lower cost solution. This paper uses $1,320$ images scraped from Street View, transfer learning on popular CNNs, and feature engineering to place images in one of three classes: (1) no utility systems, (2) utility systems with no overgrown vegetation, or (3) utility systems with overgrown vegetation. The CNN output thus yields a prioritized vegetation management system and creates a geotagged map of utility assets as a byproduct. Test set accuracy with reached $80.15\%$ using VGG11 with a trained first layer and classifier, and a model ensemble correctly classified $88.88\%$ of images with risky vegetation overgrowth.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2105.03804v1', 'http://arxiv.org/pdf/2105.03804v1']",http://arxiv.org/pdf/2105.03804v1
193,http://arxiv.org/abs/2107.10478v1,2021-07-22 06:42:54+00:00,2021-07-22 06:42:54+00:00,Inter and Intra-Annual Spatio-Temporal Variability of Habitat Suitability for Asian Elephants in India: A Random Forest Model-based Analysis,"['P. Anjali', 'Deepak N. Subramani']","We develop a Random Forest model to estimate the species distribution of Asian elephants in India and study the inter and intra-annual spatiotemporal variability of habitats suitable for them. Climatic, topographic variables and satellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP), Leaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are used as predictors, and the species sighting data of Asian elephants from Global Biodiversity Information Reserve is used to develop the Random Forest model. A careful hyper-parameter tuning and training-validation-testing cycle are completed to identify the significant predictors and develop a final model that gives precision and recall of 0.78 and 0.77. The model is applied to estimate the spatial and temporal variability of suitable habitats. We observe that seasonal reduction in the suitable habitat may explain the migration patterns of Asian elephants and the increasing human-elephant conflict. Further, the total available suitable habitat area is observed to have reduced, which exacerbates the problem. This machine learning model is intended to serve as an input to the Agent-Based Model that we are building as part of our Artificial Intelligence-driven decision support tool to reduce human-wildlife conflict.","Submitted for possible publication in the IEEE International India
  Geoscience and Remote Sensing Symposium 2021 (InGARSS 2021)",,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2107.10478v1', 'http://arxiv.org/pdf/2107.10478v1']",http://arxiv.org/pdf/2107.10478v1
194,http://arxiv.org/abs/2111.11537v1,2021-11-22 21:11:42+00:00,2021-11-22 21:11:42+00:00,Machine Learning for Mars Exploration,['Ali Momennasab'],"Risk to human astronauts and interplanetary distance causing slow and limited communication drives scientists to pursue an autonomous approach to exploring distant planets, such as Mars. A portion of exploration of Mars has been conducted through the autonomous collection and analysis of Martian data by spacecraft such as the Mars rovers and the Mars Express Orbiter. The autonomy used on these Mars exploration spacecraft and on Earth to analyze data collected by these vehicles mainly consist of machine learning, a field of artificial intelligence where algorithms collect data and self-improve with the data. Additional applications of machine learning techniques for Mars exploration have potential to resolve communication limitations and human risks of interplanetary exploration. In addition, analyzing Mars data with machine learning has the potential to provide a greater understanding of Mars in numerous domains such as its climate, atmosphere, and potential future habitation. To explore further utilizations of machine learning techniques for Mars exploration, this paper will first summarize the general features and phenomena of Mars to provide a general overview of the planet, elaborate upon uncertainties of Mars that would be beneficial to explore and understand, summarize every current or previous usage of machine learning techniques in the exploration of Mars, explore implementations of machine learning that will be utilized in future Mars exploration missions, and explore machine learning techniques used in Earthly domains to provide solutions to the previously described uncertainties of Mars.","16 pages, 0 figures",,,astro-ph.EP,"['astro-ph.EP', 'astro-ph.IM', 'cs.LG']","['http://arxiv.org/abs/2111.11537v1', 'http://arxiv.org/pdf/2111.11537v1']",http://arxiv.org/pdf/2111.11537v1
195,http://arxiv.org/abs/2112.09490v3,2022-03-24 15:28:39+00:00,2021-12-17 13:00:37+00:00,Visual Microfossil Identification via Deep Metric Learning,"['Tayfun Karaderi', 'Tilo Burghardt', 'Allison Y. Hsiang', 'Jacob Ramaer', 'Daniela N. Schmidt']","We apply deep metric learning for the first time to the problem of classifying planktic foraminifer shells on microscopic images. This species recognition task is an important information source and scientific pillar for reconstructing past climates. All foraminifer CNN recognition pipelines in the literature produce black-box classifiers that lack visualization options for human experts and cannot be applied to open-set problems. Here, we benchmark metric learning against these pipelines, produce the first scientific visualization of the phenotypic planktic foraminifer morphology space, and demonstrate that metric learning can be used to cluster species unseen during training. We show that metric learning outperforms all published CNN-based state-of-the-art benchmarks in this domain. We evaluate our approach on the 34,640 expert-annotated images of the Endless Forams public library of 35 modern planktic foraminifera species. Our results on this data show leading 92% accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data, and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered in training. We conclude that metric learning is highly effective for this domain and serves as an important tool towards expert-in-the-loop automation of microfossil identification. Keycode, network weights, and data splits are published with this paper for full reproducibility.",,,,cs.CV,"['cs.CV', 'cs.AI', 'cs.LG']","['http://arxiv.org/abs/2112.09490v3', 'http://arxiv.org/pdf/2112.09490v3']",http://arxiv.org/pdf/2112.09490v3
196,http://arxiv.org/abs/2201.01032v1,2022-01-04 08:22:03+00:00,2022-01-04 08:22:03+00:00,Learning Operators with Coupled Attention,"['Georgios Kissas', 'Jacob Seidman', 'Leonardo Ferreira Guilhoto', 'Victor M. Preciado', 'George J. Pappas', 'Paris Perdikaris']","Supervised operator learning is an emerging machine learning paradigm with applications to modeling the evolution of spatio-temporal dynamical systems and approximating general black-box relationships between functional data. We propose a novel operator learning method, LOCA (Learning Operators with Coupled Attention), motivated from the recent success of the attention mechanism. In our architecture, the input functions are mapped to a finite set of features which are then averaged with attention weights that depend on the output query locations. By coupling these attention weights together with an integral transform, LOCA is able to explicitly learn correlations in the target output functions, enabling us to approximate nonlinear operators even when the number of output function in the training set measurements is very small. Our formulation is accompanied by rigorous approximation theoretic guarantees on the universal expressiveness of the proposed model. Empirically, we evaluate the performance of LOCA on several operator learning scenarios involving systems governed by ordinary and partial differential equations, as well as a black-box climate prediction problem. Through these scenarios we demonstrate state of the art accuracy, robustness with respect to noisy input data, and a consistently small spread of errors over testing data sets, even for out-of-distribution prediction tasks.",,,,cs.LG,"['cs.LG', 'cs.AI', 'physics.comp-ph']","['http://arxiv.org/abs/2201.01032v1', 'http://arxiv.org/pdf/2201.01032v1']",http://arxiv.org/pdf/2201.01032v1
197,http://arxiv.org/abs/2202.08956v2,2022-02-21 19:59:56+00:00,2022-02-18 01:51:09+00:00,GNN-Surrogate: A Hierarchical and Adaptive Graph Neural Network for Parameter Space Exploration of Unstructured-Mesh Ocean Simulations,"['Neng Shi', 'Jiayi Xu', 'Skylar W. Wurster', 'Hanqi Guo', 'Jonathan Woodring', 'Luke P. Van Roekel', 'Han-Wei Shen']","We propose GNN-Surrogate, a graph neural network-based surrogate model to explore the parameter space of ocean climate simulations. Parameter space exploration is important for domain scientists to understand the influence of input parameters (e.g., wind stress) on the simulation output (e.g., temperature). The exploration requires scientists to exhaust the complicated parameter space by running a batch of computationally expensive simulations. Our approach improves the efficiency of parameter space exploration with a surrogate model that predicts the simulation outputs accurately and efficiently. Specifically, GNN-Surrogate predicts the output field with given simulation parameters so scientists can explore the simulation parameter space with visualizations from user-specified visual mappings. Moreover, our graph-based techniques are designed for unstructured meshes, making the exploration of simulation outputs on irregular grids efficient. For efficient training, we generate hierarchical graphs and use adaptive resolutions. We give quantitative and qualitative evaluations on the MPAS-Ocean simulation to demonstrate the effectiveness and efficiency of GNN-Surrogate. Source code is publicly available at https://github.com/trainsn/GNN-Surrogate.","Accepted by TVCG Special Issue on the 2022 IEEE Pacific Visualization
  Symposium (PacificVis)",,,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'cs.GR', 'cs.LG']","['http://arxiv.org/abs/2202.08956v2', 'http://arxiv.org/pdf/2202.08956v2']",http://arxiv.org/pdf/2202.08956v2
198,http://arxiv.org/abs/2204.08524v2,2022-11-10 07:25:37+00:00,2022-04-07 07:30:43+00:00,So2Sat POP -- A Curated Benchmark Data Set for Population Estimation from Space on a Continental Scale,"['Sugandha Doda', 'Yuanyuan Wang', 'Matthias Kahl', 'Eike Jens Hoffmann', 'Kim Ouan', 'Hannes Taubenböck', 'Xiao Xiang Zhu']","Obtaining a dynamic population distribution is key to many decision-making processes such as urban planning, disaster management and most importantly helping the government to better allocate socio-technical supply. For the aspiration of these objectives, good population data is essential. The traditional method of collecting population data through the census is expensive and tedious. In recent years, statistical and machine learning methods have been developed to estimate population distribution. Most of the methods use data sets that are either developed on a small scale or not publicly available yet. Thus, the development and evaluation of new methods become challenging. We fill this gap by providing a comprehensive data set for population estimation in 98 European cities. The data set comprises a digital elevation model, local climate zone, land use proportions, nighttime lights in combination with multi-spectral Sentinel-2 imagery, and data from the Open Street Map initiative. We anticipate that it would be a valuable addition to the research community for the development of sophisticated approaches in the field of population estimation.",,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']","['http://arxiv.org/abs/2204.08524v2', 'http://arxiv.org/pdf/2204.08524v2']",http://arxiv.org/pdf/2204.08524v2
199,http://arxiv.org/abs/2207.05753v2,2022-08-12 08:36:39+00:00,2022-07-12 08:16:44+00:00,Forecasting COVID-19 spreading trough an ensemble of classical and machine learning models: Spain's case study,"['Ignacio Heredia Cacha', 'Judith Sainz-Pardo Díaz', 'María Castrillo Melguizo', 'Álvaro López García']","In this work we evaluate the applicability of an ensemble of population models and machine learning models to predict the near future evolution of the COVID-19 pandemic, with a particular use case in Spain. We rely solely in open and public datasets, fusing incidence, vaccination, human mobility and weather data to feed our machine learning models (Random Forest, Gradient Boosting, k-Nearest Neighbours and Kernel Ridge Regression). We use the incidence data to adjust classic population models (Gompertz, Logistic, Richards, Bertalanffy) in order to be able to better capture the trend of the data. We then ensemble these two families of models in order to obtain a more robust and accurate prediction. Furthermore, we have observed an improvement in the predictions obtained with machine learning models as we add new features (vaccines, mobility, climatic conditions), analyzing the importance of each of them using Shapley Additive Explanation values. As in any other modelling work, data and predictions quality have several limitations and therefore they must be seen from a critical standpoint, as we discuss in the text. Our work concludes that the ensemble use of these models improves the individual predictions (using only machine learning models or only population models) and can be applied, with caution, in cases when compartmental models cannot be utilized due to the lack of relevant data.",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2207.05753v2', 'http://arxiv.org/pdf/2207.05753v2']",http://arxiv.org/pdf/2207.05753v2
200,http://arxiv.org/abs/2207.05833v1,2022-07-12 20:52:26+00:00,2022-07-12 20:52:26+00:00,Earthformer: Exploring Space-Time Transformers for Earth System Forecasting,"['Zhihan Gao', 'Xingjian Shi', 'Hao Wang', 'Yi Zhu', 'Yuyang Wang', 'Mu Li', 'Dit-Yan Yeung']","Conventionally, Earth system (e.g., weather and climate) forecasting relies on numerical simulation with complex physical models and are hence both expensive in computation and demanding on domain expertise. With the explosive growth of the spatiotemporal Earth observation data in the past decade, data-driven models that apply Deep Learning (DL) are demonstrating impressive potential for various Earth system forecasting tasks. The Transformer as an emerging DL architecture, despite its broad success in other domains, has limited adoption in this area. In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. Earthformer is based on a generic, flexible and efficient space-time attention block, named Cuboid Attention. The idea is to decompose the data into cuboids and apply cuboid-level self-attention in parallel. These cuboids are further connected with a collection of global vectors. We conduct experiments on the MovingMNIST dataset and a newly proposed chaotic N-body MNIST dataset to verify the effectiveness of cuboid attention and figure out the best design of Earthformer. Experiments on two real-world benchmarks about precipitation nowcasting and El Nino/Southern Oscillation (ENSO) forecasting show Earthformer achieves state-of-the-art performance.",Technical report,,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","['http://arxiv.org/abs/2207.05833v1', 'http://arxiv.org/pdf/2207.05833v1']",http://arxiv.org/pdf/2207.05833v1
201,http://arxiv.org/abs/2208.09473v1,2022-08-19 17:54:24+00:00,2022-08-19 17:54:24+00:00,Carefully choose the baseline: Lessons learned from applying XAI attribution methods for regression tasks in geoscience,"['Antonios Mamalakis', 'Elizabeth A. Barnes', 'Imme Ebert-Uphoff']","Methods of eXplainable Artificial Intelligence (XAI) are used in geoscientific applications to gain insights into the decision-making strategy of Neural Networks (NNs) highlighting which features in the input contribute the most to a NN prediction. Here, we discuss our lesson learned that the task of attributing a prediction to the input does not have a single solution. Instead, the attribution results and their interpretation depend greatly on the considered baseline (sometimes referred to as reference point) that the XAI method utilizes; a fact that has been overlooked so far in the literature. This baseline can be chosen by the user or it is set by construction in the method s algorithm, often without the user being aware of that choice. We highlight that different baselines can lead to different insights for different science questions and, thus, should be chosen accordingly. To illustrate the impact of the baseline, we use a large ensemble of historical and future climate simulations forced with the SSP3-7.0 scenario and train a fully connected NN to predict the ensemble- and global-mean temperature (i.e., the forced global warming signal) given an annual temperature map from an individual ensemble member. We then use various XAI methods and different baselines to attribute the network predictions to the input. We show that attributions differ substantially when considering different baselines, as they correspond to answering different science questions. We conclude by discussing some important implications and considerations about the use of baselines in XAI research.",,,,physics.geo-ph,"['physics.geo-ph', 'cs.LG']","['http://arxiv.org/abs/2208.09473v1', 'http://arxiv.org/pdf/2208.09473v1']",http://arxiv.org/pdf/2208.09473v1
202,http://arxiv.org/abs/2211.02108v2,2022-12-05 18:21:08+00:00,2022-11-03 19:25:28+00:00,"Sky-image-based solar forecasting using deep learning with multi-location data: training models locally, globally or via transfer learning?","['Yuhao Nie', 'Quentin Paletta', 'Andea Scott', 'Luis Martin Pomares', 'Guillaume Arbod', 'Sgouris Sgouridis', 'Joan Lasenby', 'Adam Brandt']","Solar forecasting from ground-based sky images has shown great promise in reducing the uncertainty in solar power generation. With more and more sky image datasets open sourced in recent years, the development of accurate and reliable deep learning-based solar forecasting methods has seen a huge growth in potential. In this study, we explore three different training strategies for solar forecasting models by leveraging three heterogeneous datasets collected globally with different climate patterns. Specifically, we compare the performance of local models trained individually based on single datasets and global models trained jointly based on the fusion of multiple datasets, and further examine the knowledge transfer from pre-trained solar forecasting models to a new dataset of interest. The results suggest that the local models work well when deployed locally, but significant errors are observed when applied offsite. The global model can adapt well to individual locations at the cost of a potential increase in training efforts. Pre-training models on a large and diversified source dataset and transferring to a target dataset generally achieves superior performance over the other two strategies. With 80% less training data, it can achieve comparable performance as the local baseline trained using the entire dataset.",,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2211.02108v2', 'http://arxiv.org/pdf/2211.02108v2']",http://arxiv.org/pdf/2211.02108v2
203,http://arxiv.org/abs/1801.00052v2,2018-01-07 00:01:57+00:00,2017-12-29 22:42:18+00:00,The Astrobiology of the Anthropocene,"['Jacob Haqq-Misra', 'Sanjoy Som', 'Brendan Mullan', 'Rafael Loureiro', 'Edward Schwieterman', 'Lauren Seyler', 'Haritina Mogosanu', 'Adam Frank', 'Eric Wolf', 'Duncan Forgan', 'Charles Cockell', 'Woodruff Sullivan']","Human influence on the biosphere has been evident at least since the development of widespread agriculture, and some stratigraphers have suggested that the activities of modern civilization indicate a geological epoch transition. The study of the anthropocene as a geological epoch, and its implication for the future of energy-intensive civilizations, is an emerging transdisciplinary field in which astrobiology can play a leading role. Habitability research of Earth, Mars, and exoplanets examines extreme cases relevant for understanding climate change as a planetary process. Energy-intensive civilizations will also face thermodynamic limits to growth, which provides an important constraint for estimating the longevity of human civilization and guiding the search for extraterrestrial intelligence. We recommend that missions concepts such as LUVOIR, HabEx, and OST be pursued in order to make significant progress toward understanding the future evolution of life on our planet and the possible evolution of technological, energy-intensive life elsewhere in the universe.","A white paper on ""Astrobiology Science Strategy"" submitted to the NAS",,,astro-ph.EP,"['astro-ph.EP', 'astro-ph.IM', 'physics.pop-ph', 'physics.soc-ph']","['http://arxiv.org/abs/1801.00052v2', 'http://arxiv.org/pdf/1801.00052v2']",http://arxiv.org/pdf/1801.00052v2
204,http://arxiv.org/abs/1504.04327v1,2015-04-16 18:25:11+00:00,2015-04-16 18:25:11+00:00,Control and Communication Protocols that Enable Smart Building Microgrids,"['Bowen Zhang', 'John Baillieul']","Recent communication, computation, and technology advances coupled with climate change concerns have transformed the near future prospects of electricity transmission, and, more notably, distribution systems and microgrids. Distributed resources (wind and solar generation, combined heat and power) and flexible loads (storage, computing, EV, HVAC) make it imperative to increase investment and improve operational efficiency. Commercial and residential buildings, being the largest energy consumption group among flexible loads in microgrids, have the largest potential and flexibility to provide demand side management. Recent advances in networked systems and the anticipated breakthroughs of the Internet of Things will enable significant advances in demand response capabilities of intelligent load network of power-consuming devices such as HVAC components, water heaters, and buildings. In this paper, a new operating framework, called packetized direct load control (PDLC), is proposed based on the notion of quantization of energy demand. This control protocol is built on top of two communication protocols that carry either complete or binary information regarding the operation status of the appliances. We discuss the optimal demand side operation for both protocols and analytically derive the performance differences between the protocols. We propose an optimal reservation strategy for traditional and renewable energy for the PDLC in both day-ahead and real time markets. In the end we discuss the fundamental trade-off between achieving controllability and endowing flexibility.",,,,cs.SY,['cs.SY'],"['http://arxiv.org/abs/1504.04327v1', 'http://arxiv.org/pdf/1504.04327v1']",http://arxiv.org/pdf/1504.04327v1
205,http://arxiv.org/abs/1901.05599v1,2019-01-17 03:11:58+00:00,2019-01-17 03:11:58+00:00,Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails,"['Michael L. Iuzzolino', 'Michael E. Walker', 'Daniel Szafir']","Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.",iROS 2018,"2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS) (pp. 576-582)",10.1109/IROS.2018.8593883,cs.LG,"['cs.LG', 'cs.CV', 'cs.RO', 'stat.ML']","['http://dx.doi.org/10.1109/IROS.2018.8593883', 'http://arxiv.org/abs/1901.05599v1', 'http://arxiv.org/pdf/1901.05599v1']",http://arxiv.org/pdf/1901.05599v1
206,http://arxiv.org/abs/2006.05372v1,2020-06-09 16:04:36+00:00,2020-06-09 16:04:36+00:00,Sustainability of ICT hardware procurement in Switzerland -- A status-quo analysis of the public procurement sector,"['Tobias Welz', 'Matthias Stuermer']","Sustainable procurement requires organizations to align their purchasing behavior with regard to broader goals linked to resource efficiency, climate change, social responsibility and other sustainability criteria. The level of sustainability in Information and Communication Technology (ICT) hardware procurement is analyzed for two reasons: First, ICT hardware belongs to the six key product groups in sustainable procurement. Second, ICT in general is expected to be an important enabler for low-carbon economies, providing solutions to reduce Green-House Gas (GHG) emissions. While the advantages of sustainable procurement are obvious, certain barriers hinder the adoption in day-to-day procurement. This case study on ICT hardware discusses the three important barriers ""lack of clear definitions per product group"", ""missing market intelligence about sustainable products"" and ""inflexible procedures and attitudes as barriers for innovative approaches"" based on an in-depth analysis of sustainable procurement of ICT hardware by the public sector in Switzerland. To this end, tender data published on the national procurement platform simap.ch is screened for sustainability criteria using the Common Procurement Vocabulary (CPV) nomenclature to identify relevant ICT projects. The results reveal to which extent such criteria as well as their determinants are currently included in public tenders. Using two different approaches, only a small number of tenders were found containing sustainability criteria of a wide range from basic to comprehensive. The overall performance of Swiss public procurement is benchmarked by comparing the identified sustainability criteria with available criteria from international key actors in sustainable procurement. Thus, this analysis provides novel insights on how public agencies today take sustainability into account when procuring ICT hardware.","11 pages, 9 Figures. Accepted to be presented at the ICT4S 2020
  Conference",,10.1145/3401335.3401352,cs.CY,['cs.CY'],"['http://dx.doi.org/10.1145/3401335.3401352', 'http://arxiv.org/abs/2006.05372v1', 'http://arxiv.org/pdf/2006.05372v1']",http://arxiv.org/pdf/2006.05372v1
207,http://arxiv.org/abs/2006.08846v1,2020-06-16 00:45:08+00:00,2020-06-16 00:45:08+00:00,Mining Personalized Climate Preferences for Assistant Driving,['Feng Hu'],"Both assistant driving and self-driving have attracted a great amount of attention in the last few years. However, the majority of research efforts focus on safe driving; few research has been conducted on in-vehicle climate control, or assistant driving based on travellers' personal habits or preferences. In this paper, we propose a novel approach for climate control, driver behavior recognition and driving recommendation for better fitting drivers' preferences in their daily driving. The algorithm consists three components: (1) A in-vehicle sensing and context feature enriching compnent with a Internet of Things (IoT) platform for collecting related environment, vehicle-running, and traffic parameters that affect drivers' behaviors. (2) A non-intrusive intelligent driver behaviour and vehicle status detection component, which can automatically label vehicle's status (open windows, turn on air condition, etc.), based on results of applying further feature extraction and machine learning algorithms. (3) A personalized driver habits learning and preference recommendation component for more healthy and comfortable experiences. A prototype using a client-server architecture with an iOS app and an air-quality monitoring sensor has been developed for collecting heterogeneous data and testing our algorithms. Real-world experiments on driving data of 11,370 km (320 hours) by different drivers in multiple cities worldwide have been conducted, which demonstrate the effective and accuracy of our approach.",,,,cs.LG,"['cs.LG', 'stat.ML']","['http://arxiv.org/abs/2006.08846v1', 'http://arxiv.org/pdf/2006.08846v1']",http://arxiv.org/pdf/2006.08846v1
208,http://arxiv.org/abs/2102.13477v2,2021-03-01 01:30:15+00:00,2021-02-18 21:52:56+00:00,B-ETS: A Trusted Blockchain-based Emissions Trading System for Vehicle-to-Vehicle Networks,"['Lam Duc Nguyen', 'Amari N. Lewis', 'Israel Leyva-Mayorga', 'Amelia Regan', 'Petar Popovski']","Urban areas are negatively impacted by Carbon Dioxide (CO2 ) and Nitrogen Oxide (NOx) emissions. In order to achieve a cost-effective reduction of greenhouse gas emissions and to combat climate change, the European Union (EU) introduced an Emissions Trading System (ETS) where organizations can buy or receive emission allowances as needed. The current ETS is a centralized one, consisting of a set of complex rules. It is currently administered at the organizational level and is used for fixed-point sources of pollution such as factories, power plants, and refineries. However, the current ETS cannot efficiently cope with vehicle mobility, even though vehicles are one of the primary sources of CO2 and NOx emissions. In this study, we propose a new distributed Blockchain-based emissions allowance trading system called B-ETS. This system enables transparent and trustworthy data exchange as well as trading of allowances among vehicles, relying on vehicle-to-vehicle communication. In addition, we introduce an economic incentive-based mechanism that appeals to individual drivers and leads them to modify their driving behavior in order to reduce emissions. The efficiency of the proposed system is studied through extensive simulations, showing how increased vehicle connectivity can lead to a reduction of the emissions generated from those vehicles. We demonstrate that our method can be used for full life-cycle monitoring and fuel economy reporting. This leads us to conjecture that the proposed system could lead to important behavioral changes among the drivers","Paper got accepted in 7th International Conference on Vehicle
  Technology and Intelligent Transport Systems (VEHITS) 2021","7th International Conference on Vehicle Technology and Intelligent
  Transport Systems 2021",,cs.MA,"['cs.MA', 'cs.SY', 'eess.SY']","['http://arxiv.org/abs/2102.13477v2', 'http://arxiv.org/pdf/2102.13477v2']",http://arxiv.org/pdf/2102.13477v2
209,http://arxiv.org/abs/1912.01752v2,2020-05-27 05:43:18+00:00,2019-12-04 00:37:17+00:00,Physically Interpretable Neural Networks for the Geosciences: Applications to Earth System Variability,"['Benjamin A. Toms', 'Elizabeth A. Barnes', 'Imme Ebert-Uphoff']","Neural networks have become increasingly prevalent within the geosciences, although a common limitation of their usage has been a lack of methods to interpret what the networks learn and how they make decisions. As such, neural networks have often been used within the geosciences to most accurately identify a desired output given a set of inputs, with the interpretation of what the network learns used as a secondary metric to ensure the network is making the right decision for the right reason. Neural network interpretation techniques have become more advanced in recent years, however, and we therefore propose that the ultimate objective of using a neural network can also be the interpretation of what the network has learned rather than the output itself.   We show that the interpretation of neural networks can enable the discovery of scientifically meaningful connections within geoscientific data. In particular, we use two methods for neural network interpretation called backwards optimization and layerwise relevance propagation, both of which project the decision pathways of a network back onto the original input dimensions. To the best of our knowledge, LRP has not yet been applied to geoscientific research, and we believe it has great potential in this area. We show how these interpretation techniques can be used to reliably infer scientifically meaningful information from neural networks by applying them to common climate patterns. These results suggest that combining interpretable neural networks with novel scientific hypotheses will open the door to many new avenues in neural network-related geoscience research.","The second version of this manuscript is currently under review at
  the Journal of Advances in Modeling Earth Systems (JAMES)",,10.1029/2019MS002002,physics.ao-ph,"['physics.ao-ph', 'cs.AI', 'physics.data-an', 'stat.ML']","['http://dx.doi.org/10.1029/2019MS002002', 'http://arxiv.org/abs/1912.01752v2', 'http://arxiv.org/pdf/1912.01752v2']",http://arxiv.org/pdf/1912.01752v2
210,http://arxiv.org/abs/2112.12589v2,2021-12-24 18:17:08+00:00,2021-12-20 13:46:39+00:00,A deep reinforcement learning model for predictive maintenance planning of road assets: Integrating LCA and LCCA,"['Moein Latifi', 'Fateme Golivand Darvishvand', 'Omid Khandel']","Road maintenance planning is an integral part of road asset management. One of the main challenges in Maintenance and Rehabilitation (M&R) practices is to determine maintenance type and timing. This research proposes a framework using Reinforcement Learning (RL) based on the Long Term Pavement Performance (LTPP) database to determine the type and timing of M&R practices. A predictive DNN model is first developed in the proposed algorithm, which serves as the Environment for the RL algorithm. For the Policy estimation of the RL model, both DQN and PPO models are developed. However, PPO has been selected in the end due to better convergence and higher sample efficiency. Indicators used in this study are International Roughness Index (IRI) and Rutting Depth (RD). Initially, we considered Cracking Metric (CM) as the third indicator, but it was then excluded due to the much fewer data compared to other indicators, which resulted in lower accuracy of the results. Furthermore, in cost-effectiveness calculation (reward), we considered both the economic and environmental impacts of M&R treatments. Costs and environmental impacts have been evaluated with paLATE 2.0 software. Our method is tested on a hypothetical case study of a six-lane highway with 23 kilometers length located in Texas, which has a warm and wet climate. The results propose a 20-year M&R plan in which road condition remains in an excellent condition range. Because the early state of the road is at a good level of service, there is no need for heavy maintenance practices in the first years. Later, after heavy M&R actions, there are several 1-2 years of no need for treatments. All of these show that the proposed plan has a logical result. Decision-makers and transportation agencies can use this scheme to conduct better maintenance practices that can prevent budget waste and, at the same time, minimize the environmental impacts.",,,,cs.LG,"['cs.LG', 'cs.AI']","['http://arxiv.org/abs/2112.12589v2', 'http://arxiv.org/pdf/2112.12589v2']",http://arxiv.org/pdf/2112.12589v2
211,http://arxiv.org/abs/2207.06673v1,2022-07-14 05:59:54+00:00,2022-07-14 05:59:54+00:00,Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on UAV Remote-Sensing Imagery,"['Pappu Kumar Yadav', 'J. Alex Thomasson', 'Robert Hardin', 'Stephen W. Searcy', 'Ulisses Braga-Neto', 'Sorin C. Popescu', 'Daniel E. Martin', 'Roberto Rodriguez', 'Karem Meza', 'Juan Enciso', 'Jorge Solorzano Diaz', 'Tianyi Wang']","The cotton boll weevil, Anthonomus grandis Boheman is a serious pest to the U.S. cotton industry that has cost more than 16 billion USD in damages since it entered the United States from Mexico in the late 1800s. This pest has been nearly eradicated; however, southern part of Texas still faces this issue and is always prone to the pest reinfestation each year due to its sub-tropical climate where cotton plants can grow year-round. Volunteer cotton (VC) plants growing in the fields of inter-seasonal crops, like corn, can serve as hosts to these pests once they reach pin-head square stage (5-6 leaf stage) and therefore need to be detected, located, and destroyed or sprayed . In this paper, we present a study to detect VC plants in a corn field using YOLOv3 on three band aerial images collected by unmanned aircraft system (UAS). The two-fold objectives of this paper were : (i) to determine whether YOLOv3 can be used for VC detection in a corn field using RGB (red, green, and blue) aerial images collected by UAS and (ii) to investigate the behavior of YOLOv3 on images at three different scales (320 x 320, S1; 416 x 416, S2; and 512 x 512, S3 pixels) based on average precision (AP), mean average precision (mAP) and F1-score at 95% confidence level. No significant differences existed for mAP among the three scales, while a significant difference was found for AP between S1 and S3 (p = 0.04) and S2 and S3 (p = 0.02). A significant difference was also found for F1-score between S2 and S3 (p = 0.02). The lack of significant differences of mAP at all the three scales indicated that the trained YOLOv3 model can be used on a computer vision-based remotely piloted aerial application system (RPAAS) for VC detection and spray application in near real-time.",38 Pages,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2207.06673v1', 'http://arxiv.org/pdf/2207.06673v1']",http://arxiv.org/pdf/2207.06673v1
212,http://arxiv.org/abs/2208.00519v1,2022-07-31 21:03:40+00:00,2022-07-31 21:03:40+00:00,Assessing The Performance of YOLOv5 Algorithm for Detecting Volunteer Cotton Plants in Corn Fields at Three Different Growth Stages,"['Pappu Kumar Yadav', 'J. Alex Thomasson', 'Stephen W. Searcy', 'Robert G. Hardin', 'Ulisses Braga-Neto', 'Sorin C. Popescu', 'Daniel E. Martin', 'Roberto Rodriguez', 'Karem Meza', 'Juan Enciso', 'Jorge Solorzano Diaz', 'Tianyi Wang']","The boll weevil (Anthonomus grandis L.) is a serious pest that primarily feeds on cotton plants. In places like Lower Rio Grande Valley of Texas, due to sub-tropical climatic conditions, cotton plants can grow year-round and therefore the left-over seeds from the previous season during harvest can continue to grow in the middle of rotation crops like corn (Zea mays L.) and sorghum (Sorghum bicolor L.). These feral or volunteer cotton (VC) plants when reach the pinhead squaring phase (5-6 leaf stage) can act as hosts for the boll weevil pest. The Texas Boll Weevil Eradication Program (TBWEP) employs people to locate and eliminate VC plants growing by the side of roads or fields with rotation crops but the ones growing in the middle of fields remain undetected. In this paper, we demonstrate the application of computer vision (CV) algorithm based on You Only Look Once version 5 (YOLOv5) for detecting VC plants growing in the middle of corn fields at three different growth stages (V3, V6, and VT) using unmanned aircraft systems (UAS) remote sensing imagery. All the four variants of YOLOv5 (s, m, l, and x) were used and their performances were compared based on classification accuracy, mean average precision (mAP), and F1-score. It was found that YOLOv5s could detect VC plants with a maximum classification accuracy of 98% and mAP of 96.3 % at the V6 stage of corn while YOLOv5s and YOLOv5m resulted in the lowest classification accuracy of 85% and YOLOv5m and YOLOv5l had the least mAP of 86.5% at the VT stage on images of size 416 x 416 pixels. The developed CV algorithm has the potential to effectively detect and locate VC plants growing in the middle of corn fields as well as expedite the management aspects of TBWEP.",Preprint Under Review,,,cs.CV,"['cs.CV', 'cs.AI']","['http://arxiv.org/abs/2208.00519v1', 'http://arxiv.org/pdf/2208.00519v1']",http://arxiv.org/pdf/2208.00519v1
213,http://arxiv.org/abs/2110.01866v2,2022-01-11 16:23:03+00:00,2021-10-05 08:05:52+00:00,Social physics,"['Marko Jusup', 'Petter Holme', 'Kiyoshi Kanazawa', 'Misako Takayasu', 'Ivan Romic', 'Zhen Wang', 'Suncana Gecek', 'Tomislav Lipic', 'Boris Podobnik', 'Lin Wang', 'Wei Luo', 'Tin Klanjscek', 'Jingfang Fan', 'Stefano Boccaletti', 'Matjaz Perc']","Recent decades have seen a rise in the use of physics methods to study different societal phenomena. This development has been due to physicists venturing outside of their traditional domains of interest, but also due to scientists from other disciplines taking from physics the methods that have proven so successful throughout the 19th and the 20th century. Here we dub this field 'social physics' and pay our respect to intellectual mavericks who nurtured it to maturity. We do so by reviewing the current state of the art. Starting with a set of topics that are at the heart of modern human societies, we review research dedicated to urban development and traffic, the functioning of financial markets, cooperation as the basis for our evolutionary success, the structure of social networks, and the integration of intelligent machines into these networks. We then shift our attention to a set of topics that explore potential threats to society. These include criminal behaviour, large-scale migrations, epidemics, environmental challenges, and climate change. We end the coverage of each topic with promising directions for future research. Based on this, we conclude that the future for social physics is bright. Physicists studying societal phenomena are no longer a curiosity, but rather a force to be reckoned with. Notwithstanding, it remains of the utmost importance that we continue to foster constructive dialogue and mutual respect at the interfaces of different scientific disciplines.","359 pages, 78 figures; published in Physics Reports","Phys. Rep. 948, 1-148 (2022)",10.1016/j.physrep.2021.10.005,physics.soc-ph,"['physics.soc-ph', 'cs.SI', 'nlin.AO', 'q-bio.PE']","['http://dx.doi.org/10.1016/j.physrep.2021.10.005', 'http://arxiv.org/abs/2110.01866v2', 'http://arxiv.org/pdf/2110.01866v2']",http://arxiv.org/pdf/2110.01866v2
214,http://arxiv.org/abs/1605.07130v1,2016-05-23 18:39:33+00:00,2016-05-23 18:39:33+00:00,Limit cycles can reduce the width of the habitable zone,"['Jacob Haqq-Misra', 'Ravi Kumar Kopparapu', 'Natasha E. Batalha', 'Chester E. Harman', 'James F. Kasting']","The liquid water habitable zone (HZ) describes the orbital distance at which a terrestrial planet can maintain above-freezing conditions through regulation by the carbonate-silicate cycle. Recent calculations have suggested that planets in the outer regions of the habitable zone cannot maintain stable, warm climates, but rather should oscillate between long, globally glaciated states and shorter periods of climatic warmth. Such conditions, similar to 'Snowball Earth' episodes experienced on Earth, would be inimical to the development of complex land life, including intelligent life. Here, we build upon previous studies with an updated an energy balance climate model to calculate this 'limit cycle' region of the habitable zone where such cycling would occur. We argue that an abiotic Earth would have a greater CO$_2$ partial pressure than today because plants and other biota help to enhance the storage of CO$_2$ in soil. When we tune our abiotic model accordingly, we find that limit cycles can occur but that previous calculations have overestimated their importance. For G stars like the Sun, limit cycles occur only for planets with CO$_2$ outgassing rates less than that on modern Earth. For K and M star planets, limit cycles should not occur; however, M-star planets may be inhospitable to life for other reasons. Planets orbiting late G-type and early K-type stars retain the greatest potential for maintaining warm, stable conditions. Our results suggest that host star type, planetary volcanic activity, and seafloor weathering are all important factors in determining whether planets will be prone to limit cycling.","Accepted for publication in The Astrophysical Journal. 25 pages, 5
  figures",,10.3847/0004-637X/827/2/120,astro-ph.EP,"['astro-ph.EP', 'astro-ph.IM', 'physics.ao-ph']","['http://dx.doi.org/10.3847/0004-637X/827/2/120', 'http://arxiv.org/abs/1605.07130v1', 'http://arxiv.org/pdf/1605.07130v1']",http://arxiv.org/pdf/1605.07130v1
215,http://arxiv.org/abs/1804.07091v2,2019-07-23 07:23:39+00:00,2018-04-19 11:23:07+00:00,Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly Detection,"['Björn Barz', 'Erik Rodner', 'Yanira Guanche Garcia', 'Joachim Denzler']","Automatic detection of anomalies in space- and time-varying measurements is an important tool in several fields, e.g., fraud detection, climate analysis, or healthcare monitoring. We present an algorithm for detecting anomalous regions in multivariate spatio-temporal time-series, which allows for spotting the interesting parts in large amounts of data, including video and text data. In opposition to existing techniques for detecting isolated anomalous data points, we propose the ""Maximally Divergent Intervals"" (MDI) framework for unsupervised detection of coherent spatial regions and time intervals characterized by a high Kullback-Leibler divergence compared with all other data given. In this regard, we define an unbiased Kullback-Leibler divergence that allows for ranking regions of different size and show how to enable the algorithm to run on large-scale data sets in reasonable time using an interval proposal technique. Experiments on both synthetic and real data from various domains, such as climate analysis, video surveillance, and text forensics, demonstrate that our method is widely applicable and a valuable tool for finding interesting events in different types of data.","Accepted by TPAMI. Examples and code:
  https://cvjena.github.io/libmaxdiv/","IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 41, no. 5, pp. 1088-1101, 1 May 2019",10.1109/TPAMI.2018.2823766,stat.ML,"['stat.ML', 'cs.CV', 'cs.LG', 'stat.AP']","['http://dx.doi.org/10.1109/TPAMI.2018.2823766', 'http://arxiv.org/abs/1804.07091v2', 'http://arxiv.org/pdf/1804.07091v2']",http://arxiv.org/pdf/1804.07091v2
216,http://arxiv.org/abs/1702.04215v1,2017-02-10 09:54:03+00:00,2017-02-10 09:54:03+00:00,Crowdsourcing the Policy Cycle,"['J. Prpic', 'A. Taeihagh', 'J. Melton']","Crowdsourcing is beginning to be used for policymaking. The wisdom of crowds [Surowiecki 2005], and crowdsourcing [Brabham 2008], are seen as new avenues that can shape all kinds of policy, from transportation policy [Nash 2009] to urban planning [Seltzer and Mahmoudi 2013], to climate policy. In general, many have high expectations for positive outcomes with crowdsourcing, and based on both anecdotal and empirical evidence, some of these expectations seem justified [Majchrzak and Malhotra 2013]. Yet, to our knowledge, research has yet to emerge that unpacks the different forms of crowdsourcing in light of each stage of the well-established policy cycle. This work addresses this research gap, and in doing so brings increased nuance to the application of crowdsourcing techniques for policymaking.","Collective Intelligence 2014, MIT Center for Collective Intelligence",,,cs.CY,"['cs.CY', 'cs.HC']","['http://arxiv.org/abs/1702.04215v1', 'http://arxiv.org/pdf/1702.04215v1']",http://arxiv.org/pdf/1702.04215v1
217,http://arxiv.org/abs/2106.12963v1,2021-06-21 20:57:23+00:00,2021-06-21 20:57:23+00:00,Objective discovery of dominant dynamical processes with intelligible machine learning,"['Bryan E. Kaiser', 'Juan A. Saenz', 'Maike Sonnewald', 'Daniel Livescu']","The advent of big data has vast potential for discovery in natural phenomena ranging from climate science to medicine, but overwhelming complexity stymies insight. Existing theory is often not able to succinctly describe salient phenomena, and progress has largely relied on ad hoc definitions of dynamical regimes to guide and focus exploration. We present a formal definition in which the identification of dynamical regimes is formulated as an optimization problem, and we propose an intelligible objective function. Furthermore, we propose an unsupervised learning framework which eliminates the need for a priori knowledge and ad hoc definitions; instead, the user need only choose appropriate clustering and dimensionality reduction algorithms, and this choice can be guided using our proposed objective function. We illustrate its applicability with example problems drawn from ocean dynamics, tumor angiogenesis, and turbulent boundary layers. Our method is a step towards unbiased data exploration that allows serendipitous discovery within dynamical systems, with the potential to propel the physical sciences forward.","21 pages, 7 figures",,,cs.LG,"['cs.LG', 'math-ph', 'math.MP', 'physics.data-an']","['http://arxiv.org/abs/2106.12963v1', 'http://arxiv.org/pdf/2106.12963v1']",http://arxiv.org/pdf/2106.12963v1
218,http://arxiv.org/abs/2109.11712v1,2021-09-24 02:13:23+00:00,2021-09-24 02:13:23+00:00,Feasibility study of urban flood mapping using traffic signs for route optimization,"['Bahareh Alizadeh', 'Diya Li', 'Zhe Zhang', 'Amir H. Behzadan']","Water events are the most frequent and costliest climate disasters around the world. In the U.S., an estimated 127 million people who live in coastal areas are at risk of substantial home damage from hurricanes or flooding. In flood emergency management, timely and effective spatial decision-making and intelligent routing depend on flood depth information at a fine spatiotemporal scale. In this paper, crowdsourcing is utilized to collect photos of submerged stop signs, and pair each photo with a pre-flood photo taken at the same location. Each photo pair is then analyzed using deep neural network and image processing to estimate the depth of floodwater in the location of the photo. Generated point-by-point depth data is converted to a flood inundation map and used by an A* search algorithm to determine an optimal flood-free path connecting points of interest. Results provide crucial information to rescue teams and evacuees by enabling effective wayfinding during flooding events.",URL: https://do.tu-berlin.de/handle/11303/13226,"EG-ICE 2021 Workshop on Intelligent Computing in Engineering
  (2021) 572-581",,cs.CV,"['cs.CV', 'I.2.10, I.5.4, I.4.9']","['http://arxiv.org/abs/2109.11712v1', 'http://arxiv.org/pdf/2109.11712v1']",http://arxiv.org/pdf/2109.11712v1
219,http://arxiv.org/abs/2108.06206v1,2021-08-09 06:51:39+00:00,2021-08-09 06:51:39+00:00,An Intelligent Recommendation-cum-Reminder System,"['Rohan Saxena', 'Maheep Chaudhary', 'Chandresh Kumar Maurya', 'Shitala Prasad']","Intelligent recommendation and reminder systems are the need of the fast-pacing life. Current intelligent systems such as Siri, Google Assistant, Microsoft Cortona, etc., have limited capability. For example, if you want to wake up at 6 am because you have an upcoming trip, you have to set the alarm manually. Besides, these systems do not recommend or remind what else to carry, such as carrying an umbrella during a likely rain. The present work proposes a system that takes an email as input and returns a recommendation-cumreminder list. As a first step, we parse the emails, recognize the entities using named entity recognition (NER). In the second step, information retrieval over the web is done to identify nearby places, climatic conditions, etc. Imperative sentences from the reviews of all places are extracted and passed to the object extraction module. The main challenge lies in extracting the objects (items) of interest from the review. To solve it, a modified Machine Reading Comprehension-NER (MRC-NER) model is trained to tag objects of interest by formulating annotation rules as a query. The objects so found are recommended to the user one day in advance. The final reminder list of objects is pruned by our proposed model for tracking objects kept during the ""packing activity."" Eventually, when the user leaves for the event/trip, an alert is sent containing the reminding list items. Our approach achieves superior performance compared to several baselines by as much as 30% on recall and 10% on precision.",9,,,cs.IR,"['cs.IR', 'cs.LG']","['http://arxiv.org/abs/2108.06206v1', 'http://arxiv.org/pdf/2108.06206v1']",http://arxiv.org/pdf/2108.06206v1
220,http://arxiv.org/abs/1411.7169v1,2014-11-26 10:44:10+00:00,2014-11-26 10:44:10+00:00,A model-free control strategy for an experimental greenhouse with an application to fault accommodation,"['Frédéric Lafont', 'Jean-François Balmat', 'Nathalie Pessel', 'Michel Fliess']","Writing down mathematical models of agricultural greenhouses and regulating them via advanced controllers are challenging tasks since strong perturbations, like meteorological variations, have to be taken into account. This is why we are developing here a new model-free control approach and the corresponding intelligent controllers, where the need of a good model disappears. This setting, which has been introduced quite recently and is easy to implement, is already successful in many engineering domains. Tests on a concrete greenhouse and comparisons with Boolean controllers are reported. They not only demonstrate an excellent climate control, where the reference may be modified in a straightforward way, but also an efficient fault accommodation with respect to the actuators.",,"Computers and Electronics in Agriculture, Elsevier, 2015, 110 (1),
  pp.139-149",,math.OC,['math.OC'],"['http://arxiv.org/abs/1411.7169v1', 'http://arxiv.org/pdf/1411.7169v1']",http://arxiv.org/pdf/1411.7169v1
221,http://arxiv.org/abs/2104.13896v1,2021-04-28 17:26:52+00:00,2021-04-28 17:26:52+00:00,Classification and comparison of license plates localization algorithms,"['Mustapha Saidallah', 'Fatimazahra Taki', 'Abdelbaki El Belrhiti El Alaoui', 'Abdeslam El Fergougui']","The Intelligent Transportation Systems (ITS) are the subject of a world economic competition. They are the application of new information and communication technologies in the transport sector, to make the infrastructures more efficient, more reliable and more ecological. License Plates Recognition (LPR) is the key module of these systems, in which the License Plate Localization (LPL) is the most important stage, because it determines the speed and robustness of this module. Thus, during this step the algorithm must process the image and overcome several constraints as climatic and lighting conditions, sensors and angles variety, LPs no-standardization, and the real time processing. This paper presents a classification and comparison of License Plates Localization (LPL) algorithms and describes the advantages, disadvantages and improvements made by each of them",11 pages,"April 2021, Volume 12",10.5121/sipij.2021.12201,cs.CV,"['cs.CV', 'cs.LG']","['http://dx.doi.org/10.5121/sipij.2021.12201', 'http://arxiv.org/abs/2104.13896v1', 'http://arxiv.org/pdf/2104.13896v1']",http://arxiv.org/pdf/2104.13896v1
222,http://arxiv.org/abs/2110.01394v1,2021-09-30 09:50:06+00:00,2021-09-30 09:50:06+00:00,Mulberry Leaf Yield Prediction Using Machine Learning Techniques,"['Srikantaiah K C', 'Deeksha A']","Soil nutrients are essential for the growth of healthy crops. India produces a humungous quantity of Mulberry leaves which in turn produces the raw silk. Since the climatic conditions in India is favourable, Mulberry is grown throughout the year. Majority of the farmers hardly pay attention to the nature of soil and abiotic factors due to which leaves become malnutritious and thus when they are consumed by the silkworm, desired quality end-product, raw silk, will not be produced. It is beneficial for the farmers to know the amount of yield that their land can produce so that they can plan in advance. In this paper, different Machine Learning techniques are used in predicting the yield of the Mulberry crops based on the soil parameters. Three advanced machine-learning models are selected and compared, namely, Multiple linear regression, Ridge regression and Random Forest Regression (RF). The experimental results show that Random Forest Regression outperforms other algorithms.","6 pages. Atlantis Highlights in Computer Sciences, volume 4
  Proceedings of the 3rd International Conference on Integrated Intelligent
  Computing Communication & Security (ICIIC 2021)",,,cs.LG,['cs.LG'],"['http://arxiv.org/abs/2110.01394v1', 'http://arxiv.org/pdf/2110.01394v1']",http://arxiv.org/pdf/2110.01394v1
223,http://arxiv.org/abs/2204.00922v1,2022-04-02 18:33:19+00:00,2022-04-02 18:33:19+00:00,Application of Stochastic Optimization Techniques to the Unit Commitment Problem -- A Review,['Vincent Meilinger'],"Due to the established energy production methods contribution to the climate crisis, renewable energy is to replace a substantial part of coal or nuclear plants to prevent greenhouse gases or toxic waste entering the atmosphere. This relatively quick shift in energy production, primarily pushed by increasing political and economical pressure, requires enormous effort on the part of the energy providers to balance out production fluctuations. Consequently, a lot of research is conducted in the key area of stochastic unit commitment (UC) on electrical grids and microgrids. The term unit commitment includes a large variety of optimization techniques, and in this paper we will review recent developments in this area. We start by giving an overview over different problem definitions and stochastic optimization procedures, to then assess recent contributions to this topic. We therefore compare the proposals and case studies of several papers.","Seminar thesis in the course ""Intelligent Software Systems"" at
  Technische Universit\""at Berlin",,,cs.OH,['cs.OH'],"['http://arxiv.org/abs/2204.00922v1', 'http://arxiv.org/pdf/2204.00922v1']",http://arxiv.org/pdf/2204.00922v1
224,http://arxiv.org/abs/2104.12125v1,2021-04-25 10:33:35+00:00,2021-04-25 10:33:35+00:00,Development of a Soft Actor Critic Deep Reinforcement Learning Approach for Harnessing Energy Flexibility in a Large Office Building,"['Anjukan Kathirgamanathan', 'Eleni Mangina', 'Donal P. Finn']","This research is concerned with the novel application and investigation of `Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. SAC is a model-free DRL technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the SAC DRL technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The SAC DRL was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control (RBC) scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the SAC DRL requires minimal training sample points and outperforms the RBC after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended.",submitted to Energy and AI,,10.1016/j.egyai.2021.100101,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY', 'J.2']","['http://dx.doi.org/10.1016/j.egyai.2021.100101', 'http://arxiv.org/abs/2104.12125v1', 'http://arxiv.org/pdf/2104.12125v1']",http://arxiv.org/pdf/2104.12125v1
225,http://arxiv.org/abs/1705.01985v1,2017-05-04 19:35:06+00:00,2017-05-04 19:35:06+00:00,"Research Agenda in Intelligent Infrastructure to Enhance Disaster Management, Community Resilience and Public Safety","['Michael Dunaway', 'Robin Murphy', 'Nalini Venkatasubramanian', 'Leysia Palen', 'Daniel Lopresti']","Modern societies can be understood as the intersection of four interdependent systems: (1) the natural environment of geography, climate and weather; (2) the built environment of cities, engineered systems, and physical infrastructure; (3) the social environment of human populations, communities and socio-economic activities; and (4) an information ecosystem that overlays the other three domains and provides the means for understanding, interacting with, and managing the relationships between the natural, built, and human environments.   As the nation and its communities become more connected, networked and technologically sophisticated, new challenges and opportunities arise that demand a rethinking of current approaches to public safety and emergency management. Addressing the current and future challenges requires an equally sophisticated program of research, technology development, and strategic planning. The design and integration of intelligent infrastructure-including embedded sensors, the Internet of Things (IoT), advanced wireless information technologies, real-time data capture and analysis, and machine-learning-based decision support-holds the potential to greatly enhance public safety, emergency management, disaster recovery, and overall community resilience, while addressing new and emerging threats to public safety and security. Ultimately, the objective of this program of research and development is to save lives, reduce risk and disaster impacts, permit efficient use of material and social resources, and protect quality of life and economic stability across entire regions.","A Computing Community Consortium (CCC) white paper, 4 pages",,,cs.CY,['cs.CY'],"['http://arxiv.org/abs/1705.01985v1', 'http://arxiv.org/pdf/1705.01985v1']",http://arxiv.org/pdf/1705.01985v1
226,http://arxiv.org/abs/0711.1985v1,2007-11-13 14:14:15+00:00,2007-11-13 14:14:15+00:00,Five or six step scenario for evolution?,['Brandon Carter'],"The prediction that (due to the limited amount of hydrogen available as fuel in the Sun) the future duration of our favourable terrestrial environment will be short (compared with the present age of the Earth) has been interpreted as evidence for a hard step scenario. This means that some of the essential steps (such as the development of eukaryotes) in the evolution process leading to the ultimate emergence of intelligent life would have been hard, in the sense of being against the odds in the available time, so that they are unlikely to have been achieved in most of the earth-like planets that may one day be discovered in nearby extra-solar systems. It was originally estimated that only one or two of the essential evolutionary steps had to have been hard in this sense, but it has become apparent that this figure may need upward revision, because recent studies of climatic instability suggest that the possible future duration of our biologically favourable environment may be shorter than had been supposed, only about one Giga year rather than five. On the basis of the statistical requirement of roughly equal spacing between hard steps, it is argued that the best fit with the fossil record is now obtainable by postulating the number of hard steps to be five, if our evolution was exclusively terrestrial, or six if, as now seems very plausible, the first step occurred on Mars.",11 pages Latex,Int. J. Astrobiology 7 (2008) 103516,10.1017/S1473550408004023,astro-ph,['astro-ph'],"['http://dx.doi.org/10.1017/S1473550408004023', 'http://arxiv.org/abs/0711.1985v1', 'http://arxiv.org/pdf/0711.1985v1']",http://arxiv.org/pdf/0711.1985v1
227,http://arxiv.org/abs/1212.3964v1,2012-12-17 11:47:09+00:00,2012-12-17 11:47:09+00:00,Advanced Bloom Filter Based Algorithms for Efficient Approximate Data De-Duplication in Streams,"['Suman K. Bera', 'Sourav Dutta', 'Ankur Narang', 'Souvik Bhattacherjee']","Applications involving telecommunication call data records, web pages, online transactions, medical records, stock markets, climate warning systems, etc., necessitate efficient management and processing of such massively exponential amount of data from diverse sources. De-duplication or Intelligent Compression in streaming scenarios for approximate identification and elimination of duplicates from such unbounded data stream is a greater challenge given the real-time nature of data arrival. Stable Bloom Filters (SBF) addresses this problem to a certain extent. .   In this work, we present several novel algorithms for the problem of approximate detection of duplicates in data streams. We propose the Reservoir Sampling based Bloom Filter (RSBF) combining the working principle of reservoir sampling and Bloom Filters. We also present variants of the novel Biased Sampling based Bloom Filter (BSBF) based on biased sampling concepts. We also propose a randomized load balanced variant of the sampling Bloom Filter approach to efficiently tackle the duplicate detection. In this work, we thus provide a generic framework for de-duplication using Bloom Filters. Using detailed theoretical analysis we prove analytical bounds on the false positive rate, false negative rate and convergence rate of the proposed structures. We exhibit that our models clearly outperform the existing methods. We also demonstrate empirical analysis of the structures using real-world datasets (3 million records) and also with synthetic datasets (1 billion records) capturing various input distributions.",41 pages,,,cs.IR,['cs.IR'],"['http://arxiv.org/abs/1212.3964v1', 'http://arxiv.org/pdf/1212.3964v1']",http://arxiv.org/pdf/1212.3964v1
228,http://arxiv.org/abs/1902.08035v1,2019-02-11 18:04:59+00:00,2019-02-11 18:04:59+00:00,Introduction: Detectability of Future Earth,['Jacob Haqq-Misra'],"Earth's future detectability depends upon the trajectory of our civilization over the coming centuries. Human civilization is also the only known example of an energy-intensive civilization, so our history and future trajectories provide the basis for thinking about how to find life elsewhere. This special issue of Futures features contributions that consider the future evolution of the Earth system from an astrobiological perspective, with the goal of exploring the extent to which anthropogenic influence could be detectable across interstellar distances. This collection emphasizes the connection between the unfolding future of the Anthropocene with the search for extraterrestrial civilizations. Our rate of energy consumption will characterize the extent to which our energy-intensive society exerts direct influence on climate, which in turn may limit the ultimate lifetime of our civilization. If the answer to Fermi's question is that we are alone, so that our civilization represents the only form of intelligent life in the galaxy (or even the universe), then our responsibility to survive is even greater. If we do find evidence of another civilization on a distant exoplanet, then at least we will know that our trajectory can be managed. But as long as our searches turn up empty, we must stay vigilant to keep our future secure.","Introduction to the special issue on the Detectability of Future
  Earth, published in Futures (volume 106, pages 1-44)","Futures 106:1-3, 2019",10.1016/j.futures.2018.11.006,physics.gen-ph,"['physics.gen-ph', 'astro-ph.EP', 'physics.pop-ph']","['http://dx.doi.org/10.1016/j.futures.2018.11.006', 'http://arxiv.org/abs/1902.08035v1', 'http://arxiv.org/pdf/1902.08035v1']",http://arxiv.org/pdf/1902.08035v1
229,http://arxiv.org/abs/2005.09023v2,2020-10-23 11:25:26+00:00,2020-05-18 18:45:09+00:00,Automating Turbulence Modeling by Multi-Agent Reinforcement Learning,"['Guido Novati', 'Hugues Lascombes de Laroussilhe', 'Petros Koumoutsakos']","The modeling of turbulent flows is critical to scientific and engineering problems ranging from aircraft design to weather forecasting and climate prediction. Over the last sixty years numerous turbulence models have been proposed, largely based on physical insight and engineering intuition. Recent advances in machine learning and data science have incited new efforts to complement these approaches. To date, all such efforts have focused on supervised learning which, despite demonstrated promise, encounters difficulties in generalizing beyond the distributions of the training data. In this work we introduce multi-agent reinforcement learning (MARL) as an automated discovery tool of turbulence models. We demonstrate the potential of this approach on Large Eddy Simulations of homogeneous and isotropic turbulence using as reward the recovery of the statistical properties of Direct Numerical Simulations. Here, the closure model is formulated as a control policy enacted by cooperating agents, which detect critical spatio-temporal patterns in the flow field to estimate the unresolved sub-grid scale (SGS) physics. The present results are obtained with state-of-the-art algorithms based on experience replay and compare favorably with established dynamic SGS modeling approaches. Moreover, we show that the present turbulence models generalize across grid sizes and flow conditions as expressed by the Reynolds numbers.",To be published in Nature Machine Intelligence,,,physics.comp-ph,"['physics.comp-ph', 'cs.LG']","['http://arxiv.org/abs/2005.09023v2', 'http://arxiv.org/pdf/2005.09023v2']",http://arxiv.org/pdf/2005.09023v2
230,http://arxiv.org/abs/2105.06637v3,2022-04-22 04:32:53+00:00,2021-05-14 04:17:24+00:00,"Understanding occupants' behaviour, engagement, emotion, and comfort indoors with heterogeneous sensors and wearables","['Nan Gao', 'Max Marschall', 'Jane Burry', 'Simon Watkins', 'Flora D. Salim']","We conducted a field study at a K-12 private school in the suburbs of Melbourne, Australia. The data capture contained two elements: First, a 5-month longitudinal field study In-Gauge using two outdoor weather stations, as well as indoor weather stations in 17 classrooms and temperature sensors on the vents of occupant-controlled room air-conditioners; these were collated into individual datasets for each classroom at a 5-minute logging frequency, including additional data on occupant presence. The dataset was used to derive predictive models of how occupants operate room air-conditioning units. Second, we tracked 23 students and 6 teachers in a 4-week cross-sectional study En-Gage, using wearable sensors to log physiological data, as well as daily surveys to query the occupants' thermal comfort, learning engagement, emotions and seating behaviours. Overall, the combined dataset could be used to analyse the relationships between indoor/outdoor climates and students' behaviours/mental states on campus, which provide opportunities for the future design of intelligent feedback systems to benefit both students and staff.","This paper has been accepted by Nature Scientific Data. The link for
  the datasets:
  https://rmit.figshare.com/articles/dataset/In-Gauge_and_En-Gage_Datasets/14578908",,,cs.HC,"['cs.HC', 'cs.CY', 'cs.LG']","['http://arxiv.org/abs/2105.06637v3', 'http://arxiv.org/pdf/2105.06637v3']",http://arxiv.org/pdf/2105.06637v3
231,http://arxiv.org/abs/2108.02281v2,2022-03-22 11:57:17+00:00,2021-08-04 20:41:30+00:00,Context-Aware Environment Monitoring to Support LPWAN-based Battlefield Applications,"['Guilherme Rotth Zibetti', 'Juliano Araujo Wickboldt', 'Edison Pignaton de Freitas']","The use of IoT-related technologies is growing in several areas. Applications of environmental monitoring, logistics, smart cities are examples of applications that benefit from advances in IoT. In the military context, IoT applications can support the decision-making process by delivering information collected directly from the battlefield to Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) systems. Taking the benefit of the installed IoT network in the battlefield, the use of the data collected by the IoT nodes is a way to improve resiliency and increase the survivability of networks, as well as to optimize the use of available resources. Towards improving the communication network present on the battlefield, this work presents a context-aware environmental monitoring system that uses real-time battlefield information to increase military networks' resilience and survivability. The proposed approach is validated by a proof-of-concept experiment. The obtained results show that the implementation of this system can improve the communication process even when the network is exposed to unfavorable climatic factors.",,Computer Communications 189C (2022) pp. 18-27,10.1016/j.comcom.2022.02.020,cs.NI,"['cs.NI', 'cs.SY', 'eess.SY']","['http://dx.doi.org/10.1016/j.comcom.2022.02.020', 'http://arxiv.org/abs/2108.02281v2', 'http://arxiv.org/pdf/2108.02281v2']",http://arxiv.org/pdf/2108.02281v2
232,http://arxiv.org/abs/1201.2288v1,2012-01-11 12:02:39+00:00,2012-01-11 12:02:39+00:00,Nimble@ITCEcnoGrid: A Grid in Research Domain for Weather Forecasting,"['Vijay Dhir', 'Rattan K. Datta', 'Maitreyee Dutta']","Computer Technology has Revolutionized Science. This has motivated scientists to develop mathematical model to simulate salient features of Physical universe. These models can approximate reality at many levels of scale such as atomic nucleus, Earth's biosphere & weather/climate assessment. If the computer power is greater, the greater will be the accuracy in approximation i.e. close will be the approximation to the reality. The speed of the computer required for solution of such problems require computers with processing power of teraflops to Pets flops speed.. The way to speed up the computation is to ""parallelize"" it. One of the approach is to use multimillion dollar Supercomputer or use Computational Grid (which is also called poor man's supercomputer) having geographically distributed resources e.g. SETI@home (Used to detect radio waves emitted by intelligent civilizations outside earth) has 4.6 million participants computers. There are many alternatives tools available to achieve this goal like Globus Toolkit, Entropia, Legion, BOINC etc but they are mainly based on Linux platform. As majority of the computers available are windows based, so it will be easy to develop a larger network of computers which will use the free cycles of the computer to solve the complex problem at window platform. Nimble@ITCEcnoGrid has been developed. It includes the feature of Inter Thread Communication which is missing in any of the toolkits available. Nimble@ITCEcnoGrid Framework (A Fast Grid with Inter-thread communication with Economic Based Policy) was tested for computation of 'PI' up to 120 decimal points. Encouraged by the speed the same system has been utilized to computes the Momentum, Thermodynamics and Continuity equations for the Weather Forecasting using the Windows based Desktop computers.",11 pages,"International Journal of Grid Computing & Applications (IJGCA)
  Vol.2, No.4, December 2011, 37-47",10.5121/ijgca.2011.2404,cs.DC,"['cs.DC', 'cs.NI']","['http://dx.doi.org/10.5121/ijgca.2011.2404', 'http://arxiv.org/abs/1201.2288v1', 'http://arxiv.org/pdf/1201.2288v1']",http://arxiv.org/pdf/1201.2288v1
233,http://arxiv.org/abs/2001.08966v1,2020-01-24 12:46:36+00:00,2020-01-24 12:46:36+00:00,Design optimisation of a multi-mode wave energy converter,"['Nataliia Y. Sergiienko', 'Mehdi Neshat', 'Leandro S. P. da Silva', 'Bradley Alexander', 'Markus Wagner']","A wave energy converter (WEC) similar to the CETO system developed by Carnegie Clean Energy is considered for design optimisation. This WEC is able to absorb power from heave, surge and pitch motion modes, making the optimisation problem nontrivial. The WEC dynamics is simulated using the spectral-domain model taking into account hydrodynamic forces, viscous drag, and power take-off forces. The design parameters for optimisation include the buoy radius, buoy height, tether inclination angles, and control variables (damping and stiffness). The WEC design is optimised for the wave climate at Albany test site in Western Australia considering unidirectional irregular waves. Two objective functions are considered: (i) maximisation of the annual average power output, and (ii) minimisation of the levelised cost of energy (LCoE) for a given sea site. The LCoE calculation is approximated as a ratio of the produced energy to the significant mass of the system that includes the mass of the buoy and anchor system. Six different heuristic optimisation methods are applied in order to evaluate and compare the performance of the best known evolutionary algorithms, a swarm intelligence technique and a numerical optimisation approach. The results demonstrate that if we are interested in maximising energy production without taking into account the cost of manufacturing such a system, the buoy should be built as large as possible (20 m radius and 30 m height). However, if we want the system that produces cheap energy, then the radius of the buoy should be approximately 11-14~m while the height should be as low as possible. These results coincide with the overall design that Carnegie Clean Energy has selected for its CETO 6 multi-moored unit. However, it should be noted that this study is not informed by them, so this can be seen as an independent validation of the design choices.",,,,cs.NE,['cs.NE'],"['http://arxiv.org/abs/2001.08966v1', 'http://arxiv.org/pdf/2001.08966v1']",http://arxiv.org/pdf/2001.08966v1
